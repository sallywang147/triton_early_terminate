`torch_dtype` is deprecated! Use `dtype` instead!
2025-12-10 21:24:19.746384: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-10 21:24:19.765104: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1765401859.786490   19059 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1765401859.793374   19059 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1765401859.810433   19059 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1765401859.810462   19059 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1765401859.810465   19059 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1765401859.810467   19059 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-12-10 21:24:19.815218: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
==PROF== Connected to process 19059 (/usr/bin/python3.12)
==PROF== Profiling "indexSelectSmallIndex" - 0: 0%....50%....100% - 10 passes
==PROF== Profiling "elementwise_kernel_with_index" - 1: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 2: 0%....50%....100% - 10 passes
==PROF== Profiling "reduce_kernel" - 3: 0%....50%....100% - 10 passes
==PROF== Profiling "DeviceScanInitKernel" - 4: 0%....50%....100% - 10 passes
==PROF== Profiling "DeviceScanKernel" - 5: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 6: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 7: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 8: 0%....50%....100% - 10 passes
==PROF== Profiling "indexSelectSmallIndex" - 9: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 10: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_layer_norm_kernel" - 11: 0%....50%....100% - 10 passes
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
==PROF== Profiling "triton_linear_kernel" - 12: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 13: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 14: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 15: 0%....50%....100% - 10 passes
==PROF== Profiling "CatArrayBatchedCopy" - 16: 0%....50%....100% - 10 passes
==PROF== Profiling "CatArrayBatchedCopy" - 17: 0%....50%....100% - 10 passes
==PROF== Profiling "fmha_cutlassF_f32_aligned_64x..." - 18: 0%....50%....100% - 10 passes
==PROF== Profiling "ampere_sgemm_128x32_tn" - 19: 0%....50%....100% - 10 passes
==PROF== Profiling "splitKreduce_kernel" - 20: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 21: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_layer_norm_kernel" - 22: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 23: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 24: 0%....50%....100% - 10 passes
==PROF== Profiling "ampere_sgemm_128x32_sliced1x4..." - 25: 0%....50%....100% - 10 passes
==PROF== Profiling "splitKreduce_kernel" - 26: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 27: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_layer_norm_kernel" - 28: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 29: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 30: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 31: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 32: 0%....50%....100% - 10 passes
==PROF== Profiling "CatArrayBatchedCopy" - 33: 0%....50%....100% - 10 passes
==PROF== Profiling "CatArrayBatchedCopy" - 34: 0%....50%....100% - 10 passes
==PROF== Profiling "fmha_cutlassF_f32_aligned_64x..." - 35: 0%....50%....100% - 10 passes
==PROF== Profiling "ampere_sgemm_128x32_tn" - 36: 0%....50%....100% - 10 passes
==PROF== Profiling "splitKreduce_kernel" - 37: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 38: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_layer_norm_kernel" - 39: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 40: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 41: 0%....50%....100% - 10 passes
==PROF== Profiling "ampere_sgemm_128x32_sliced1x4..." - 42: 0%....50%....100% - 10 passes
==PROF== Profiling "splitKreduce_kernel" - 43: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 44: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_layer_norm_kernel" - 45: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 46: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 47: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 48: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 49: 0%....50%....100% - 10 passes
==PROF== Profiling "CatArrayBatchedCopy" - 50: 0%....50%....100% - 10 passes
==PROF== Profiling "CatArrayBatchedCopy" - 51: 0%....50%....100% - 10 passes
==PROF== Profiling "fmha_cutlassF_f32_aligned_64x..." - 52: 0%....50%....100% - 10 passes
==PROF== Profiling "ampere_sgemm_128x32_tn" - 53: 0%....50%....100% - 10 passes
==PROF== Profiling "splitKreduce_kernel" - 54: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 55: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_layer_norm_kernel" - 56: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 57: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 58: 0%....50%....100% - 10 passes
==PROF== Profiling "ampere_sgemm_128x32_sliced1x4..." - 59: 0%....50%....100% - 10 passes
==PROF== Profiling "splitKreduce_kernel" - 60: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 61: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_layer_norm_kernel" - 62: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 63: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 64: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 65: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 66: 0%....50%....100% - 10 passes
==PROF== Profiling "CatArrayBatchedCopy" - 67: 0%....50%....100% - 10 passes
==PROF== Profiling "CatArrayBatchedCopy" - 68: 0%....50%....100% - 10 passes
==PROF== Profiling "fmha_cutlassF_f32_aligned_64x..." - 69: 0%....50%....100% - 10 passes
==PROF== Profiling "ampere_sgemm_128x32_tn" - 70: 0%....50%....100% - 10 passes
==PROF== Profiling "splitKreduce_kernel" - 71: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 72: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_layer_norm_kernel" - 73: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 74: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 75: 0%....50%....100% - 10 passes
==PROF== Profiling "ampere_sgemm_128x32_sliced1x4..." - 76: 0%....50%....100% - 10 passes
==PROF== Profiling "splitKreduce_kernel" - 77: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 78: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_layer_norm_kernel" - 79: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 80: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 81: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 82: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 83: 0%....50%....100% - 10 passes
==PROF== Profiling "CatArrayBatchedCopy" - 84: 0%....50%....100% - 10 passes
==PROF== Profiling "CatArrayBatchedCopy" - 85: 0%....50%....100% - 10 passes
==PROF== Profiling "fmha_cutlassF_f32_aligned_64x..." - 86: 0%....50%....100% - 10 passes
==PROF== Profiling "ampere_sgemm_128x32_tn" - 87: 0%....50%....100% - 10 passes
==PROF== Profiling "splitKreduce_kernel" - 88: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 89: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_layer_norm_kernel" - 90: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 91: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 92: 0%....50%....100% - 10 passes
==PROF== Profiling "ampere_sgemm_128x32_sliced1x4..." - 93: 0%....50%....100% - 10 passes
==PROF== Profiling "splitKreduce_kernel" - 94: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 95: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_layer_norm_kernel" - 96: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 97: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 98: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 99: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 100: 0%....50%....100% - 10 passes
==PROF== Profiling "CatArrayBatchedCopy" - 101: 0%....50%....100% - 10 passes
==PROF== Profiling "CatArrayBatchedCopy" - 102: 0%....50%....100% - 10 passes
==PROF== Profiling "fmha_cutlassF_f32_aligned_64x..." - 103: 0%....50%....100% - 10 passes
==PROF== Profiling "ampere_sgemm_128x32_tn" - 104: 0%....50%....100% - 10 passes
==PROF== Profiling "splitKreduce_kernel" - 105: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 106: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_layer_norm_kernel" - 107: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 108: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 109: 0%....50%....100% - 10 passes
==PROF== Profiling "ampere_sgemm_128x32_sliced1x4..." - 110: 0%....50%....100% - 10 passes
==PROF== Profiling "splitKreduce_kernel" - 111: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 112: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_layer_norm_kernel" - 113: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 114: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 115: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 116: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 117: 0%....50%....100% - 10 passes
==PROF== Profiling "CatArrayBatchedCopy" - 118: 0%....50%....100% - 10 passes
==PROF== Profiling "CatArrayBatchedCopy" - 119: 0%....50%....100% - 10 passes
==PROF== Profiling "fmha_cutlassF_f32_aligned_64x..." - 120: 0%....50%....100% - 10 passes
==PROF== Profiling "ampere_sgemm_128x32_tn" - 121: 0%....50%....100% - 10 passes
==PROF== Profiling "splitKreduce_kernel" - 122: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 123: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_layer_norm_kernel" - 124: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 125: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 126: 0%....50%....100% - 10 passes
==PROF== Profiling "ampere_sgemm_128x32_sliced1x4..." - 127: 0%....50%....100% - 10 passes
==PROF== Profiling "splitKreduce_kernel" - 128: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 129: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_layer_norm_kernel" - 130: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 131: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 132: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 133: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 134: 0%....50%....100% - 10 passes
==PROF== Profiling "CatArrayBatchedCopy" - 135: 0%....50%....100% - 10 passes
==PROF== Profiling "CatArrayBatchedCopy" - 136: 0%....50%....100% - 10 passes
==PROF== Profiling "fmha_cutlassF_f32_aligned_64x..." - 137: 0%....50%....100% - 10 passes
==PROF== Profiling "ampere_sgemm_128x32_tn" - 138: 0%....50%....100% - 10 passes
==PROF== Profiling "splitKreduce_kernel" - 139: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 140: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_layer_norm_kernel" - 141: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 142: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 143: 0%....50%....100% - 10 passes
==PROF== Profiling "ampere_sgemm_128x32_sliced1x4..." - 144: 0%....50%....100% - 10 passes
==PROF== Profiling "splitKreduce_kernel" - 145: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 146: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_layer_norm_kernel" - 147: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 148: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 149: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 150: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 151: 0%....50%....100% - 10 passes
==PROF== Profiling "CatArrayBatchedCopy" - 152: 0%....50%....100% - 10 passes
==PROF== Profiling "CatArrayBatchedCopy" - 153: 0%....50%....100% - 10 passes
==PROF== Profiling "fmha_cutlassF_f32_aligned_64x..." - 154: 0%....50%....100% - 10 passes
==PROF== Profiling "ampere_sgemm_128x32_tn" - 155: 0%....50%....100% - 10 passes
==PROF== Profiling "splitKreduce_kernel" - 156: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 157: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_layer_norm_kernel" - 158: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 159: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 160: 0%....50%....100% - 10 passes
==PROF== Profiling "ampere_sgemm_128x32_sliced1x4..." - 161: 0%....50%....100% - 10 passes
==PROF== Profiling "splitKreduce_kernel" - 162: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 163: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_layer_norm_kernel" - 164: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 165: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 166: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 167: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 168: 0%....50%....100% - 10 passes
==PROF== Profiling "CatArrayBatchedCopy" - 169: 0%....50%....100% - 10 passes
==PROF== Profiling "CatArrayBatchedCopy" - 170: 0%....50%....100% - 10 passes
==PROF== Profiling "fmha_cutlassF_f32_aligned_64x..." - 171: 0%....50%....100% - 10 passes
==PROF== Profiling "ampere_sgemm_128x32_tn" - 172: 0%....50%....100% - 10 passes
==PROF== Profiling "splitKreduce_kernel" - 173: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 174: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_layer_norm_kernel" - 175: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 176: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 177: 0%....50%....100% - 10 passes
==PROF== Profiling "ampere_sgemm_128x32_sliced1x4..." - 178: 0%....50%....100% - 10 passes
==PROF== Profiling "splitKreduce_kernel" - 179: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 180: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_layer_norm_kernel" - 181: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 182: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 183: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 184: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 185: 0%....50%....100% - 10 passes
==PROF== Profiling "CatArrayBatchedCopy" - 186: 0%....50%....100% - 10 passes
==PROF== Profiling "CatArrayBatchedCopy" - 187: 0%....50%....100% - 10 passes
==PROF== Profiling "fmha_cutlassF_f32_aligned_64x..." - 188: 0%....50%....100% - 10 passes
==PROF== Profiling "ampere_sgemm_128x32_tn" - 189: 0%....50%....100% - 10 passes
==PROF== Profiling "splitKreduce_kernel" - 190: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 191: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_layer_norm_kernel" - 192: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 193: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 194: 0%....50%....100% - 10 passes
==PROF== Profiling "ampere_sgemm_128x32_sliced1x4..." - 195: 0%....50%....100% - 10 passes
==PROF== Profiling "splitKreduce_kernel" - 196: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 197: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_layer_norm_kernel" - 198: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 199: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 200: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 201: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 202: 0%....50%....100% - 10 passes
==PROF== Profiling "CatArrayBatchedCopy" - 203: 0%....50%....100% - 10 passes
==PROF== Profiling "CatArrayBatchedCopy" - 204: 0%....50%....100% - 10 passes
==PROF== Profiling "fmha_cutlassF_f32_aligned_64x..." - 205: 0%....50%....100% - 10 passes
==PROF== Profiling "ampere_sgemm_128x32_tn" - 206: 0%....50%....100% - 10 passes
==PROF== Profiling "splitKreduce_kernel" - 207: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 208: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_layer_norm_kernel" - 209: 0%....50%....100% - 10 passes
==PROF== Profiling "triton_linear_kernel" - 210: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 211: 0%....50%....100% - 10 passes
==PROF== Profiling "ampere_sgemm_128x32_sliced1x4..." - 212: 0%....50%....100% - 10 passes
==PROF== Profiling "splitKreduce_kernel" - 213: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 214: 0%....50%....100% - 10 passes
==PROF== Profiling "vectorized_layer_norm_kernel" - 215: 0%....50%....100% - 10 passes
==PROF== Profiling "scal_64addr_kernel" - 216: 0%....50%....100% - 10 passes
==PROF== Profiling "sgemm_largek_lds64" - 217: 0%....50%....100% - 10 passes
==PROF== Profiling "reduce_kernel" - 218: 0%....50%....100% - 10 passes
✅ Patched OPT: q_proj/k_proj/v_proj → TritonLinear, fc1 → TritonLinearET
Logits mean: -4.414247989654541
==PROF== Disconnected from process 19059
[19059] python3.12@127.0.0.1
  void at::<unnamed>::indexSelectSmallIndex<float, long, unsigned int, 2, 2, (int)-2>(cuda::TensorInfo<T1, T3>, cuda::TensorInfo<const T1, T3>, cuda::TensorInfo<const T2, T3>, int, int, T3, long) (6, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle       11,996
    Memory Throughput                 %         0.25
    DRAM Throughput                   %         0.14
    Duration                         us        10.43
    L1/TEX Cache Throughput           %         2.29
    L2 Cache Throughput               %         0.35
    SM Active Cycles              cycle       544.85
    Compute (SM) Throughput           %         0.31
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      6
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             768
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 94.44%                                                                                          
          The grid for this launch is configured to execute only 6 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         6.20
    Achieved Active Warps Per SM           warp         3.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.8%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        22.70
    Total DRAM Elapsed Cycles        cycle      660,992
    Average L1 Active Cycles         cycle       544.85
    Total L1 Elapsed Cycles          cycle    1,294,210
    Average L2 Active Cycles         cycle     1,094.81
    Total L2 Elapsed Cycles          cycle      909,920
    Average SM Active Cycles         cycle       544.85
    Total SM Elapsed Cycles          cycle    1,294,210
    Average SMSP Active Cycles       cycle       532.06
    Total SMSP Elapsed Cycles        cycle    5,176,840
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.455%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 56.68% above the average, while the minimum instance value is 92.88% below the      
          average.                                                                                                      

  void <unnamed>::elementwise_kernel_with_index<int, at::arange_cuda_out(const c10::Scalar &, const c10::Scalar &, const c10::Scalar &, at::Tensor &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long) (instance 1)]>(T1, T2, function_traits<T2>::result_type *) (1, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        3,205
    Memory Throughput                 %         0.47
    DRAM Throughput                   %         0.05
    Duration                         us         2.82
    L1/TEX Cache Throughput           %        48.11
    L2 Cache Throughput               %         0.47
    SM Active Cycles              cycle        12.47
    Compute (SM) Throughput           %         0.00
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    64
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread              64
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 99.07%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           32
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         3.10
    Achieved Active Warps Per SM           warp         1.99
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 96.9%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (3.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle         2.10
    Total DRAM Elapsed Cycles        cycle      178,688
    Average L1 Active Cycles         cycle        12.47
    Total L1 Elapsed Cycles          cycle      347,802
    Average L2 Active Cycles         cycle       148.09
    Total L2 Elapsed Cycles          cycle      243,040
    Average SM Active Cycles         cycle        12.47
    Total SM Elapsed Cycles          cycle      347,802
    Average SMSP Active Cycles       cycle         5.67
    Total SMSP Elapsed Cycles        cycle    1,391,208
    -------------------------- ----------- ------------

  void at::vectorized_elementwise_kernel<4, at::AUnaryFunctor<long, long, bool, at::<unnamed>::CompareEqFunctor<long>>, std::array<char *, 2>>(int, T2, T3) (1, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.57
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        4,236
    Memory Throughput                 %         0.45
    DRAM Throughput                   %         0.09
    Duration                         us         3.71
    L1/TEX Cache Throughput           %        27.70
    L2 Cache Throughput               %         0.45
    SM Active Cycles              cycle        21.66
    Compute (SM) Throughput           %         0.03
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             128
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 99.07%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         5.00
    Achieved Active Warps Per SM           warp         3.20
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 95%                                                                                       
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (5.0%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle         5.10
    Total DRAM Elapsed Cycles        cycle      232,960
    Average L1 Active Cycles         cycle        21.66
    Total L1 Elapsed Cycles          cycle      452,542
    Average L2 Active Cycles         cycle       210.80
    Total L2 Elapsed Cycles          cycle      321,200
    Average SM Active Cycles         cycle        21.66
    Total SM Elapsed Cycles          cycle      452,542
    Average SMSP Active Cycles       cycle        17.32
    Total SMSP Elapsed Cycles        cycle    1,810,168
    -------------------------- ----------- ------------

  void at::reduce_kernel<512, 1, at::ReduceOp<bool, at::func_wrapper_t<bool, at::and_kernel_cuda(at::TensorIterator &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 12)]::operator ()() const::[lambda(bool, bool) (instance 1)]>, unsigned int, bool, 4, 4>>(T3) (1, 1, 1)x(8, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        7,085
    Memory Throughput                 %         1.17
    DRAM Throughput                   %         0.43
    Duration                         us         6.21
    L1/TEX Cache Throughput           %        12.32
    L2 Cache Throughput               %         1.17
    SM Active Cycles              cycle        48.77
    Compute (SM) Throughput           %         0.01
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     8
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block              16
    # SMs                                         SM             108
    Threads                                   thread               8
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 75%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 8      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 99.07%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           56
    Block Limit Warps                     block           64
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %         1.54
    Achieved Active Warps Per SM           warp         0.98
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 96.93%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (1.5%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        42.30
    Total DRAM Elapsed Cycles        cycle      392,192
    Average L1 Active Cycles         cycle        48.77
    Total L1 Elapsed Cycles          cycle      757,996
    Average L2 Active Cycles         cycle       660.25
    Total L2 Elapsed Cycles          cycle      537,120
    Average SM Active Cycles         cycle        48.77
    Total SM Elapsed Cycles          cycle      757,996
    Average SMSP Active Cycles       cycle        12.51
    Total SMSP Elapsed Cycles        cycle    3,031,984
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.69%                                                                                           
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 57.87% above the average, while the minimum instance value is 88.19% below the      
          average.                                                                                                      

  void at_cuda_detail::DeviceScanInitKernel<at_cuda_detail::ScanTileState<long, 1>>(T1, int) (1, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
     CCCL
        <0,cub::DeviceScan::InclusiveScan>
          REGISTERED: cub::DeviceScan::InclusiveScan
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.56
    SM Frequency                    Ghz         1.13
    Elapsed Cycles                cycle        3,054
    Memory Throughput                 %         0.49
    DRAM Throughput                   %         0.05
    Duration                         us         2.69
    L1/TEX Cache Throughput           %        49.62
    L2 Cache Throughput               %         0.49
    SM Active Cycles              cycle        12.09
    Compute (SM) Throughput           %         0.01
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             128
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 99.07%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         6.04
    Achieved Active Warps Per SM           warp         3.87
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.96%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.0%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle         2.10
    Total DRAM Elapsed Cycles        cycle      167,424
    Average L1 Active Cycles         cycle        12.09
    Total L1 Elapsed Cycles          cycle      332,548
    Average L2 Active Cycles         cycle       157.10
    Total L2 Elapsed Cycles          cycle      231,600
    Average SM Active Cycles         cycle        12.09
    Total SM Elapsed Cycles          cycle      332,548
    Average SMSP Active Cycles       cycle        11.50
    Total SMSP Elapsed Cycles        cycle    1,330,192
    -------------------------- ----------- ------------

  void at_cuda_detail::DeviceScanKernel<at_cuda_detail::DeviceScanPolicy<long, std::plus<long>>::Policy900, const long *, long *, at_cuda_detail::ScanTileState<long, 1>, std::plus<long>, at_cuda_detail::NullType, int, long>(T2, T3, T4, int, T5, T6, T7) (1, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
     CCCL
        <0,cub::DeviceScan::InclusiveScan>
          REGISTERED: cub::DeviceScan::InclusiveScan
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        4,904
    Memory Throughput                 %         0.81
    DRAM Throughput                   %         0.28
    Duration                         us         4.29
    L1/TEX Cache Throughput           %        21.64
    L2 Cache Throughput               %         0.81
    SM Active Cycles              cycle        27.90
    Compute (SM) Throughput           %         0.05
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              46
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            7.18
    # SMs                                         SM             108
    Threads                                   thread             128
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 99.07%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           12
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           40
    Theoretical Occupancy                     %        62.50
    Achieved Occupancy                        %         6.18
    Achieved Active Warps Per SM           warp         3.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.12%                                                                                    
          The difference between calculated theoretical (62.5%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 37.5%                                                                                     
          The 10.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the      
          hardware maximum of 16. This kernel's theoretical occupancy (62.5%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        18.60
    Total DRAM Elapsed Cycles        cycle      270,336
    Average L1 Active Cycles         cycle        27.90
    Total L1 Elapsed Cycles          cycle      527,772
    Average L2 Active Cycles         cycle       323.02
    Total L2 Elapsed Cycles          cycle      371,600
    Average SM Active Cycles         cycle        27.90
    Total SM Elapsed Cycles          cycle      527,772
    Average SMSP Active Cycles       cycle        27.75
    Total SMSP Elapsed Cycles        cycle    2,111,088
    -------------------------- ----------- ------------

  void at::vectorized_elementwise_kernel<2, at::BinaryFunctor<long, long, long, binary_internal::MulFunctor<long>>, std::array<char *, 3>>(int, T2, T3) (1, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        4,100
    Memory Throughput                 %         0.46
    DRAM Throughput                   %         0.09
    Duration                         us         3.58
    L1/TEX Cache Throughput           %        30.17
    L2 Cache Throughput               %         0.46
    SM Active Cycles              cycle        19.89
    Compute (SM) Throughput           %         0.03
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             128
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 99.07%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %           75
    Achieved Occupancy                        %         5.36
    Achieved Active Warps Per SM           warp         3.43
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 92.85%                                                                                    
          The difference between calculated theoretical (75.0%) and measured achieved occupancy (5.4%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 25%                                                                                       
          The 12.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the      
          hardware maximum of 16. This kernel's theoretical occupancy (75.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle         5.10
    Total DRAM Elapsed Cycles        cycle      226,304
    Average L1 Active Cycles         cycle        19.89
    Total L1 Elapsed Cycles          cycle      432,942
    Average L2 Active Cycles         cycle       244.46
    Total L2 Elapsed Cycles          cycle      310,800
    Average SM Active Cycles         cycle        19.89
    Total SM Elapsed Cycles          cycle      432,942
    Average SMSP Active Cycles       cycle        16.91
    Total SMSP Elapsed Cycles        cycle    1,731,768
    -------------------------- ----------- ------------

  void at::vectorized_elementwise_kernel<2, at::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3) (1, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.57
    SM Frequency                    Ghz         1.13
    Elapsed Cycles                cycle        3,743
    Memory Throughput                 %         0.49
    DRAM Throughput                   %         0.09
    Duration                         us         3.30
    L1/TEX Cache Throughput           %        32.68
    L2 Cache Throughput               %         0.49
    SM Active Cycles              cycle        18.36
    Compute (SM) Throughput           %         0.03
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              36
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             128
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 99.07%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %           75
    Achieved Occupancy                        %         5.58
    Achieved Active Warps Per SM           warp         3.57
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 92.56%                                                                                    
          The difference between calculated theoretical (75.0%) and measured achieved occupancy (5.6%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 25%                                                                                       
          The 12.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the      
          hardware maximum of 16. This kernel's theoretical occupancy (75.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle         4.50
    Total DRAM Elapsed Cycles        cycle      207,616
    Average L1 Active Cycles         cycle        18.36
    Total L1 Elapsed Cycles          cycle      407,772
    Average L2 Active Cycles         cycle       208.94
    Total L2 Elapsed Cycles          cycle      283,840
    Average SM Active Cycles         cycle        18.36
    Total SM Elapsed Cycles          cycle      407,772
    Average SMSP Active Cycles       cycle        15.94
    Total SMSP Elapsed Cycles        cycle    1,631,088
    -------------------------- ----------- ------------

  void at::vectorized_elementwise_kernel<2, at::CUDAFunctorOnSelf_add<long>, std::array<char *, 2>>(int, T2, T3) (1, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.57
    SM Frequency                    Ghz         1.13
    Elapsed Cycles                cycle        3,813
    Memory Throughput                 %         0.48
    DRAM Throughput                   %         0.09
    Duration                         us         3.36
    L1/TEX Cache Throughput           %        32.55
    L2 Cache Throughput               %         0.48
    SM Active Cycles              cycle        18.44
    Compute (SM) Throughput           %         0.03
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              36
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             128
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 99.07%                                                                                          
          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %           75
    Achieved Occupancy                        %         5.52
    Achieved Active Warps Per SM           warp         3.54
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 92.63%                                                                                    
          The difference between calculated theoretical (75.0%) and measured achieved occupancy (5.5%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 25%                                                                                       
          The 12.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the      
          hardware maximum of 16. This kernel's theoretical occupancy (75.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle         4.50
    Total DRAM Elapsed Cycles        cycle      210,944
    Average L1 Active Cycles         cycle        18.44
    Total L1 Elapsed Cycles          cycle      402,964
    Average L2 Active Cycles         cycle       199.81
    Total L2 Elapsed Cycles          cycle      289,200
    Average SM Active Cycles         cycle        18.44
    Total SM Elapsed Cycles          cycle      402,964
    Average SMSP Active Cycles       cycle        16.08
    Total SMSP Elapsed Cycles        cycle    1,611,856
    -------------------------- ----------- ------------

  void at::<unnamed>::indexSelectSmallIndex<float, long, unsigned int, 2, 2, (int)-2>(cuda::TensorInfo<T1, T3>, cuda::TensorInfo<const T1, T3>, cuda::TensorInfo<const T2, T3>, int, int, T3, long) (6, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle       12,154
    Memory Throughput                 %         0.24
    DRAM Throughput                   %         0.14
    Duration                         us        10.56
    L1/TEX Cache Throughput           %         2.31
    L2 Cache Throughput               %         0.33
    SM Active Cycles              cycle          538
    Compute (SM) Throughput           %         0.32
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      6
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             768
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 94.44%                                                                                          
          The grid for this launch is configured to execute only 6 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         6.23
    Achieved Active Warps Per SM           warp         3.99
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.77%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        22.70
    Total DRAM Elapsed Cycles        cycle      670,720
    Average L1 Active Cycles         cycle          538
    Total L1 Elapsed Cycles          cycle    1,277,840
    Average L2 Active Cycles         cycle       981.70
    Total L2 Elapsed Cycles          cycle      921,680
    Average SM Active Cycles         cycle          538
    Total SM Elapsed Cycles          cycle    1,277,840
    Average SMSP Active Cycles       cycle       531.16
    Total SMSP Elapsed Cycles        cycle    5,111,360
    -------------------------- ----------- ------------

  void at::vectorized_elementwise_kernel<4, at::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3) (6, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.56
    SM Frequency                    Ghz         1.13
    Elapsed Cycles                cycle        3,989
    Memory Throughput                 %         0.86
    DRAM Throughput                   %         0.77
    Duration                         us         3.52
    L1/TEX Cache Throughput           %        12.52
    L2 Cache Throughput               %         1.32
    SM Active Cycles              cycle       102.94
    Compute (SM) Throughput           %         0.05
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      6
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             768
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 94.44%                                                                                          
          The grid for this launch is configured to execute only 6 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         5.98
    Achieved Active Warps Per SM           warp         3.82
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.02%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.0%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        42.50
    Total DRAM Elapsed Cycles        cycle      219,776
    Average L1 Active Cycles         cycle       102.94
    Total L1 Elapsed Cycles          cycle      418,520
    Average L2 Active Cycles         cycle       672.69
    Total L2 Elapsed Cycles          cycle      302,400
    Average SM Active Cycles         cycle       102.94
    Total SM Elapsed Cycles          cycle      418,520
    Average SMSP Active Cycles       cycle        97.65
    Total SMSP Elapsed Cycles        cycle    1,674,080
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.57%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 65.04% above the average, while the minimum instance value is 88.40% below the      
          average.                                                                                                      

  void at::<unnamed>::vectorized_layer_norm_kernel<float, float, 0>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *) (8, 1, 1)x(32, 4, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        8,348
    Memory Throughput                 %         0.71
    DRAM Throughput                   %         0.38
    Duration                         us         7.30
    L1/TEX Cache Throughput           %         6.15
    L2 Cache Throughput               %         0.79
    SM Active Cycles              cycle       466.14
    Compute (SM) Throughput           %         0.45
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              38
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block              24
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 92.59%                                                                                          
          The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           28
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %           75
    Achieved Occupancy                        %         6.09
    Achieved Active Warps Per SM           warp         3.90
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.88%                                                                                    
          The difference between calculated theoretical (75.0%) and measured achieved occupancy (6.1%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 25%                                                                                       
          The 12.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the      
          hardware maximum of 16. This kernel's theoretical occupancy (75.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        43.50
    Total DRAM Elapsed Cycles        cycle      460,672
    Average L1 Active Cycles         cycle       466.14
    Total L1 Elapsed Cycles          cycle      920,224
    Average L2 Active Cycles         cycle       816.88
    Total L2 Elapsed Cycles          cycle      632,800
    Average SM Active Cycles         cycle       466.14
    Total SM Elapsed Cycles          cycle      920,224
    Average SMSP Active Cycles       cycle       439.01
    Total SMSP Elapsed Cycles        cycle    3,680,896
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.069%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 92.66% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.069%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 92.66% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.2%                                                                                            
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 60.04% above the average, while the minimum instance value is 55.56% below the      
          average.                                                                                                      

  triton_linear_kernel (1, 12, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle       20,351
    Memory Throughput                 %         6.68
    DRAM Throughput                   %         6.68
    Duration                         us        17.66
    L1/TEX Cache Throughput           %        35.99
    L2 Cache Throughput               %         6.48
    SM Active Cycles              cycle     1,994.08
    Compute (SM) Throughput           %         1.68
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.24
    Achieved Active Warps Per SM           warp         3.99
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.36%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,871.50
    Total DRAM Elapsed Cycles        cycle    1,121,280
    Average L1 Active Cycles         cycle     1,994.08
    Total L1 Elapsed Cycles          cycle    2,188,838
    Average L2 Active Cycles         cycle    14,759.96
    Total L2 Elapsed Cycles          cycle    1,542,880
    Average SM Active Cycles         cycle     1,994.08
    Total SM Elapsed Cycles          cycle    2,188,838
    Average SMSP Active Cycles       cycle     1,975.29
    Total SMSP Elapsed Cycles        cycle    8,755,352
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.767%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.10% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.685%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.11% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.767%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.10% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  void at::vectorized_elementwise_kernel<4, at::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3) (6, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.57
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        3,720
    Memory Throughput                 %         0.77
    DRAM Throughput                   %         0.45
    Duration                         us         3.26
    L1/TEX Cache Throughput           %        13.21
    L2 Cache Throughput               %         0.99
    SM Active Cycles              cycle        90.82
    Compute (SM) Throughput           %         0.05
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      6
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             768
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 94.44%                                                                                          
          The grid for this launch is configured to execute only 6 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         6.02
    Achieved Active Warps Per SM           warp         3.85
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.98%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.0%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        22.90
    Total DRAM Elapsed Cycles        cycle      204,800
    Average L1 Active Cycles         cycle        90.82
    Total L1 Elapsed Cycles          cycle      402,542
    Average L2 Active Cycles         cycle       553.92
    Total L2 Elapsed Cycles          cycle      281,920
    Average SM Active Cycles         cycle        90.82
    Total SM Elapsed Cycles          cycle      402,542
    Average SMSP Active Cycles       cycle        90.32
    Total SMSP Elapsed Cycles        cycle    1,610,168
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 9.734%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 61.93% above the average, while the minimum instance value is 86.46% below the      
          average.                                                                                                      

  triton_linear_kernel (1, 12, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle       20,399
    Memory Throughput                 %         6.64
    DRAM Throughput                   %         6.64
    Duration                         us        17.76
    L1/TEX Cache Throughput           %        36.39
    L2 Cache Throughput               %         6.40
    SM Active Cycles              cycle     1,975.14
    Compute (SM) Throughput           %         1.69
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.23
    Achieved Active Warps Per SM           warp         3.99
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.39%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,871.50
    Total DRAM Elapsed Cycles        cycle    1,127,424
    Average L1 Active Cycles         cycle     1,975.14
    Total L1 Elapsed Cycles          cycle    2,179,006
    Average L2 Active Cycles         cycle    14,782.52
    Total L2 Elapsed Cycles          cycle    1,546,080
    Average SM Active Cycles         cycle     1,975.14
    Total SM Elapsed Cycles          cycle    2,179,006
    Average SMSP Active Cycles       cycle     1,979.17
    Total SMSP Elapsed Cycles        cycle    8,716,024
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.724%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.11% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.747%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.17% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.724%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.11% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  triton_linear_kernel (1, 12, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle       20,386
    Memory Throughput                 %         6.65
    DRAM Throughput                   %         6.65
    Duration                         us        17.73
    L1/TEX Cache Throughput           %        36.33
    L2 Cache Throughput               %         6.43
    SM Active Cycles              cycle     1,975.08
    Compute (SM) Throughput           %         1.68
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.29
    Achieved Active Warps Per SM           warp         4.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.21%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.3%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,871.50
    Total DRAM Elapsed Cycles        cycle    1,125,888
    Average L1 Active Cycles         cycle     1,975.08
    Total L1 Elapsed Cycles          cycle    2,189,194
    Average L2 Active Cycles         cycle    14,749.69
    Total L2 Elapsed Cycles          cycle    1,543,120
    Average SM Active Cycles         cycle     1,975.08
    Total SM Elapsed Cycles          cycle    2,189,194
    Average SMSP Active Cycles       cycle     1,978.35
    Total SMSP Elapsed Cycles        cycle    8,756,776
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.686%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.15% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.71%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.24% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.686%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.15% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  void at::<unnamed>::CatArrayBatchedCopy<at::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, at::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, at::<unnamed>::TensorSizeStride<T2, 4>, int, T2) (216, 2, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        7,514
    Memory Throughput                 %         0.92
    DRAM Throughput                   %         0.24
    Duration                         us         6.56
    L1/TEX Cache Throughput           %         3.85
    L2 Cache Throughput               %         0.60
    SM Active Cycles              cycle        1,764
    Compute (SM) Throughput           %         5.11
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    432
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread         221,184
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        68.90
    Achieved Active Warps Per SM           warp        44.10
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 31.1%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (68.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24.70
    Total DRAM Elapsed Cycles        cycle      414,720
    Average L1 Active Cycles         cycle        1,764
    Total L1 Elapsed Cycles          cycle      796,100
    Average L2 Active Cycles         cycle       793.46
    Total L2 Elapsed Cycles          cycle      569,280
    Average SM Active Cycles         cycle        1,764
    Total SM Elapsed Cycles          cycle      796,100
    Average SMSP Active Cycles       cycle     1,710.36
    Total SMSP Elapsed Cycles        cycle    3,184,400
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 16.27%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 67.97% above the average, while the minimum instance value is 36.05% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 15.98%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 68.89% above the average, while the minimum instance value is 42.35% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 16.27%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 67.97% above the average, while the minimum instance value is 36.05% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.432%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 57.68% above the average, while the minimum instance value is 90.55% below the      
          average.                                                                                                      

  void at::<unnamed>::CatArrayBatchedCopy<at::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, at::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, at::<unnamed>::TensorSizeStride<T2, 4>, int, T2) (216, 2, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.57
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        7,674
    Memory Throughput                 %         0.92
    DRAM Throughput                   %         0.23
    Duration                         us         6.72
    L1/TEX Cache Throughput           %         3.84
    L2 Cache Throughput               %         0.53
    SM Active Cycles              cycle     1,768.81
    Compute (SM) Throughput           %         5.12
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    432
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread         221,184
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        68.36
    Achieved Active Warps Per SM           warp        43.75
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 31.64%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (68.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24.60
    Total DRAM Elapsed Cycles        cycle      422,912
    Average L1 Active Cycles         cycle     1,768.81
    Total L1 Elapsed Cycles          cycle      795,026
    Average L2 Active Cycles         cycle       765.96
    Total L2 Elapsed Cycles          cycle      581,920
    Average SM Active Cycles         cycle     1,768.81
    Total SM Elapsed Cycles          cycle      795,026
    Average SMSP Active Cycles       cycle     1,716.05
    Total SMSP Elapsed Cycles        cycle    3,180,104
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 16.5%                                                                                           
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 68.68% above the average, while the minimum instance value is 36.23% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 15.8%                                                                                           
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 67.79% above the average, while the minimum instance value is 42.31% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 16.5%                                                                                           
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 68.68% above the average, while the minimum instance value is 36.23% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.827%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 64.83% above the average, while the minimum instance value is 90.21% below the      
          average.                                                                                                      

  fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::Sm80, 1, 64, 64, 64, 1, 1>::Params) (1, 12, 1)x(32, 4, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle       17,684
    Memory Throughput                 %         2.16
    DRAM Throughput                   %         0.45
    Duration                         us        15.36
    L1/TEX Cache Throughput           %        20.94
    L2 Cache Throughput               %         2.42
    SM Active Cycles              cycle     1,698.30
    Compute (SM) Throughput           %         2.33
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread             168
    Shared Memory Configuration Size           Kbyte          135.17
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           36.35
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.04
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %        18.75
    Achieved Occupancy                        %         6.27
    Achieved Active Warps Per SM           warp         4.01
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.57%                                                                                    
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.3%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 81.25%                                                                                    
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (18.8%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       110.10
    Total DRAM Elapsed Cycles        cycle      976,512
    Average L1 Active Cycles         cycle     1,698.30
    Total L1 Elapsed Cycles          cycle    1,904,848
    Average L2 Active Cycles         cycle     4,045.31
    Total L2 Elapsed Cycles          cycle    1,340,640
    Average SM Active Cycles         cycle     1,698.30
    Total SM Elapsed Cycles          cycle    1,904,848
    Average SMSP Active Cycles       cycle     1,708.52
    Total SMSP Elapsed Cycles        cycle    7,619,392
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.579%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.10% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.632%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.11% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.579%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.10% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 9.694%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 40.16% above the average, while the minimum instance value is 64.70% below the      
          average.                                                                                                      

  ampere_sgemm_128x32_tn (6, 1, 13)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.13
    Elapsed Cycles                cycle       13,926
    Memory Throughput                 %        23.75
    DRAM Throughput                   %         9.61
    Duration                         us        12.29
    L1/TEX Cache Throughput           %        41.33
    L2 Cache Throughput               %        13.36
    SM Active Cycles              cycle     7,800.81
    Compute (SM) Throughput           %        22.07
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     78
    Registers Per Thread             register/thread              57
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           16.38
    # SMs                                         SM             108
    Threads                                   thread          19,968
    Uses Green Context                                             0
    Waves Per SM                                                0.18
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 27.78%                                                                                          
          The grid for this launch is configured to execute only 78 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            5
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        12.35
    Achieved Active Warps Per SM           warp         7.90
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75.3%                                                                                     
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (12.4%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,872.20
    Total DRAM Elapsed Cycles        cycle      779,264
    Average L1 Active Cycles         cycle     7,800.81
    Total L1 Elapsed Cycles          cycle    1,466,570
    Average L2 Active Cycles         cycle     8,665.45
    Total L2 Elapsed Cycles          cycle    1,061,120
    Average SM Active Cycles         cycle     7,800.81
    Total SM Elapsed Cycles          cycle    1,466,570
    Average SMSP Active Cycles       cycle     7,709.71
    Total SMSP Elapsed Cycles        cycle    5,866,280
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 18.83%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 32.79% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.51%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 32.60% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.83%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 32.79% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.714%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L2 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 8.75% above the average, while the minimum instance value is 9.60% below    
          the average.                                                                                                  

  void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T9 *, T8 *, T5 *, const T6 *, const T6 *, const T10 *, const T4 *, T10 *, void *, long, T6 *, int *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *) (24, 1, 1)x(32, 16, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        6,310
    Memory Throughput                 %         2.97
    DRAM Throughput                   %         2.97
    Duration                         us         5.50
    L1/TEX Cache Throughput           %         5.99
    L2 Cache Throughput               %         4.07
    SM Active Cycles              cycle       879.88
    Compute (SM) Throughput           %         1.78
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     24
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          12,288
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 77.78%                                                                                          
          The grid for this launch is configured to execute only 24 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        17.63
    Achieved Active Warps Per SM           warp        11.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 82.37%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (17.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       259.40
    Total DRAM Elapsed Cycles        cycle      348,928
    Average L1 Active Cycles         cycle       879.88
    Total L1 Elapsed Cycles          cycle      684,114
    Average L2 Active Cycles         cycle     2,460.44
    Total L2 Elapsed Cycles          cycle      478,000
    Average SM Active Cycles         cycle       879.88
    Total SM Elapsed Cycles          cycle      684,114
    Average SMSP Active Cycles       cycle       904.56
    Total SMSP Elapsed Cycles        cycle    2,736,456
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 10.98%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 79.02% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.34%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 79.42% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.98%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 79.02% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.065%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L2 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 17.16% above the average, while the minimum instance value is 18.75% below  
          the average.                                                                                                  

  void at::vectorized_elementwise_kernel<4, at::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3) (6, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.56
    SM Frequency                    Ghz         1.13
    Elapsed Cycles                cycle        3,808
    Memory Throughput                 %         0.87
    DRAM Throughput                   %         0.81
    Duration                         us         3.36
    L1/TEX Cache Throughput           %        13.55
    L2 Cache Throughput               %         1.01
    SM Active Cycles              cycle        95.13
    Compute (SM) Throughput           %         0.05
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      6
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             768
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 94.44%                                                                                          
          The grid for this launch is configured to execute only 6 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         6.30
    Achieved Active Warps Per SM           warp         4.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.7%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.3%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        42.50
    Total DRAM Elapsed Cycles        cycle      209,920
    Average L1 Active Cycles         cycle        95.13
    Total L1 Elapsed Cycles          cycle      420,986
    Average L2 Active Cycles         cycle       664.14
    Total L2 Elapsed Cycles          cycle      288,720
    Average SM Active Cycles         cycle        95.13
    Total SM Elapsed Cycles          cycle      420,986
    Average SMSP Active Cycles       cycle        92.97
    Total SMSP Elapsed Cycles        cycle    1,683,944
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 10.08%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 54.76% above the average, while the minimum instance value is 85.70% below the      
          average.                                                                                                      

  void at::<unnamed>::vectorized_layer_norm_kernel<float, float, 0>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *) (8, 1, 1)x(32, 4, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        8,710
    Memory Throughput                 %         0.68
    DRAM Throughput                   %         0.36
    Duration                         us         7.62
    L1/TEX Cache Throughput           %         5.84
    L2 Cache Throughput               %         0.80
    SM Active Cycles              cycle       490.37
    Compute (SM) Throughput           %         0.44
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              38
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block              24
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 92.59%                                                                                          
          The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           28
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %           75
    Achieved Occupancy                        %         6.01
    Achieved Active Warps Per SM           warp         3.85
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.99%                                                                                    
          The difference between calculated theoretical (75.0%) and measured achieved occupancy (6.0%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 25%                                                                                       
          The 12.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the      
          hardware maximum of 16. This kernel's theoretical occupancy (75.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        43.50
    Total DRAM Elapsed Cycles        cycle      481,280
    Average L1 Active Cycles         cycle       490.37
    Total L1 Elapsed Cycles          cycle      955,438
    Average L2 Active Cycles         cycle     1,059.67
    Total L2 Elapsed Cycles          cycle      660,480
    Average SM Active Cycles         cycle       490.37
    Total SM Elapsed Cycles          cycle      955,438
    Average SMSP Active Cycles       cycle       457.74
    Total SMSP Elapsed Cycles        cycle    3,821,752
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.135%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 92.64% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.135%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 92.64% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.871%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 45.74% above the average, while the minimum instance value is 63.76% below the      
          average.                                                                                                      

  triton_linear_kernel (1, 48, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle       21,367
    Memory Throughput                 %        24.90
    DRAM Throughput                   %        24.90
    Duration                         us        18.72
    L1/TEX Cache Throughput           %        35.27
    L2 Cache Throughput               %        23.13
    SM Active Cycles              cycle     8,158.14
    Compute (SM) Throughput           %         6.40
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     48
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           6,144
    Uses Green Context                                             0
    Waves Per SM                                                0.07
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 55.56%                                                                                          
          The grid for this launch is configured to execute only 48 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.24
    Achieved Active Warps Per SM           warp         4.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.35%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     7,408.30
    Total DRAM Elapsed Cycles        cycle    1,189,888
    Average L1 Active Cycles         cycle     8,158.14
    Total L1 Elapsed Cycles          cycle    2,305,378
    Average L2 Active Cycles         cycle    16,513.45
    Total L2 Elapsed Cycles          cycle    1,621,120
    Average SM Active Cycles         cycle     8,158.14
    Total SM Elapsed Cycles          cycle    2,305,378
    Average SMSP Active Cycles       cycle     8,291.63
    Total SMSP Elapsed Cycles        cycle    9,221,512
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 21.9%                                                                                           
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 57.30% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.27%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 57.34% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.9%                                                                                           
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 57.30% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  void at::vectorized_elementwise_kernel<4, at::<unnamed>::launch_clamp_scalar(at::TensorIteratorBase &, c10::Scalar, c10::Scalar, detail::ClampLimits)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3) (24, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        4,114
    Memory Throughput                 %         1.62
    DRAM Throughput                   %         1.46
    Duration                         us         3.62
    L1/TEX Cache Throughput           %         8.91
    L2 Cache Throughput               %         3.17
    SM Active Cycles              cycle       403.96
    Compute (SM) Throughput           %         0.25
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     24
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           3,072
    Uses Green Context                                             0
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 77.78%                                                                                          
          The grid for this launch is configured to execute only 24 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         6.13
    Achieved Active Warps Per SM           warp         3.92
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.87%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        83.10
    Total DRAM Elapsed Cycles        cycle      228,096
    Average L1 Active Cycles         cycle       403.96
    Total L1 Elapsed Cycles          cycle      431,330
    Average L2 Active Cycles         cycle     1,155.71
    Total L2 Elapsed Cycles          cycle      312,080
    Average SM Active Cycles         cycle       403.96
    Total SM Elapsed Cycles          cycle      431,330
    Average SMSP Active Cycles       cycle       406.38
    Total SMSP Elapsed Cycles        cycle    1,725,320
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.078%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 79.86% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.125%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 79.85% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.078%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 79.86% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.78%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 39.78% above the average, while the minimum instance value is 51.03% below the      
          average.                                                                                                      

  ampere_sgemm_128x32_sliced1x4_tn (6, 1, 9)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle       36,863
    Memory Throughput                 %        24.09
    DRAM Throughput                   %        14.55
    Duration                         us        32.26
    L1/TEX Cache Throughput           %        52.18
    L2 Cache Throughput               %        14.37
    SM Active Cycles              cycle    16,957.46
    Compute (SM) Throughput           %        31.38
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.5 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     54
    Registers Per Thread             register/thread             134
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.01
    # SMs                                         SM             108
    Threads                                   thread          13,824
    Uses Green Context                                             0
    Waves Per SM                                                0.50
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          The grid for this launch is configured to execute only 54 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        12.50
    Achieved Occupancy                        %        12.51
    Achieved Active Warps Per SM           warp         8.01
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 87.5%                                                                                     
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     7,467.40
    Total DRAM Elapsed Cycles        cycle    2,052,608
    Average L1 Active Cycles         cycle    16,957.46
    Total L1 Elapsed Cycles          cycle    3,966,962
    Average L2 Active Cycles         cycle    22,574.06
    Total L2 Elapsed Cycles          cycle    2,799,840
    Average SM Active Cycles         cycle    16,957.46
    Total SM Elapsed Cycles          cycle    3,966,962
    Average SMSP Active Cycles       cycle    16,945.17
    Total SMSP Elapsed Cycles        cycle   15,867,848
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.45%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 50.80% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.47%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 50.88% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.45%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 50.80% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T9 *, T8 *, T5 *, const T6 *, const T6 *, const T10 *, const T4 *, T10 *, void *, long, T6 *, int *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *) (24, 1, 1)x(32, 16, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.57
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        5,807
    Memory Throughput                 %         2.29
    DRAM Throughput                   %         2.29
    Duration                         us         5.09
    L1/TEX Cache Throughput           %         6.01
    L2 Cache Throughput               %         3.42
    SM Active Cycles              cycle       755.77
    Compute (SM) Throughput           %         1.93
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     24
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          12,288
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 77.78%                                                                                          
          The grid for this launch is configured to execute only 24 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        17.75
    Achieved Active Warps Per SM           warp        11.36
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 82.25%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (17.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       182.60
    Total DRAM Elapsed Cycles        cycle      318,976
    Average L1 Active Cycles         cycle       755.77
    Total L1 Elapsed Cycles          cycle      585,466
    Average L2 Active Cycles         cycle     1,757.54
    Total L2 Elapsed Cycles          cycle      440,160
    Average SM Active Cycles         cycle       755.77
    Total SM Elapsed Cycles          cycle      585,466
    Average SMSP Active Cycles       cycle       735.90
    Total SMSP Elapsed Cycles        cycle    2,341,864
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.14%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 79.89% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.91%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 80.39% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.14%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 79.89% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.09%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 31.59% above the average, while the minimum instance value is 41.45% below the      
          average.                                                                                                      

  void at::vectorized_elementwise_kernel<4, at::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3) (6, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.57
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        3,979
    Memory Throughput                 %         0.89
    DRAM Throughput                   %         0.78
    Duration                         us         3.49
    L1/TEX Cache Throughput           %        12.81
    L2 Cache Throughput               %         1.46
    SM Active Cycles              cycle       100.59
    Compute (SM) Throughput           %         0.05
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      6
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             768
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 94.44%                                                                                          
          The grid for this launch is configured to execute only 6 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         6.35
    Achieved Active Warps Per SM           warp         4.06
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.65%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.3%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        42.50
    Total DRAM Elapsed Cycles        cycle      218,624
    Average L1 Active Cycles         cycle       100.59
    Total L1 Elapsed Cycles          cycle      426,360
    Average L2 Active Cycles         cycle       866.35
    Total L2 Elapsed Cycles          cycle      301,600
    Average SM Active Cycles         cycle       100.59
    Total SM Elapsed Cycles          cycle      426,360
    Average SMSP Active Cycles       cycle        95.28
    Total SMSP Elapsed Cycles        cycle    1,705,440
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.141%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 22.37% above the average, while the minimum instance value is 54.06% below the      
          average.                                                                                                      

  void at::<unnamed>::vectorized_layer_norm_kernel<float, float, 0>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *) (8, 1, 1)x(32, 4, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.57
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle        8,581
    Memory Throughput                 %         0.69
    DRAM Throughput                   %         0.37
    Duration                         us         7.49
    L1/TEX Cache Throughput           %         5.97
    L2 Cache Throughput               %         0.80
    SM Active Cycles              cycle       480.04
    Compute (SM) Throughput           %         0.46
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              38
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block              24
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 92.59%                                                                                          
          The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           28
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %           75
    Achieved Occupancy                        %         5.88
    Achieved Active Warps Per SM           warp         3.76
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 92.17%                                                                                    
          The difference between calculated theoretical (75.0%) and measured achieved occupancy (5.9%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 25%                                                                                       
          The 12.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the      
          hardware maximum of 16. This kernel's theoretical occupancy (75.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        43.50
    Total DRAM Elapsed Cycles        cycle      471,552
    Average L1 Active Cycles         cycle       480.04
    Total L1 Elapsed Cycles          cycle      918,212
    Average L2 Active Cycles         cycle     1,017.86
    Total L2 Elapsed Cycles          cycle      650,720
    Average SM Active Cycles         cycle       480.04
    Total SM Elapsed Cycles          cycle      918,212
    Average SMSP Active Cycles       cycle       465.09
    Total SMSP Elapsed Cycles        cycle    3,672,848
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.232%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 92.66% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.088%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 93.01% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.232%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 92.66% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.807%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 54.40% above the average, while the minimum instance value is 92.63% below the      
          average.                                                                                                      

  triton_linear_kernel (1, 12, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle       20,358
    Memory Throughput                 %         6.67
    DRAM Throughput                   %         6.67
    Duration                         us        17.70
    L1/TEX Cache Throughput           %        36.15
    L2 Cache Throughput               %         6.38
    SM Active Cycles              cycle     1,982.56
    Compute (SM) Throughput           %         1.68
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.19
    Achieved Active Warps Per SM           warp         3.96
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.5%                                                                                     
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,873.50
    Total DRAM Elapsed Cycles        cycle    1,124,352
    Average L1 Active Cycles         cycle     1,982.56
    Total L1 Elapsed Cycles          cycle    2,188,752
    Average L2 Active Cycles         cycle    14,714.41
    Total L2 Elapsed Cycles          cycle    1,541,680
    Average SM Active Cycles         cycle     1,982.56
    Total SM Elapsed Cycles          cycle    2,188,752
    Average SMSP Active Cycles       cycle     1,976.19
    Total SMSP Elapsed Cycles        cycle    8,755,008
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.719%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.12% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.69%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.12% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.719%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.12% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  void at::vectorized_elementwise_kernel<4, at::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3) (6, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        4,002
    Memory Throughput                 %         0.71
    DRAM Throughput                   %         0.41
    Duration                         us         3.52
    L1/TEX Cache Throughput           %        12.25
    L2 Cache Throughput               %         0.88
    SM Active Cycles              cycle        97.95
    Compute (SM) Throughput           %         0.05
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      6
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             768
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 94.44%                                                                                          
          The grid for this launch is configured to execute only 6 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         5.59
    Achieved Active Warps Per SM           warp         3.58
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.41%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (5.6%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        22.90
    Total DRAM Elapsed Cycles        cycle      222,208
    Average L1 Active Cycles         cycle        97.95
    Total L1 Elapsed Cycles          cycle      400,636
    Average L2 Active Cycles         cycle       482.60
    Total L2 Elapsed Cycles          cycle      303,680
    Average SM Active Cycles         cycle        97.95
    Total SM Elapsed Cycles          cycle      400,636
    Average SMSP Active Cycles       cycle        86.94
    Total SMSP Elapsed Cycles        cycle    1,602,544
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.729%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 60.80% above the average, while the minimum instance value is 83.84% below the      
          average.                                                                                                      

  triton_linear_kernel (1, 12, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle       20,540
    Memory Throughput                 %         6.60
    DRAM Throughput                   %         6.60
    Duration                         us        17.86
    L1/TEX Cache Throughput           %        35.96
    L2 Cache Throughput               %         6.36
    SM Active Cycles              cycle     1,993.26
    Compute (SM) Throughput           %         1.67
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.19
    Achieved Active Warps Per SM           warp         3.96
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.49%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,871.40
    Total DRAM Elapsed Cycles        cycle    1,134,080
    Average L1 Active Cycles         cycle     1,993.26
    Total L1 Elapsed Cycles          cycle    2,205,062
    Average L2 Active Cycles         cycle    14,965.08
    Total L2 Elapsed Cycles          cycle    1,556,480
    Average SM Active Cycles         cycle     1,993.26
    Total SM Elapsed Cycles          cycle    2,205,062
    Average SMSP Active Cycles       cycle     1,973.36
    Total SMSP Elapsed Cycles        cycle    8,820,248
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.698%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.09% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.638%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.37% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.698%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.09% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  triton_linear_kernel (1, 12, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle       20,386
    Memory Throughput                 %         6.65
    DRAM Throughput                   %         6.65
    Duration                         us        17.73
    L1/TEX Cache Throughput           %        36.38
    L2 Cache Throughput               %         6.35
    SM Active Cycles              cycle     1,968.25
    Compute (SM) Throughput           %         1.69
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.30
    Achieved Active Warps Per SM           warp         4.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.21%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.3%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,871.40
    Total DRAM Elapsed Cycles        cycle    1,126,400
    Average L1 Active Cycles         cycle     1,968.25
    Total L1 Elapsed Cycles          cycle    2,182,320
    Average L2 Active Cycles         cycle    14,733.29
    Total L2 Elapsed Cycles          cycle    1,542,640
    Average SM Active Cycles         cycle     1,968.25
    Total SM Elapsed Cycles          cycle    2,182,320
    Average SMSP Active Cycles       cycle     1,975.06
    Total SMSP Elapsed Cycles        cycle    8,729,280
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.685%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.16% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.726%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.27% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.685%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.16% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  void at::<unnamed>::CatArrayBatchedCopy<at::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, at::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, at::<unnamed>::TensorSizeStride<T2, 4>, int, T2) (216, 2, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        7,480
    Memory Throughput                 %         0.90
    DRAM Throughput                   %         0.24
    Duration                         us         6.53
    L1/TEX Cache Throughput           %         3.88
    L2 Cache Throughput               %         0.54
    SM Active Cycles              cycle     1,756.75
    Compute (SM) Throughput           %         4.98
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    432
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread         221,184
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        82.27
    Achieved Active Warps Per SM           warp        52.65
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 17.73%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24.60
    Total DRAM Elapsed Cycles        cycle      413,184
    Average L1 Active Cycles         cycle     1,756.75
    Total L1 Elapsed Cycles          cycle      817,272
    Average L2 Active Cycles         cycle       887.56
    Total L2 Elapsed Cycles          cycle      566,960
    Average SM Active Cycles         cycle     1,756.75
    Total SM Elapsed Cycles          cycle      817,272
    Average SMSP Active Cycles       cycle     1,795.15
    Total SMSP Elapsed Cycles        cycle    3,269,088
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 15.83%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 68.17% above the average, while the minimum instance value is 36.13% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 16.08%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 67.77% above the average, while the minimum instance value is 45.07% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 15.83%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 68.17% above the average, while the minimum instance value is 36.13% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.22%                                                                                           
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 41.68% above the average, while the minimum instance value is 88.85% below the      
          average.                                                                                                      

  void at::<unnamed>::CatArrayBatchedCopy<at::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, at::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, at::<unnamed>::TensorSizeStride<T2, 4>, int, T2) (216, 2, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        7,611
    Memory Throughput                 %         0.92
    DRAM Throughput                   %         0.23
    Duration                         us         6.66
    L1/TEX Cache Throughput           %         3.85
    L2 Cache Throughput               %         0.63
    SM Active Cycles              cycle     1,764.44
    Compute (SM) Throughput           %         5.13
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    432
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread         221,184
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        68.36
    Achieved Active Warps Per SM           warp        43.75
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 31.64%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (68.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24.60
    Total DRAM Elapsed Cycles        cycle      420,352
    Average L1 Active Cycles         cycle     1,764.44
    Total L1 Elapsed Cycles          cycle      792,952
    Average L2 Active Cycles         cycle       898.26
    Total L2 Elapsed Cycles          cycle      577,040
    Average SM Active Cycles         cycle     1,764.44
    Total SM Elapsed Cycles          cycle      792,952
    Average SMSP Active Cycles       cycle     1,709.62
    Total SMSP Elapsed Cycles        cycle    3,171,808
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 16.48%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 68.59% above the average, while the minimum instance value is 36.35% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 15.83%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 67.97% above the average, while the minimum instance value is 43.38% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 16.48%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 68.59% above the average, while the minimum instance value is 36.35% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.541%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 60.55% above the average, while the minimum instance value is 91.65% below the      
          average.                                                                                                      

  fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::Sm80, 1, 64, 64, 64, 1, 1>::Params) (1, 12, 1)x(32, 4, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle       18,177
    Memory Throughput                 %         2.10
    DRAM Throughput                   %         0.44
    Duration                         us        15.81
    L1/TEX Cache Throughput           %        20.61
    L2 Cache Throughput               %         2.36
    SM Active Cycles              cycle     1,724.61
    Compute (SM) Throughput           %         2.31
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread             168
    Shared Memory Configuration Size           Kbyte          135.17
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           36.35
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.04
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %        18.75
    Achieved Occupancy                        %         6.25
    Achieved Active Warps Per SM           warp         4.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.68%                                                                                    
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 81.25%                                                                                    
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (18.8%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       110.10
    Total DRAM Elapsed Cycles        cycle    1,003,520
    Average L1 Active Cycles         cycle     1,724.61
    Total L1 Elapsed Cycles          cycle    1,924,654
    Average L2 Active Cycles         cycle     4,263.65
    Total L2 Elapsed Cycles          cycle    1,377,760
    Average SM Active Cycles         cycle     1,724.61
    Total SM Elapsed Cycles          cycle    1,924,654
    Average SMSP Active Cycles       cycle     1,732.20
    Total SMSP Elapsed Cycles        cycle    7,698,616
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.64%                                                                                           
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.28% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.676%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.26% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.64%                                                                                           
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.28% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.965%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L2 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 36.21% above the average, while the minimum instance value is 34.47% below  
          the average.                                                                                                  

  ampere_sgemm_128x32_tn (6, 1, 13)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.13
    Elapsed Cycles                cycle       13,791
    Memory Throughput                 %        23.48
    DRAM Throughput                   %         9.69
    Duration                         us        12.19
    L1/TEX Cache Throughput           %        41.72
    L2 Cache Throughput               %        13.53
    SM Active Cycles              cycle     7,728.21
    Compute (SM) Throughput           %        21.82
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     78
    Registers Per Thread             register/thread              57
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           16.38
    # SMs                                         SM             108
    Threads                                   thread          19,968
    Uses Green Context                                             0
    Waves Per SM                                                0.18
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 27.78%                                                                                          
          The grid for this launch is configured to execute only 78 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            5
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        12.43
    Achieved Active Warps Per SM           warp         7.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75.14%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (12.4%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,872.20
    Total DRAM Elapsed Cycles        cycle      773,120
    Average L1 Active Cycles         cycle     7,728.21
    Total L1 Elapsed Cycles          cycle    1,483,332
    Average L2 Active Cycles         cycle     8,835.12
    Total L2 Elapsed Cycles          cycle    1,050,640
    Average SM Active Cycles         cycle     7,728.21
    Total SM Elapsed Cycles          cycle    1,483,332
    Average SMSP Active Cycles       cycle     7,698.42
    Total SMSP Elapsed Cycles        cycle    5,933,328
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 18.59%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 33.04% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.95%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 33.81% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.59%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 33.04% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.036%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L2 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 7.49% above the average, while the minimum instance value is 6.86% below    
          the average.                                                                                                  

  void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T9 *, T8 *, T5 *, const T6 *, const T6 *, const T10 *, const T4 *, T10 *, void *, long, T6 *, int *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *) (24, 1, 1)x(32, 16, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        6,297
    Memory Throughput                 %         2.98
    DRAM Throughput                   %         2.98
    Duration                         us         5.50
    L1/TEX Cache Throughput           %         6.01
    L2 Cache Throughput               %         4.07
    SM Active Cycles              cycle       875.22
    Compute (SM) Throughput           %         1.81
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     24
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          12,288
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 77.78%                                                                                          
          The grid for this launch is configured to execute only 24 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        17.43
    Achieved Active Warps Per SM           warp        11.16
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 82.57%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (17.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       259.40
    Total DRAM Elapsed Cycles        cycle      348,160
    Average L1 Active Cycles         cycle       875.22
    Total L1 Elapsed Cycles          cycle      674,900
    Average L2 Active Cycles         cycle     2,430.44
    Total L2 Elapsed Cycles          cycle      477,280
    Average SM Active Cycles         cycle       875.22
    Total SM Elapsed Cycles          cycle      674,900
    Average SMSP Active Cycles       cycle       904.33
    Total SMSP Elapsed Cycles        cycle    2,699,600
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.06%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.98% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.48%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 79.30% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.06%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.98% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.233%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L2 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 20.21% above the average, while the minimum instance value is 20.22% below  
          the average.                                                                                                  

  void at::vectorized_elementwise_kernel<4, at::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3) (6, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.13
    Elapsed Cycles                cycle        4,111
    Memory Throughput                 %         0.81
    DRAM Throughput                   %         0.75
    Duration                         us         3.62
    L1/TEX Cache Throughput           %        11.56
    L2 Cache Throughput               %         1.02
    SM Active Cycles              cycle       111.46
    Compute (SM) Throughput           %         0.05
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      6
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             768
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 94.44%                                                                                          
          The grid for this launch is configured to execute only 6 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         5.65
    Achieved Active Warps Per SM           warp         3.62
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.35%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (5.7%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        42.50
    Total DRAM Elapsed Cycles        cycle      227,840
    Average L1 Active Cycles         cycle       111.46
    Total L1 Elapsed Cycles          cycle      427,394
    Average L2 Active Cycles         cycle       658.62
    Total L2 Elapsed Cycles          cycle      311,760
    Average SM Active Cycles         cycle       111.46
    Total SM Elapsed Cycles          cycle      427,394
    Average SMSP Active Cycles       cycle        99.87
    Total SMSP Elapsed Cycles        cycle    1,709,576
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.515%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 44.47% above the average, while the minimum instance value is 88.61% below the      
          average.                                                                                                      

  void at::<unnamed>::vectorized_layer_norm_kernel<float, float, 0>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *) (8, 1, 1)x(32, 4, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        8,466
    Memory Throughput                 %         0.69
    DRAM Throughput                   %         0.37
    Duration                         us         7.39
    L1/TEX Cache Throughput           %         6.06
    L2 Cache Throughput               %         0.69
    SM Active Cycles              cycle       473.75
    Compute (SM) Throughput           %         0.46
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              38
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block              24
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 92.59%                                                                                          
          The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           28
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %           75
    Achieved Occupancy                        %         5.85
    Achieved Active Warps Per SM           warp         3.74
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 92.2%                                                                                     
          The difference between calculated theoretical (75.0%) and measured achieved occupancy (5.8%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 25%                                                                                       
          The 12.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the      
          hardware maximum of 16. This kernel's theoretical occupancy (75.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        43.50
    Total DRAM Elapsed Cycles        cycle      468,224
    Average L1 Active Cycles         cycle       473.75
    Total L1 Elapsed Cycles          cycle      910,476
    Average L2 Active Cycles         cycle       887.74
    Total L2 Elapsed Cycles          cycle      641,840
    Average SM Active Cycles         cycle       473.75
    Total SM Elapsed Cycles          cycle      910,476
    Average SMSP Active Cycles       cycle       445.54
    Total SMSP Elapsed Cycles        cycle    3,641,904
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.208%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 92.67% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.208%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 92.67% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.331%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 57.22% above the average, while the minimum instance value is 72.63% below the      
          average.                                                                                                      

  triton_linear_kernel (1, 48, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle       21,611
    Memory Throughput                 %        24.62
    DRAM Throughput                   %        24.62
    Duration                         us        18.94
    L1/TEX Cache Throughput           %        35.20
    L2 Cache Throughput               %        22.86
    SM Active Cycles              cycle     8,185.78
    Compute (SM) Throughput           %         6.48
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     48
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           6,144
    Uses Green Context                                             0
    Waves Per SM                                                0.07
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 55.56%                                                                                          
          The grid for this launch is configured to execute only 48 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.25
    Achieved Active Warps Per SM           warp            4
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.33%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     7,408.30
    Total DRAM Elapsed Cycles        cycle    1,203,584
    Average L1 Active Cycles         cycle     8,185.78
    Total L1 Elapsed Cycles          cycle    2,275,976
    Average L2 Active Cycles         cycle    16,451.22
    Total L2 Elapsed Cycles          cycle    1,640,000
    Average SM Active Cycles         cycle     8,185.78
    Total SM Elapsed Cycles          cycle    2,275,976
    Average SMSP Active Cycles       cycle     8,135.58
    Total SMSP Elapsed Cycles        cycle    9,103,904
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 22.5%                                                                                           
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 57.91% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.1%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 57.26% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.5%                                                                                           
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 57.91% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.148%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L2 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 6.42% above the average, while the minimum instance value is 7.16% below    
          the average.                                                                                                  

  void at::vectorized_elementwise_kernel<4, at::<unnamed>::launch_clamp_scalar(at::TensorIteratorBase &, c10::Scalar, c10::Scalar, detail::ClampLimits)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3) (24, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.57
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        4,194
    Memory Throughput                 %         1.59
    DRAM Throughput                   %         1.43
    Duration                         us         3.68
    L1/TEX Cache Throughput           %         8.02
    L2 Cache Throughput               %         3.09
    SM Active Cycles              cycle          449
    Compute (SM) Throughput           %         0.24
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     24
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           3,072
    Uses Green Context                                             0
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 77.78%                                                                                          
          The grid for this launch is configured to execute only 24 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         5.60
    Achieved Active Warps Per SM           warp         3.58
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.4%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (5.6%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        82.50
    Total DRAM Elapsed Cycles        cycle      230,912
    Average L1 Active Cycles         cycle          449
    Total L1 Elapsed Cycles          cycle      454,376
    Average L2 Active Cycles         cycle     1,019.26
    Total L2 Elapsed Cycles          cycle      317,840
    Average SM Active Cycles         cycle          449
    Total SM Elapsed Cycles          cycle      454,376
    Average SMSP Active Cycles       cycle       396.07
    Total SMSP Elapsed Cycles        cycle    1,817,504
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.457%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 79.24% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.608%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 80.81% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.457%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 79.24% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.596%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 29.61% above the average, while the minimum instance value is 27.01% below the      
          average.                                                                                                      

  ampere_sgemm_128x32_sliced1x4_tn (6, 1, 9)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle       36,703
    Memory Throughput                 %        24.14
    DRAM Throughput                   %        14.62
    Duration                         us        32.10
    L1/TEX Cache Throughput           %        52.35
    L2 Cache Throughput               %        14.42
    SM Active Cycles              cycle    16,911.41
    Compute (SM) Throughput           %        31.43
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.5 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     54
    Registers Per Thread             register/thread             134
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.01
    # SMs                                         SM             108
    Threads                                   thread          13,824
    Uses Green Context                                             0
    Waves Per SM                                                0.50
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          The grid for this launch is configured to execute only 54 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        12.50
    Achieved Occupancy                        %        12.49
    Achieved Active Warps Per SM           warp         7.99
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 87.5%                                                                                     
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     7,467.40
    Total DRAM Elapsed Cycles        cycle    2,042,368
    Average L1 Active Cycles         cycle    16,911.41
    Total L1 Elapsed Cycles          cycle    3,960,200
    Average L2 Active Cycles         cycle    23,129.66
    Total L2 Elapsed Cycles          cycle    2,788,160
    Average SM Active Cycles         cycle    16,911.41
    Total SM Elapsed Cycles          cycle    3,960,200
    Average SMSP Active Cycles       cycle    16,901.37
    Total SMSP Elapsed Cycles        cycle   15,840,800
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.53%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 51.02% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.42%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 50.81% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.53%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 51.02% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T9 *, T8 *, T5 *, const T6 *, const T6 *, const T10 *, const T4 *, T10 *, void *, long, T6 *, int *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *) (24, 1, 1)x(32, 16, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        5,478
    Memory Throughput                 %         2.40
    DRAM Throughput                   %         2.40
    Duration                         us         4.80
    L1/TEX Cache Throughput           %         6.51
    L2 Cache Throughput               %         3.67
    SM Active Cycles              cycle       699.52
    Compute (SM) Throughput           %         1.86
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     24
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          12,288
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 77.78%                                                                                          
          The grid for this launch is configured to execute only 24 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        19.50
    Achieved Active Warps Per SM           warp        12.48
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 80.5%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (19.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       182.60
    Total DRAM Elapsed Cycles        cycle      303,744
    Average L1 Active Cycles         cycle       699.52
    Total L1 Elapsed Cycles          cycle      606,394
    Average L2 Active Cycles         cycle     1,857.81
    Total L2 Elapsed Cycles          cycle      414,960
    Average SM Active Cycles         cycle       699.52
    Total SM Elapsed Cycles          cycle      606,394
    Average SMSP Active Cycles       cycle       754.39
    Total SMSP Elapsed Cycles        cycle    2,425,576
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 9.861%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 79.15% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.74%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 79.91% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 9.861%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 79.15% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.234%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L2 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 20.20% above the average, while the minimum instance value is 20.23% below  
          the average.                                                                                                  

  void at::vectorized_elementwise_kernel<4, at::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3) (6, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.57
    SM Frequency                    Ghz         1.13
    Elapsed Cycles                cycle        3,818
    Memory Throughput                 %         0.86
    DRAM Throughput                   %         0.81
    Duration                         us         3.36
    L1/TEX Cache Throughput           %        13.42
    L2 Cache Throughput               %         0.99
    SM Active Cycles              cycle        96.02
    Compute (SM) Throughput           %         0.05
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      6
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             768
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 94.44%                                                                                          
          The grid for this launch is configured to execute only 6 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         6.35
    Achieved Active Warps Per SM           warp         4.07
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.65%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.4%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        42.50
    Total DRAM Elapsed Cycles        cycle      210,432
    Average L1 Active Cycles         cycle        96.02
    Total L1 Elapsed Cycles          cycle      432,832
    Average L2 Active Cycles         cycle       665.61
    Total L2 Elapsed Cycles          cycle      289,280
    Average SM Active Cycles         cycle        96.02
    Total SM Elapsed Cycles          cycle      432,832
    Average SMSP Active Cycles       cycle        93.67
    Total SMSP Elapsed Cycles        cycle    1,731,328
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 10.8%                                                                                           
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 58.66% above the average, while the minimum instance value is 85.73% below the      
          average.                                                                                                      

  void at::<unnamed>::vectorized_layer_norm_kernel<float, float, 0>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *) (8, 1, 1)x(32, 4, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle        8,527
    Memory Throughput                 %         0.70
    DRAM Throughput                   %         0.37
    Duration                         us         7.42
    L1/TEX Cache Throughput           %         6.01
    L2 Cache Throughput               %         0.87
    SM Active Cycles              cycle       476.95
    Compute (SM) Throughput           %         0.45
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              38
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block              24
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 92.59%                                                                                          
          The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           28
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %           75
    Achieved Occupancy                        %         6.06
    Achieved Active Warps Per SM           warp         3.88
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.91%                                                                                    
          The difference between calculated theoretical (75.0%) and measured achieved occupancy (6.1%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 25%                                                                                       
          The 12.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the      
          hardware maximum of 16. This kernel's theoretical occupancy (75.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        43.50
    Total DRAM Elapsed Cycles        cycle      469,504
    Average L1 Active Cycles         cycle       476.95
    Total L1 Elapsed Cycles          cycle      936,648
    Average L2 Active Cycles         cycle     1,012.12
    Total L2 Elapsed Cycles          cycle      646,240
    Average SM Active Cycles         cycle       476.95
    Total SM Elapsed Cycles          cycle      936,648
    Average SMSP Active Cycles       cycle       461.48
    Total SMSP Elapsed Cycles        cycle    3,746,592
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.096%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 92.67% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.096%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 92.67% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.417%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 43.23% above the average, while the minimum instance value is 61.96% below the      
          average.                                                                                                      

  triton_linear_kernel (1, 12, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle       20,615
    Memory Throughput                 %         6.57
    DRAM Throughput                   %         6.57
    Duration                         us        17.92
    L1/TEX Cache Throughput           %        36.26
    L2 Cache Throughput               %         6.28
    SM Active Cycles              cycle     1,978.31
    Compute (SM) Throughput           %         1.68
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.27
    Achieved Active Warps Per SM           warp         4.01
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.27%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.3%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,871.40
    Total DRAM Elapsed Cycles        cycle    1,139,712
    Average L1 Active Cycles         cycle     1,978.31
    Total L1 Elapsed Cycles          cycle    2,199,618
    Average L2 Active Cycles         cycle    14,778.51
    Total L2 Elapsed Cycles          cycle    1,562,400
    Average SM Active Cycles         cycle     1,978.31
    Total SM Elapsed Cycles          cycle    2,199,618
    Average SMSP Active Cycles       cycle     1,971.41
    Total SMSP Elapsed Cycles        cycle    8,798,472
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.67%                                                                                           
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.26% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.634%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.20% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.67%                                                                                           
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.26% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.509%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 7.28% above the average, while the minimum instance value is 3.56% below the        
          average.                                                                                                      

  void at::vectorized_elementwise_kernel<4, at::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3) (6, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.57
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        3,904
    Memory Throughput                 %         0.73
    DRAM Throughput                   %         0.43
    Duration                         us         3.42
    L1/TEX Cache Throughput           %        12.67
    L2 Cache Throughput               %         0.94
    SM Active Cycles              cycle        94.68
    Compute (SM) Throughput           %         0.05
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      6
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             768
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 94.44%                                                                                          
          The grid for this launch is configured to execute only 6 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         5.79
    Achieved Active Warps Per SM           warp         3.70
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.21%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (5.8%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        22.90
    Total DRAM Elapsed Cycles        cycle      215,040
    Average L1 Active Cycles         cycle        94.68
    Total L1 Elapsed Cycles          cycle      404,252
    Average L2 Active Cycles         cycle       557.09
    Total L2 Elapsed Cycles          cycle      295,840
    Average SM Active Cycles         cycle        94.68
    Total SM Elapsed Cycles          cycle      404,252
    Average SMSP Active Cycles       cycle        86.49
    Total SMSP Elapsed Cycles        cycle    1,617,008
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 9.473%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 62.89% above the average, while the minimum instance value is 86.54% below the      
          average.                                                                                                      

  triton_linear_kernel (1, 12, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle       20,517
    Memory Throughput                 %         6.61
    DRAM Throughput                   %         6.61
    Duration                         us        17.86
    L1/TEX Cache Throughput           %        36.22
    L2 Cache Throughput               %         6.30
    SM Active Cycles              cycle     1,985.69
    Compute (SM) Throughput           %         1.68
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.23
    Achieved Active Warps Per SM           warp         3.99
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.39%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,871.40
    Total DRAM Elapsed Cycles        cycle    1,132,032
    Average L1 Active Cycles         cycle     1,985.69
    Total L1 Elapsed Cycles          cycle    2,193,270
    Average L2 Active Cycles         cycle    14,778.58
    Total L2 Elapsed Cycles          cycle    1,555,280
    Average SM Active Cycles         cycle     1,985.69
    Total SM Elapsed Cycles          cycle    2,193,270
    Average SMSP Active Cycles       cycle     1,980.61
    Total SMSP Elapsed Cycles        cycle    8,773,080
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.72%                                                                                           
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.18% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.689%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.09% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.72%                                                                                           
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.18% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  triton_linear_kernel (1, 12, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle       20,261
    Memory Throughput                 %         6.69
    DRAM Throughput                   %         6.69
    Duration                         us        17.57
    L1/TEX Cache Throughput           %        35.85
    L2 Cache Throughput               %         6.38
    SM Active Cycles              cycle     1,995.12
    Compute (SM) Throughput           %         1.68
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.20
    Achieved Active Warps Per SM           warp         3.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.47%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,871.50
    Total DRAM Elapsed Cycles        cycle    1,118,848
    Average L1 Active Cycles         cycle     1,995.12
    Total L1 Elapsed Cycles          cycle    2,197,478
    Average L2 Active Cycles         cycle    14,799.83
    Total L2 Elapsed Cycles          cycle    1,536,480
    Average SM Active Cycles         cycle     1,995.12
    Total SM Elapsed Cycles          cycle    2,197,478
    Average SMSP Active Cycles       cycle     1,973.72
    Total SMSP Elapsed Cycles        cycle    8,789,912
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.728%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.02% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.64%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.07% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.728%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.02% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  void at::<unnamed>::CatArrayBatchedCopy<at::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, at::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, at::<unnamed>::TensorSizeStride<T2, 4>, int, T2) (216, 2, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        7,585
    Memory Throughput                 %         0.91
    DRAM Throughput                   %         0.23
    Duration                         us         6.62
    L1/TEX Cache Throughput           %         3.85
    L2 Cache Throughput               %         0.52
    SM Active Cycles              cycle     1,765.59
    Compute (SM) Throughput           %         5.06
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    432
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread         221,184
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        68.56
    Achieved Active Warps Per SM           warp        43.88
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 31.44%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (68.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24.60
    Total DRAM Elapsed Cycles        cycle      419,840
    Average L1 Active Cycles         cycle     1,765.59
    Total L1 Elapsed Cycles          cycle      805,582
    Average L2 Active Cycles         cycle       835.23
    Total L2 Elapsed Cycles          cycle      574,720
    Average SM Active Cycles         cycle     1,765.59
    Total SM Elapsed Cycles          cycle      805,582
    Average SMSP Active Cycles       cycle     1,704.47
    Total SMSP Elapsed Cycles        cycle    3,222,328
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 16.21%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 68.49% above the average, while the minimum instance value is 36.11% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 15.65%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 68.51% above the average, while the minimum instance value is 42.68% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 16.21%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 68.49% above the average, while the minimum instance value is 36.11% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.831%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 58.75% above the average, while the minimum instance value is 91.02% below the      
          average.                                                                                                      

  void at::<unnamed>::CatArrayBatchedCopy<at::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, at::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, at::<unnamed>::TensorSizeStride<T2, 4>, int, T2) (216, 2, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        7,415
    Memory Throughput                 %         0.93
    DRAM Throughput                   %         0.24
    Duration                         us         6.46
    L1/TEX Cache Throughput           %         3.86
    L2 Cache Throughput               %         0.54
    SM Active Cycles              cycle     1,762.33
    Compute (SM) Throughput           %         5.17
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    432
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread         221,184
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        68.07
    Achieved Active Warps Per SM           warp        43.56
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 31.93%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (68.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24.60
    Total DRAM Elapsed Cycles        cycle      409,600
    Average L1 Active Cycles         cycle     1,762.33
    Total L1 Elapsed Cycles          cycle      786,982
    Average L2 Active Cycles         cycle       768.50
    Total L2 Elapsed Cycles          cycle      561,520
    Average SM Active Cycles         cycle     1,762.33
    Total SM Elapsed Cycles          cycle      786,982
    Average SMSP Active Cycles       cycle     1,710.77
    Total SMSP Elapsed Cycles        cycle    3,147,928
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 16.24%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 67.16% above the average, while the minimum instance value is 36.50% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 16%                                                                                             
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 68.17% above the average, while the minimum instance value is 42.42% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 16.24%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 67.16% above the average, while the minimum instance value is 36.50% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.618%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 60.45% above the average, while the minimum instance value is 90.24% below the      
          average.                                                                                                      

  fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::Sm80, 1, 64, 64, 64, 1, 1>::Params) (1, 12, 1)x(32, 4, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle       17,884
    Memory Throughput                 %         2.13
    DRAM Throughput                   %         0.45
    Duration                         us        15.55
    L1/TEX Cache Throughput           %        20.65
    L2 Cache Throughput               %         2.34
    SM Active Cycles              cycle     1,721.73
    Compute (SM) Throughput           %         2.32
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread             168
    Shared Memory Configuration Size           Kbyte          135.17
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           36.35
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.04
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %        18.75
    Achieved Occupancy                        %         6.22
    Achieved Active Warps Per SM           warp         3.98
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.85%                                                                                    
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 81.25%                                                                                    
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (18.8%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       110.20
    Total DRAM Elapsed Cycles        cycle      987,648
    Average L1 Active Cycles         cycle     1,721.73
    Total L1 Elapsed Cycles          cycle    1,910,594
    Average L2 Active Cycles         cycle     4,087.75
    Total L2 Elapsed Cycles          cycle    1,354,720
    Average SM Active Cycles         cycle     1,721.73
    Total SM Elapsed Cycles          cycle    1,910,594
    Average SMSP Active Cycles       cycle     1,719.08
    Total SMSP Elapsed Cycles        cycle    7,642,376
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.674%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.12% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.659%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.11% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.674%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.12% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 9.609%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 39.81% above the average, while the minimum instance value is 66.07% below the      
          average.                                                                                                      

  ampere_sgemm_128x32_tn (6, 1, 13)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle       13,934
    Memory Throughput                 %        23.42
    DRAM Throughput                   %         9.67
    Duration                         us        12.19
    L1/TEX Cache Throughput           %        41.69
    L2 Cache Throughput               %        13.43
    SM Active Cycles              cycle     7,735.13
    Compute (SM) Throughput           %        21.76
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     78
    Registers Per Thread             register/thread              57
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           16.38
    # SMs                                         SM             108
    Threads                                   thread          19,968
    Uses Green Context                                             0
    Waves Per SM                                                0.18
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 27.78%                                                                                          
          The grid for this launch is configured to execute only 78 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            5
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        12.48
    Achieved Active Warps Per SM           warp         7.99
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75.03%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (12.5%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,872.20
    Total DRAM Elapsed Cycles        cycle      774,144
    Average L1 Active Cycles         cycle     7,735.13
    Total L1 Elapsed Cycles          cycle    1,486,992
    Average L2 Active Cycles         cycle     8,682.08
    Total L2 Elapsed Cycles          cycle    1,059,120
    Average SM Active Cycles         cycle     7,735.13
    Total SM Elapsed Cycles          cycle    1,486,992
    Average SMSP Active Cycles       cycle     7,761.56
    Total SMSP Elapsed Cycles        cycle    5,947,968
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 18.94%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 33.71% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.56%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 32.92% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.94%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 33.71% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.711%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L2 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 11.76% above the average, while the minimum instance value is 10.36% below  
          the average.                                                                                                  

  void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T9 *, T8 *, T5 *, const T6 *, const T6 *, const T10 *, const T4 *, T10 *, void *, long, T6 *, int *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *) (24, 1, 1)x(32, 16, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        6,474
    Memory Throughput                 %         2.90
    DRAM Throughput                   %         2.90
    Duration                         us         5.66
    L1/TEX Cache Throughput           %         5.89
    L2 Cache Throughput               %         3.98
    SM Active Cycles              cycle       896.91
    Compute (SM) Throughput           %         1.75
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     24
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          12,288
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 77.78%                                                                                          
          The grid for this launch is configured to execute only 24 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        17.85
    Achieved Active Warps Per SM           warp        11.42
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 82.15%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (17.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       259.40
    Total DRAM Elapsed Cycles        cycle      358,400
    Average L1 Active Cycles         cycle       896.91
    Total L1 Elapsed Cycles          cycle      698,078
    Average L2 Active Cycles         cycle     2,677.49
    Total L2 Elapsed Cycles          cycle      490,720
    Average SM Active Cycles         cycle       896.91
    Total SM Elapsed Cycles          cycle      698,078
    Average SMSP Active Cycles       cycle       957.82
    Total SMSP Elapsed Cycles        cycle    2,792,312
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.01%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 79.31% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.73%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 79.18% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.01%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 79.31% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.601%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 17.41% above the average, while the minimum instance value is 24.00% below the      
          average.                                                                                                      

  void at::vectorized_elementwise_kernel<4, at::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3) (6, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        3,986
    Memory Throughput                 %         0.84
    DRAM Throughput                   %         0.77
    Duration                         us         3.49
    L1/TEX Cache Throughput           %        13.02
    L2 Cache Throughput               %         1.10
    SM Active Cycles              cycle        98.98
    Compute (SM) Throughput           %         0.05
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      6
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             768
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 94.44%                                                                                          
          The grid for this launch is configured to execute only 6 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         6.48
    Achieved Active Warps Per SM           warp         4.15
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.52%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.5%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        42.50
    Total DRAM Elapsed Cycles        cycle      220,032
    Average L1 Active Cycles         cycle        98.98
    Total L1 Elapsed Cycles          cycle      437,792
    Average L2 Active Cycles         cycle       751.02
    Total L2 Elapsed Cycles          cycle      302,240
    Average SM Active Cycles         cycle        98.98
    Total SM Elapsed Cycles          cycle      437,792
    Average SMSP Active Cycles       cycle        98.17
    Total SMSP Elapsed Cycles        cycle    1,751,168
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 9.625%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 48.42% above the average, while the minimum instance value is 87.35% below the      
          average.                                                                                                      

  void at::<unnamed>::vectorized_layer_norm_kernel<float, float, 0>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *) (8, 1, 1)x(32, 4, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle        8,520
    Memory Throughput                 %         0.68
    DRAM Throughput                   %         0.38
    Duration                         us         7.42
    L1/TEX Cache Throughput           %         6.02
    L2 Cache Throughput               %         0.68
    SM Active Cycles              cycle       475.92
    Compute (SM) Throughput           %         0.45
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              38
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block              24
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 92.59%                                                                                          
          The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           28
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %           75
    Achieved Occupancy                        %         5.94
    Achieved Active Warps Per SM           warp         3.80
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 92.08%                                                                                    
          The difference between calculated theoretical (75.0%) and measured achieved occupancy (5.9%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 25%                                                                                       
          The 12.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the      
          hardware maximum of 16. This kernel's theoretical occupancy (75.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        44.90
    Total DRAM Elapsed Cycles        cycle      470,912
    Average L1 Active Cycles         cycle       475.92
    Total L1 Elapsed Cycles          cycle      925,900
    Average L2 Active Cycles         cycle       828.86
    Total L2 Elapsed Cycles          cycle      646,160
    Average SM Active Cycles         cycle       475.92
    Total SM Elapsed Cycles          cycle      925,900
    Average SMSP Active Cycles       cycle       462.33
    Total SMSP Elapsed Cycles        cycle    3,703,600
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.141%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 92.61% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.019%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 93.08% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.141%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 92.61% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.132%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 50.01% above the average, while the minimum instance value is 90.95% below the      
          average.                                                                                                      

  triton_linear_kernel (1, 48, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle       21,168
    Memory Throughput                 %        25.16
    DRAM Throughput                   %        25.16
    Duration                         us        18.56
    L1/TEX Cache Throughput           %        35.37
    L2 Cache Throughput               %        23.36
    SM Active Cycles              cycle     8,150.47
    Compute (SM) Throughput           %         6.47
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     48
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           6,144
    Uses Green Context                                             0
    Waves Per SM                                                0.07
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 55.56%                                                                                          
          The grid for this launch is configured to execute only 48 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.25
    Achieved Active Warps Per SM           warp         4.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.34%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     7,408.20
    Total DRAM Elapsed Cycles        cycle    1,177,600
    Average L1 Active Cycles         cycle     8,150.47
    Total L1 Elapsed Cycles          cycle    2,279,802
    Average L2 Active Cycles         cycle    16,454.22
    Total L2 Elapsed Cycles          cycle    1,605,280
    Average SM Active Cycles         cycle     8,150.47
    Total SM Elapsed Cycles          cycle    2,279,802
    Average SMSP Active Cycles       cycle     8,159.92
    Total SMSP Elapsed Cycles        cycle    9,119,208
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 22.09%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 57.21% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.97%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 56.84% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.09%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 57.21% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  void at::vectorized_elementwise_kernel<4, at::<unnamed>::launch_clamp_scalar(at::TensorIteratorBase &, c10::Scalar, c10::Scalar, detail::ClampLimits)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3) (24, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        3,972
    Memory Throughput                 %         1.67
    DRAM Throughput                   %         1.50
    Duration                         us         3.49
    L1/TEX Cache Throughput           %         9.12
    L2 Cache Throughput               %         3.24
    SM Active Cycles              cycle       394.81
    Compute (SM) Throughput           %         0.24
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     24
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           3,072
    Uses Green Context                                             0
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 77.78%                                                                                          
          The grid for this launch is configured to execute only 24 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         6.31
    Achieved Active Warps Per SM           warp         4.04
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.69%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.3%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        82.50
    Total DRAM Elapsed Cycles        cycle      220,160
    Average L1 Active Cycles         cycle       394.81
    Total L1 Elapsed Cycles          cycle      444,030
    Average L2 Active Cycles         cycle     1,270.10
    Total L2 Elapsed Cycles          cycle      300,880
    Average SM Active Cycles         cycle       394.81
    Total SM Elapsed Cycles          cycle      444,030
    Average SMSP Active Cycles       cycle       389.63
    Total SMSP Elapsed Cycles        cycle    1,776,120
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.63%                                                                                           
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 79.46% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.622%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 80.43% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.63%                                                                                           
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 79.46% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.75%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 31.83% above the average, while the minimum instance value is 55.12% below the      
          average.                                                                                                      

  ampere_sgemm_128x32_sliced1x4_tn (6, 1, 9)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle       36,735
    Memory Throughput                 %        24.13
    DRAM Throughput                   %        14.62
    Duration                         us        32.13
    L1/TEX Cache Throughput           %        52.31
    L2 Cache Throughput               %        14.43
    SM Active Cycles              cycle    16,920.88
    Compute (SM) Throughput           %        31.41
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.5 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     54
    Registers Per Thread             register/thread             134
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.01
    # SMs                                         SM             108
    Threads                                   thread          13,824
    Uses Green Context                                             0
    Waves Per SM                                                0.50
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          The grid for this launch is configured to execute only 54 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        12.50
    Achieved Occupancy                        %        12.52
    Achieved Active Warps Per SM           warp         8.01
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 87.5%                                                                                     
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     7,467.40
    Total DRAM Elapsed Cycles        cycle    2,042,880
    Average L1 Active Cycles         cycle    16,920.88
    Total L1 Elapsed Cycles          cycle    3,962,562
    Average L2 Active Cycles         cycle    22,618.31
    Total L2 Elapsed Cycles          cycle    2,789,840
    Average SM Active Cycles         cycle    16,920.88
    Total SM Elapsed Cycles          cycle    3,962,562
    Average SMSP Active Cycles       cycle    16,861.45
    Total SMSP Elapsed Cycles        cycle   15,850,248
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.5%                                                                                           
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 50.95% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.48%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 51.09% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.5%                                                                                           
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 50.95% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T9 *, T8 *, T5 *, const T6 *, const T6 *, const T10 *, const T4 *, T10 *, void *, long, T6 *, int *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *) (24, 1, 1)x(32, 16, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        5,529
    Memory Throughput                 %         2.38
    DRAM Throughput                   %         2.38
    Duration                         us         4.83
    L1/TEX Cache Throughput           %         6.31
    L2 Cache Throughput               %         3.69
    SM Active Cycles              cycle       723.35
    Compute (SM) Throughput           %         1.76
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     24
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          12,288
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 77.78%                                                                                          
          The grid for this launch is configured to execute only 24 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        19.91
    Achieved Active Warps Per SM           warp        12.74
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 80.09%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (19.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       182.60
    Total DRAM Elapsed Cycles        cycle      306,688
    Average L1 Active Cycles         cycle       723.35
    Total L1 Elapsed Cycles          cycle      641,426
    Average L2 Active Cycles         cycle        1,799
    Total L2 Elapsed Cycles          cycle      418,720
    Average SM Active Cycles         cycle       723.35
    Total SM Elapsed Cycles          cycle      641,426
    Average SMSP Active Cycles       cycle       735.55
    Total SMSP Elapsed Cycles        cycle    2,565,704
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 9.624%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 79.02% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 9.839%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 79.44% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 9.624%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 79.02% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.92%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 37.60% above the average, while the minimum instance value is 41.63% below the      
          average.                                                                                                      

  void at::vectorized_elementwise_kernel<4, at::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3) (6, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.57
    SM Frequency                    Ghz         1.13
    Elapsed Cycles                cycle        4,030
    Memory Throughput                 %         0.84
    DRAM Throughput                   %         0.76
    Duration                         us         3.55
    L1/TEX Cache Throughput           %        12.39
    L2 Cache Throughput               %         1.15
    SM Active Cycles              cycle       104.02
    Compute (SM) Throughput           %         0.05
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      6
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             768
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 94.44%                                                                                          
          The grid for this launch is configured to execute only 6 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         5.83
    Achieved Active Warps Per SM           warp         3.73
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.17%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (5.8%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        42.50
    Total DRAM Elapsed Cycles        cycle      222,592
    Average L1 Active Cycles         cycle       104.02
    Total L1 Elapsed Cycles          cycle      411,844
    Average L2 Active Cycles         cycle       750.01
    Total L2 Elapsed Cycles          cycle      305,680
    Average SM Active Cycles         cycle       104.02
    Total SM Elapsed Cycles          cycle      411,844
    Average SMSP Active Cycles       cycle       108.06
    Total SMSP Elapsed Cycles        cycle    1,647,376
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 9.113%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 46.43% above the average, while the minimum instance value is 90.00% below the      
          average.                                                                                                      

  void at::<unnamed>::vectorized_layer_norm_kernel<float, float, 0>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *) (8, 1, 1)x(32, 4, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.57
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        8,410
    Memory Throughput                 %         0.69
    DRAM Throughput                   %         0.38
    Duration                         us         7.36
    L1/TEX Cache Throughput           %         6.14
    L2 Cache Throughput               %         0.69
    SM Active Cycles              cycle       466.56
    Compute (SM) Throughput           %         0.46
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              38
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block              24
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 92.59%                                                                                          
          The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           28
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %           75
    Achieved Occupancy                        %         6.01
    Achieved Active Warps Per SM           warp         3.85
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.98%                                                                                    
          The difference between calculated theoretical (75.0%) and measured achieved occupancy (6.0%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 25%                                                                                       
          The 12.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the      
          hardware maximum of 16. This kernel's theoretical occupancy (75.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        43.50
    Total DRAM Elapsed Cycles        cycle      462,720
    Average L1 Active Cycles         cycle       466.56
    Total L1 Elapsed Cycles          cycle      911,406
    Average L2 Active Cycles         cycle          872
    Total L2 Elapsed Cycles          cycle      637,280
    Average SM Active Cycles         cycle       466.56
    Total SM Elapsed Cycles          cycle      911,406
    Average SMSP Active Cycles       cycle       456.96
    Total SMSP Elapsed Cycles        cycle    3,645,624
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.123%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 92.67% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.04%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 93.08% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.123%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 92.67% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.963%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 63.61% above the average, while the minimum instance value is 72.25% below the      
          average.                                                                                                      

  triton_linear_kernel (1, 12, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle       20,150
    Memory Throughput                 %         6.72
    DRAM Throughput                   %         6.72
    Duration                         us        17.50
    L1/TEX Cache Throughput           %        36.36
    L2 Cache Throughput               %         6.40
    SM Active Cycles              cycle     1,972.58
    Compute (SM) Throughput           %         1.68
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.24
    Achieved Active Warps Per SM           warp         3.99
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.36%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,871.50
    Total DRAM Elapsed Cycles        cycle    1,114,624
    Average L1 Active Cycles         cycle     1,972.58
    Total L1 Elapsed Cycles          cycle    2,189,844
    Average L2 Active Cycles         cycle    14,822.25
    Total L2 Elapsed Cycles          cycle    1,528,240
    Average SM Active Cycles         cycle     1,972.58
    Total SM Elapsed Cycles          cycle    2,189,844
    Average SMSP Active Cycles       cycle     1,989.75
    Total SMSP Elapsed Cycles        cycle    8,759,376
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.66%                                                                                           
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.01% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.754%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.21% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.66%                                                                                           
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.01% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  void at::vectorized_elementwise_kernel<4, at::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3) (6, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        3,840
    Memory Throughput                 %         0.79
    DRAM Throughput                   %         0.43
    Duration                         us         3.36
    L1/TEX Cache Throughput           %        12.76
    L2 Cache Throughput               %         1.27
    SM Active Cycles              cycle        94.06
    Compute (SM) Throughput           %         0.05
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      6
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             768
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 94.44%                                                                                          
          The grid for this launch is configured to execute only 6 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         6.41
    Achieved Active Warps Per SM           warp         4.10
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.59%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.4%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        22.90
    Total DRAM Elapsed Cycles        cycle      213,760
    Average L1 Active Cycles         cycle        94.06
    Total L1 Elapsed Cycles          cycle      420,734
    Average L2 Active Cycles         cycle       692.14
    Total L2 Elapsed Cycles          cycle      291,200
    Average SM Active Cycles         cycle        94.06
    Total SM Elapsed Cycles          cycle      420,734
    Average SMSP Active Cycles       cycle        91.08
    Total SMSP Elapsed Cycles        cycle    1,682,936
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.871%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 41.39% above the average, while the minimum instance value is 66.48% below the      
          average.                                                                                                      

  triton_linear_kernel (1, 12, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle       20,400
    Memory Throughput                 %         6.65
    DRAM Throughput                   %         6.65
    Duration                         us        17.76
    L1/TEX Cache Throughput           %        35.92
    L2 Cache Throughput               %         6.35
    SM Active Cycles              cycle     1,993.99
    Compute (SM) Throughput           %         1.69
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.21
    Achieved Active Warps Per SM           warp         3.98
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.43%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,871.50
    Total DRAM Elapsed Cycles        cycle    1,126,400
    Average L1 Active Cycles         cycle     1,993.99
    Total L1 Elapsed Cycles          cycle    2,182,852
    Average L2 Active Cycles         cycle    14,841.84
    Total L2 Elapsed Cycles          cycle    1,544,720
    Average SM Active Cycles         cycle     1,993.99
    Total SM Elapsed Cycles          cycle    2,182,852
    Average SMSP Active Cycles       cycle     1,992.92
    Total SMSP Elapsed Cycles        cycle    8,731,408
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.789%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.09% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.785%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.09% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.789%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.09% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  triton_linear_kernel (1, 12, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle       20,332
    Memory Throughput                 %         6.67
    DRAM Throughput                   %         6.67
    Duration                         us        17.70
    L1/TEX Cache Throughput           %        36.23
    L2 Cache Throughput               %         6.35
    SM Active Cycles              cycle     1,975.41
    Compute (SM) Throughput           %         1.69
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.24
    Achieved Active Warps Per SM           warp         3.99
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.37%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,871.50
    Total DRAM Elapsed Cycles        cycle    1,121,792
    Average L1 Active Cycles         cycle     1,975.41
    Total L1 Elapsed Cycles          cycle    2,175,848
    Average L2 Active Cycles         cycle    14,734.49
    Total L2 Elapsed Cycles          cycle    1,541,520
    Average SM Active Cycles         cycle     1,975.41
    Total SM Elapsed Cycles          cycle    2,175,848
    Average SMSP Active Cycles       cycle     2,004.56
    Total SMSP Elapsed Cycles        cycle    8,703,392
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.742%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.15% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.868%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.13% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.742%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.15% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  void at::<unnamed>::CatArrayBatchedCopy<at::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, at::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, at::<unnamed>::TensorSizeStride<T2, 4>, int, T2) (216, 2, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        7,492
    Memory Throughput                 %         0.90
    DRAM Throughput                   %         0.24
    Duration                         us         6.53
    L1/TEX Cache Throughput           %         3.72
    L2 Cache Throughput               %         0.59
    SM Active Cycles              cycle     1,833.95
    Compute (SM) Throughput           %         4.98
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    432
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread         221,184
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        65.13
    Achieved Active Warps Per SM           warp        41.68
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 34.87%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (65.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24.70
    Total DRAM Elapsed Cycles        cycle      413,184
    Average L1 Active Cycles         cycle     1,833.95
    Total L1 Elapsed Cycles          cycle      817,276
    Average L2 Active Cycles         cycle       827.99
    Total L2 Elapsed Cycles          cycle      567,920
    Average SM Active Cycles         cycle     1,833.95
    Total SM Elapsed Cycles          cycle      817,276
    Average SMSP Active Cycles       cycle     1,701.15
    Total SMSP Elapsed Cycles        cycle    3,269,104
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 16.08%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 66.33% above the average, while the minimum instance value is 34.51% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 15.32%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 68.14% above the average, while the minimum instance value is 42.16% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 16.08%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 66.33% above the average, while the minimum instance value is 34.51% below the      
          average.                                                                                                      

  void at::<unnamed>::CatArrayBatchedCopy<at::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, at::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, at::<unnamed>::TensorSizeStride<T2, 4>, int, T2) (216, 2, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        7,620
    Memory Throughput                 %         0.92
    DRAM Throughput                   %         0.24
    Duration                         us         6.66
    L1/TEX Cache Throughput           %         3.84
    L2 Cache Throughput               %         0.52
    SM Active Cycles              cycle     1,767.94
    Compute (SM) Throughput           %         5.13
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    432
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread         221,184
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        68.42
    Achieved Active Warps Per SM           warp        43.79
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 31.58%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (68.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24.70
    Total DRAM Elapsed Cycles        cycle      419,840
    Average L1 Active Cycles         cycle     1,767.94
    Total L1 Elapsed Cycles          cycle      794,086
    Average L2 Active Cycles         cycle       815.70
    Total L2 Elapsed Cycles          cycle      577,920
    Average SM Active Cycles         cycle     1,767.94
    Total SM Elapsed Cycles          cycle      794,086
    Average SMSP Active Cycles       cycle     1,723.88
    Total SMSP Elapsed Cycles        cycle    3,176,344
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 16.49%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 68.59% above the average, while the minimum instance value is 36.93% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 16.19%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 69.07% above the average, while the minimum instance value is 43.56% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 16.49%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 68.59% above the average, while the minimum instance value is 36.93% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.239%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 64.11% above the average, while the minimum instance value is 90.81% below the      
          average.                                                                                                      

  fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::Sm80, 1, 64, 64, 64, 1, 1>::Params) (1, 12, 1)x(32, 4, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle       17,628
    Memory Throughput                 %         2.17
    DRAM Throughput                   %         0.45
    Duration                         us        15.33
    L1/TEX Cache Throughput           %        20.88
    L2 Cache Throughput               %         2.42
    SM Active Cycles              cycle     1,702.20
    Compute (SM) Throughput           %         2.31
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread             168
    Shared Memory Configuration Size           Kbyte          135.17
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           36.35
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.04
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %        18.75
    Achieved Occupancy                        %         6.35
    Achieved Active Warps Per SM           warp         4.07
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.11%                                                                                    
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.4%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 81.25%                                                                                    
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (18.8%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       110.10
    Total DRAM Elapsed Cycles        cycle      972,800
    Average L1 Active Cycles         cycle     1,702.20
    Total L1 Elapsed Cycles          cycle    1,919,386
    Average L2 Active Cycles         cycle     4,247.14
    Total L2 Elapsed Cycles          cycle    1,335,120
    Average SM Active Cycles         cycle     1,702.20
    Total SM Elapsed Cycles          cycle    1,919,386
    Average SMSP Active Cycles       cycle     1,706.75
    Total SMSP Elapsed Cycles        cycle    7,677,544
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.526%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.01% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.551%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.04% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.526%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.01% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 9.836%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 38.65% above the average, while the minimum instance value is 60.18% below the      
          average.                                                                                                      

  ampere_sgemm_128x32_tn (6, 1, 13)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.13
    Elapsed Cycles                cycle       13,945
    Memory Throughput                 %        23.39
    DRAM Throughput                   %         9.58
    Duration                         us        12.32
    L1/TEX Cache Throughput           %        41.02
    L2 Cache Throughput               %        13.38
    SM Active Cycles              cycle     7,860.46
    Compute (SM) Throughput           %        21.74
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     78
    Registers Per Thread             register/thread              57
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           16.38
    # SMs                                         SM             108
    Threads                                   thread          19,968
    Uses Green Context                                             0
    Waves Per SM                                                0.18
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 27.78%                                                                                          
          The grid for this launch is configured to execute only 78 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            5
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        12.30
    Achieved Active Warps Per SM           warp         7.87
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75.4%                                                                                     
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (12.3%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,872.20
    Total DRAM Elapsed Cycles        cycle      781,824
    Average L1 Active Cycles         cycle     7,860.46
    Total L1 Elapsed Cycles          cycle    1,488,956
    Average L2 Active Cycles         cycle     8,757.91
    Total L2 Elapsed Cycles          cycle    1,061,920
    Average SM Active Cycles         cycle     7,860.46
    Total SM Elapsed Cycles          cycle    1,488,956
    Average SMSP Active Cycles       cycle     7,840.96
    Total SMSP Elapsed Cycles        cycle    5,955,824
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 18.45%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 32.36% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.78%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 33.01% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.45%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 32.36% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T9 *, T8 *, T5 *, const T6 *, const T6 *, const T10 *, const T4 *, T10 *, void *, long, T6 *, int *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *) (24, 1, 1)x(32, 16, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        6,495
    Memory Throughput                 %         2.88
    DRAM Throughput                   %         2.88
    Duration                         us         5.70
    L1/TEX Cache Throughput           %         5.84
    L2 Cache Throughput               %         3.95
    SM Active Cycles              cycle       896.92
    Compute (SM) Throughput           %         1.79
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     24
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          12,288
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 77.78%                                                                                          
          The grid for this launch is configured to execute only 24 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        17.45
    Achieved Active Warps Per SM           warp        11.17
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 82.55%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (17.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       259.40
    Total DRAM Elapsed Cycles        cycle      359,936
    Average L1 Active Cycles         cycle       896.92
    Total L1 Elapsed Cycles          cycle      680,730
    Average L2 Active Cycles         cycle     2,420.09
    Total L2 Elapsed Cycles          cycle      492,240
    Average SM Active Cycles         cycle       896.92
    Total SM Elapsed Cycles          cycle      680,730
    Average SMSP Active Cycles       cycle       887.07
    Total SMSP Elapsed Cycles        cycle    2,722,920
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.33%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 79.65% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.13%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 79.08% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.33%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 79.65% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.892%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 22.61% above the average, while the minimum instance value is 20.13% below the      
          average.                                                                                                      

  void at::vectorized_elementwise_kernel<4, at::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3) (6, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.55
    SM Frequency                    Ghz         1.13
    Elapsed Cycles                cycle        3,837
    Memory Throughput                 %         0.86
    DRAM Throughput                   %         0.81
    Duration                         us         3.39
    L1/TEX Cache Throughput           %        13.65
    L2 Cache Throughput               %         1.04
    SM Active Cycles              cycle        94.42
    Compute (SM) Throughput           %         0.05
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      6
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             768
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 94.44%                                                                                          
          The grid for this launch is configured to execute only 6 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         6.34
    Achieved Active Warps Per SM           warp         4.06
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.66%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.3%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        42.50
    Total DRAM Elapsed Cycles        cycle      210,432
    Average L1 Active Cycles         cycle        94.42
    Total L1 Elapsed Cycles          cycle      408,766
    Average L2 Active Cycles         cycle       671.19
    Total L2 Elapsed Cycles          cycle      290,880
    Average SM Active Cycles         cycle        94.42
    Total SM Elapsed Cycles          cycle      408,766
    Average SMSP Active Cycles       cycle        91.88
    Total SMSP Elapsed Cycles        cycle    1,635,064
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 9.462%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 51.26% above the average, while the minimum instance value is 88.83% below the      
          average.                                                                                                      

  void at::<unnamed>::vectorized_layer_norm_kernel<float, float, 0>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *) (8, 1, 1)x(32, 4, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        8,619
    Memory Throughput                 %         0.69
    DRAM Throughput                   %         0.37
    Duration                         us         7.52
    L1/TEX Cache Throughput           %         5.91
    L2 Cache Throughput               %         0.82
    SM Active Cycles              cycle       485.31
    Compute (SM) Throughput           %         0.45
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              38
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block              24
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 92.59%                                                                                          
          The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           28
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %           75
    Achieved Occupancy                        %         5.88
    Achieved Active Warps Per SM           warp         3.76
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 92.17%                                                                                    
          The difference between calculated theoretical (75.0%) and measured achieved occupancy (5.9%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 25%                                                                                       
          The 12.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the      
          hardware maximum of 16. This kernel's theoretical occupancy (75.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        43.50
    Total DRAM Elapsed Cycles        cycle      474,624
    Average L1 Active Cycles         cycle       485.31
    Total L1 Elapsed Cycles          cycle      929,244
    Average L2 Active Cycles         cycle       977.17
    Total L2 Elapsed Cycles          cycle      653,200
    Average SM Active Cycles         cycle       485.31
    Total SM Elapsed Cycles          cycle      929,244
    Average SMSP Active Cycles       cycle       466.38
    Total SMSP Elapsed Cycles        cycle    3,716,976
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.223%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 92.61% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.042%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 93.02% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.223%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 92.61% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.409%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 61.90% above the average, while the minimum instance value is 92.32% below the      
          average.                                                                                                      

  triton_linear_kernel (1, 48, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle       21,181
    Memory Throughput                 %        25.15
    DRAM Throughput                   %        25.15
    Duration                         us        18.59
    L1/TEX Cache Throughput           %        35.31
    L2 Cache Throughput               %        23.31
    SM Active Cycles              cycle     8,153.31
    Compute (SM) Throughput           %         6.41
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     48
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           6,144
    Uses Green Context                                             0
    Waves Per SM                                                0.07
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 55.56%                                                                                          
          The grid for this launch is configured to execute only 48 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.25
    Achieved Active Warps Per SM           warp         4.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.33%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.3%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     7,408.30
    Total DRAM Elapsed Cycles        cycle    1,178,112
    Average L1 Active Cycles         cycle     8,153.31
    Total L1 Elapsed Cycles          cycle    2,300,314
    Average L2 Active Cycles         cycle    16,475.45
    Total L2 Elapsed Cycles          cycle    1,608,480
    Average SM Active Cycles         cycle     8,153.31
    Total SM Elapsed Cycles          cycle    2,300,314
    Average SMSP Active Cycles       cycle     8,160.24
    Total SMSP Elapsed Cycles        cycle    9,201,256
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 21.87%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 57.12% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.86%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 57.06% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.87%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 57.12% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  void at::vectorized_elementwise_kernel<4, at::<unnamed>::launch_clamp_scalar(at::TensorIteratorBase &, c10::Scalar, c10::Scalar, detail::ClampLimits)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3) (24, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.56
    SM Frequency                    Ghz         1.13
    Elapsed Cycles                cycle        4,103
    Memory Throughput                 %         1.63
    DRAM Throughput                   %         1.46
    Duration                         us         3.62
    L1/TEX Cache Throughput           %         9.04
    L2 Cache Throughput               %         2.64
    SM Active Cycles              cycle       398.22
    Compute (SM) Throughput           %         0.24
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     24
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           3,072
    Uses Green Context                                             0
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 77.78%                                                                                          
          The grid for this launch is configured to execute only 24 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         6.16
    Achieved Active Warps Per SM           warp         3.94
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.84%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        82.50
    Total DRAM Elapsed Cycles        cycle      226,304
    Average L1 Active Cycles         cycle       398.22
    Total L1 Elapsed Cycles          cycle      452,062
    Average L2 Active Cycles         cycle     1,000.86
    Total L2 Elapsed Cycles          cycle      310,800
    Average SM Active Cycles         cycle       398.22
    Total SM Elapsed Cycles          cycle      452,062
    Average SMSP Active Cycles       cycle       414.08
    Total SMSP Elapsed Cycles        cycle    1,808,248
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.702%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 80.96% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.938%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 80.24% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.702%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 80.96% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.56%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 40.99% above the average, while the minimum instance value is 77.52% below the      
          average.                                                                                                      

  ampere_sgemm_128x32_sliced1x4_tn (6, 1, 9)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle       36,773
    Memory Throughput                 %        24.10
    DRAM Throughput                   %        14.60
    Duration                         us        32.16
    L1/TEX Cache Throughput           %        52.36
    L2 Cache Throughput               %        14.40
    SM Active Cycles              cycle    16,911.78
    Compute (SM) Throughput           %        31.37
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.5 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     54
    Registers Per Thread             register/thread             134
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.01
    # SMs                                         SM             108
    Threads                                   thread          13,824
    Uses Green Context                                             0
    Waves Per SM                                                0.50
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          The grid for this launch is configured to execute only 54 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        12.50
    Achieved Occupancy                        %        12.56
    Achieved Active Warps Per SM           warp         8.04
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 87.5%                                                                                     
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     7,467.40
    Total DRAM Elapsed Cycles        cycle    2,045,440
    Average L1 Active Cycles         cycle    16,911.78
    Total L1 Elapsed Cycles          cycle    3,968,378
    Average L2 Active Cycles         cycle    23,494.08
    Total L2 Elapsed Cycles          cycle    2,795,200
    Average SM Active Cycles         cycle    16,911.78
    Total SM Elapsed Cycles          cycle    3,968,378
    Average SMSP Active Cycles       cycle    16,935.70
    Total SMSP Elapsed Cycles        cycle   15,873,512
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.52%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 51.10% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.66%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 51.33% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.52%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 51.10% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T9 *, T8 *, T5 *, const T6 *, const T6 *, const T10 *, const T4 *, T10 *, void *, long, T6 *, int *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *) (24, 1, 1)x(32, 16, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.57
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        5,620
    Memory Throughput                 %         2.35
    DRAM Throughput                   %         2.35
    Duration                         us         4.93
    L1/TEX Cache Throughput           %         6.05
    L2 Cache Throughput               %         3.57
    SM Active Cycles              cycle       752.32
    Compute (SM) Throughput           %         1.86
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     24
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          12,288
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 77.78%                                                                                          
          The grid for this launch is configured to execute only 24 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        18.30
    Achieved Active Warps Per SM           warp        11.71
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 81.7%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (18.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       182.60
    Total DRAM Elapsed Cycles        cycle      310,272
    Average L1 Active Cycles         cycle       752.32
    Total L1 Elapsed Cycles          cycle      605,248
    Average L2 Active Cycles         cycle     1,714.12
    Total L2 Elapsed Cycles          cycle      425,760
    Average SM Active Cycles         cycle       752.32
    Total SM Elapsed Cycles          cycle      605,248
    Average SMSP Active Cycles       cycle       739.92
    Total SMSP Elapsed Cycles        cycle    2,420,992
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 10.59%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.85% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.45%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 79.16% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.59%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.85% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.69%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 36.30% above the average, while the minimum instance value is 40.20% below the      
          average.                                                                                                      

  void at::vectorized_elementwise_kernel<4, at::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3) (6, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.55
    SM Frequency                    Ghz         1.13
    Elapsed Cycles                cycle        3,865
    Memory Throughput                 %         0.86
    DRAM Throughput                   %         0.80
    Duration                         us         3.42
    L1/TEX Cache Throughput           %        13.27
    L2 Cache Throughput               %         1.13
    SM Active Cycles              cycle        97.12
    Compute (SM) Throughput           %         0.05
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      6
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             768
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 94.44%                                                                                          
          The grid for this launch is configured to execute only 6 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         6.37
    Achieved Active Warps Per SM           warp         4.07
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.63%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.4%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        42.50
    Total DRAM Elapsed Cycles        cycle      212,224
    Average L1 Active Cycles         cycle        97.12
    Total L1 Elapsed Cycles          cycle      414,900
    Average L2 Active Cycles         cycle       663.67
    Total L2 Elapsed Cycles          cycle      292,960
    Average SM Active Cycles         cycle        97.12
    Total SM Elapsed Cycles          cycle      414,900
    Average SMSP Active Cycles       cycle        94.65
    Total SMSP Elapsed Cycles        cycle    1,659,600
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.149%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 39.45% above the average, while the minimum instance value is 85.69% below the      
          average.                                                                                                      

  void at::<unnamed>::vectorized_layer_norm_kernel<float, float, 0>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *) (8, 1, 1)x(32, 4, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle        8,735
    Memory Throughput                 %         0.68
    DRAM Throughput                   %         0.36
    Duration                         us         7.62
    L1/TEX Cache Throughput           %         5.86
    L2 Cache Throughput               %         0.77
    SM Active Cycles              cycle       488.72
    Compute (SM) Throughput           %         0.46
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              38
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block              24
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 92.59%                                                                                          
          The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           28
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %           75
    Achieved Occupancy                        %         5.72
    Achieved Active Warps Per SM           warp         3.66
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 92.38%                                                                                    
          The difference between calculated theoretical (75.0%) and measured achieved occupancy (5.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 25%                                                                                       
          The 12.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the      
          hardware maximum of 16. This kernel's theoretical occupancy (75.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        43.50
    Total DRAM Elapsed Cycles        cycle      482,560
    Average L1 Active Cycles         cycle       488.72
    Total L1 Elapsed Cycles          cycle      910,168
    Average L2 Active Cycles         cycle       897.75
    Total L2 Elapsed Cycles          cycle      662,480
    Average SM Active Cycles         cycle       488.72
    Total SM Elapsed Cycles          cycle      910,168
    Average SMSP Active Cycles       cycle       442.78
    Total SMSP Elapsed Cycles        cycle    3,640,672
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.373%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 92.65% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.373%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 92.65% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.617%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 51.81% above the average, while the minimum instance value is 61.35% below the      
          average.                                                                                                      

  triton_linear_kernel (1, 12, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle       20,474
    Memory Throughput                 %         6.62
    DRAM Throughput                   %         6.62
    Duration                         us        17.82
    L1/TEX Cache Throughput           %        36.02
    L2 Cache Throughput               %         6.31
    SM Active Cycles              cycle     1,991.04
    Compute (SM) Throughput           %         1.67
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.27
    Achieved Active Warps Per SM           warp         4.01
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.29%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.3%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,871.50
    Total DRAM Elapsed Cycles        cycle    1,131,008
    Average L1 Active Cycles         cycle     1,991.04
    Total L1 Elapsed Cycles          cycle    2,201,136
    Average L2 Active Cycles         cycle    14,782.15
    Total L2 Elapsed Cycles          cycle    1,552,080
    Average SM Active Cycles         cycle     1,991.04
    Total SM Elapsed Cycles          cycle    2,201,136
    Average SMSP Active Cycles       cycle     1,979.89
    Total SMSP Elapsed Cycles        cycle    8,804,544
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.706%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.12% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.651%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.05% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.706%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.12% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  void at::vectorized_elementwise_kernel<4, at::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3) (6, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        3,795
    Memory Throughput                 %         0.75
    DRAM Throughput                   %         0.44
    Duration                         us         3.33
    L1/TEX Cache Throughput           %        13.20
    L2 Cache Throughput               %         0.97
    SM Active Cycles              cycle        90.90
    Compute (SM) Throughput           %         0.05
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      6
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             768
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 94.44%                                                                                          
          The grid for this launch is configured to execute only 6 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         6.12
    Achieved Active Warps Per SM           warp         3.92
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.88%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        22.90
    Total DRAM Elapsed Cycles        cycle      209,920
    Average L1 Active Cycles         cycle        90.90
    Total L1 Elapsed Cycles          cycle      408,460
    Average L2 Active Cycles         cycle       555.64
    Total L2 Elapsed Cycles          cycle      287,840
    Average SM Active Cycles         cycle        90.90
    Total SM Elapsed Cycles          cycle      408,460
    Average SMSP Active Cycles       cycle        91.94
    Total SMSP Elapsed Cycles        cycle    1,633,840
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 9.542%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 61.79% above the average, while the minimum instance value is 86.50% below the      
          average.                                                                                                      

  triton_linear_kernel (1, 12, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle       20,450
    Memory Throughput                 %         6.62
    DRAM Throughput                   %         6.62
    Duration                         us        17.76
    L1/TEX Cache Throughput           %        36.22
    L2 Cache Throughput               %         6.32
    SM Active Cycles              cycle     1,987.26
    Compute (SM) Throughput           %         1.70
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.21
    Achieved Active Warps Per SM           warp         3.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.45%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,871.40
    Total DRAM Elapsed Cycles        cycle    1,131,008
    Average L1 Active Cycles         cycle     1,987.26
    Total L1 Elapsed Cycles          cycle    2,172,352
    Average L2 Active Cycles         cycle    14,805.38
    Total L2 Elapsed Cycles          cycle    1,550,640
    Average SM Active Cycles         cycle     1,987.26
    Total SM Elapsed Cycles          cycle    2,172,352
    Average SMSP Active Cycles       cycle     1,984.41
    Total SMSP Elapsed Cycles        cycle    8,689,408
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.808%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.15% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.783%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.03% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.808%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.15% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  triton_linear_kernel (1, 12, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle       20,419
    Memory Throughput                 %         6.64
    DRAM Throughput                   %         6.64
    Duration                         us        17.76
    L1/TEX Cache Throughput           %        36.15
    L2 Cache Throughput               %         6.35
    SM Active Cycles              cycle     1,980.48
    Compute (SM) Throughput           %         1.67
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.25
    Achieved Active Warps Per SM           warp         4.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.32%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.3%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,871.40
    Total DRAM Elapsed Cycles        cycle    1,127,936
    Average L1 Active Cycles         cycle     1,980.48
    Total L1 Elapsed Cycles          cycle    2,206,510
    Average L2 Active Cycles         cycle    14,849.20
    Total L2 Elapsed Cycles          cycle    1,545,120
    Average SM Active Cycles         cycle     1,980.48
    Total SM Elapsed Cycles          cycle    2,206,510
    Average SMSP Active Cycles       cycle     1,983.80
    Total SMSP Elapsed Cycles        cycle    8,826,040
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.637%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.10% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.65%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.08% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.637%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.10% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  void at::<unnamed>::CatArrayBatchedCopy<at::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, at::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, at::<unnamed>::TensorSizeStride<T2, 4>, int, T2) (216, 2, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        7,471
    Memory Throughput                 %         0.93
    DRAM Throughput                   %         0.24
    Duration                         us         6.53
    L1/TEX Cache Throughput           %         3.82
    L2 Cache Throughput               %         0.63
    SM Active Cycles              cycle     1,774.48
    Compute (SM) Throughput           %         5.18
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    432
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread         221,184
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        67.82
    Achieved Active Warps Per SM           warp        43.40
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 32.18%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (67.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24.60
    Total DRAM Elapsed Cycles        cycle      414,208
    Average L1 Active Cycles         cycle     1,774.48
    Total L1 Elapsed Cycles          cycle      787,714
    Average L2 Active Cycles         cycle       901.25
    Total L2 Elapsed Cycles          cycle      566,000
    Average SM Active Cycles         cycle     1,774.48
    Total SM Elapsed Cycles          cycle      787,714
    Average SMSP Active Cycles       cycle     1,712.46
    Total SMSP Elapsed Cycles        cycle    3,150,856
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 16.37%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 67.28% above the average, while the minimum instance value is 36.77% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 16.18%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 68.91% above the average, while the minimum instance value is 44.12% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 16.37%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 67.28% above the average, while the minimum instance value is 36.77% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.482%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 50.89% above the average, while the minimum instance value is 91.35% below the      
          average.                                                                                                      

  void at::<unnamed>::CatArrayBatchedCopy<at::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, at::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, at::<unnamed>::TensorSizeStride<T2, 4>, int, T2) (216, 2, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        7,468
    Memory Throughput                 %         0.91
    DRAM Throughput                   %         0.24
    Duration                         us         6.53
    L1/TEX Cache Throughput           %         3.65
    L2 Cache Throughput               %         0.55
    SM Active Cycles              cycle     1,865.40
    Compute (SM) Throughput           %         5.07
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    432
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread         221,184
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        65.11
    Achieved Active Warps Per SM           warp        41.67
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 34.89%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (65.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24.60
    Total DRAM Elapsed Cycles        cycle      414,208
    Average L1 Active Cycles         cycle     1,865.40
    Total L1 Elapsed Cycles          cycle      803,520
    Average L2 Active Cycles         cycle       878.61
    Total L2 Elapsed Cycles          cycle      565,680
    Average SM Active Cycles         cycle     1,865.40
    Total SM Elapsed Cycles          cycle      803,520
    Average SMSP Active Cycles       cycle     1,704.36
    Total SMSP Elapsed Cycles        cycle    3,214,080
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 16.51%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 65.86% above the average, while the minimum instance value is 34.01% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 15.67%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 68.40% above the average, while the minimum instance value is 42.73% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 16.51%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 65.86% above the average, while the minimum instance value is 34.01% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.397%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 51.48% above the average, while the minimum instance value is 71.09% below the      
          average.                                                                                                      

  fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::Sm80, 1, 64, 64, 64, 1, 1>::Params) (1, 12, 1)x(32, 4, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle       17,794
    Memory Throughput                 %         2.14
    DRAM Throughput                   %         0.45
    Duration                         us        15.46
    L1/TEX Cache Throughput           %        20.68
    L2 Cache Throughput               %         2.40
    SM Active Cycles              cycle     1,718.54
    Compute (SM) Throughput           %         2.32
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread             168
    Shared Memory Configuration Size           Kbyte          135.17
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           36.35
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.04
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %        18.75
    Achieved Occupancy                        %         6.25
    Achieved Active Warps Per SM           warp         4.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.69%                                                                                    
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 81.25%                                                                                    
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (18.8%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       110.10
    Total DRAM Elapsed Cycles        cycle      983,040
    Average L1 Active Cycles         cycle     1,718.54
    Total L1 Elapsed Cycles          cycle    1,913,526
    Average L2 Active Cycles         cycle     4,231.62
    Total L2 Elapsed Cycles          cycle    1,348,960
    Average SM Active Cycles         cycle     1,718.54
    Total SM Elapsed Cycles          cycle    1,913,526
    Average SMSP Active Cycles       cycle     1,713.34
    Total SMSP Elapsed Cycles        cycle    7,654,104
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.638%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.05% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.616%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.10% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.638%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.05% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.763%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 30.93% above the average, while the minimum instance value is 45.17% below the      
          average.                                                                                                      

  ampere_sgemm_128x32_tn (6, 1, 13)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.13
    Elapsed Cycles                cycle       14,002
    Memory Throughput                 %        23.53
    DRAM Throughput                   %         9.54
    Duration                         us        12.35
    L1/TEX Cache Throughput           %        40.90
    L2 Cache Throughput               %        13.30
    SM Active Cycles              cycle     7,883.38
    Compute (SM) Throughput           %        21.87
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     78
    Registers Per Thread             register/thread              57
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           16.38
    # SMs                                         SM             108
    Threads                                   thread          19,968
    Uses Green Context                                             0
    Waves Per SM                                                0.18
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 27.78%                                                                                          
          The grid for this launch is configured to execute only 78 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            5
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        12.26
    Achieved Active Warps Per SM           warp         7.85
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75.47%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (12.3%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,872.20
    Total DRAM Elapsed Cycles        cycle      784,896
    Average L1 Active Cycles         cycle     7,883.38
    Total L1 Elapsed Cycles          cycle    1,479,756
    Average L2 Active Cycles         cycle     8,754.08
    Total L2 Elapsed Cycles          cycle    1,067,440
    Average SM Active Cycles         cycle     7,883.38
    Total SM Elapsed Cycles          cycle    1,479,756
    Average SMSP Active Cycles       cycle     7,704.19
    Total SMSP Elapsed Cycles        cycle    5,919,024
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 18.89%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 32.83% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.21%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 32.38% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.89%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 32.83% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.418%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L2 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 8.26% above the average, while the minimum instance value is 7.48% below    
          the average.                                                                                                  

  void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T9 *, T8 *, T5 *, const T6 *, const T6 *, const T10 *, const T4 *, T10 *, void *, long, T6 *, int *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *) (24, 1, 1)x(32, 16, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        6,268
    Memory Throughput                 %         3.00
    DRAM Throughput                   %         3.00
    Duration                         us         5.47
    L1/TEX Cache Throughput           %         5.95
    L2 Cache Throughput               %         4.11
    SM Active Cycles              cycle       882.28
    Compute (SM) Throughput           %         1.79
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     24
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          12,288
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 77.78%                                                                                          
          The grid for this launch is configured to execute only 24 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        17.58
    Achieved Active Warps Per SM           warp        11.25
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 82.42%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (17.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       259.40
    Total DRAM Elapsed Cycles        cycle      345,600
    Average L1 Active Cycles         cycle       882.28
    Total L1 Elapsed Cycles          cycle      681,390
    Average L2 Active Cycles         cycle     2,432.47
    Total L2 Elapsed Cycles          cycle      474,800
    Average SM Active Cycles         cycle       882.28
    Total SM Elapsed Cycles          cycle      681,390
    Average SMSP Active Cycles       cycle       897.26
    Total SMSP Elapsed Cycles        cycle    2,725,560
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.03%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.84% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.21%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.86% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.03%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.84% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.598%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L2 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 18.54% above the average, while the minimum instance value is 17.45% below  
          the average.                                                                                                  

  void at::vectorized_elementwise_kernel<4, at::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3) (6, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.54
    SM Frequency                    Ghz         1.13
    Elapsed Cycles                cycle        3,932
    Memory Throughput                 %         0.84
    DRAM Throughput                   %         0.79
    Duration                         us         3.49
    L1/TEX Cache Throughput           %        13.00
    L2 Cache Throughput               %         0.97
    SM Active Cycles              cycle        99.13
    Compute (SM) Throughput           %         0.05
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      6
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             768
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 94.44%                                                                                          
          The grid for this launch is configured to execute only 6 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         6.00
    Achieved Active Warps Per SM           warp         3.84
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 94%                                                                                       
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.0%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        42.50
    Total DRAM Elapsed Cycles        cycle      215,552
    Average L1 Active Cycles         cycle        99.13
    Total L1 Elapsed Cycles          cycle      415,660
    Average L2 Active Cycles         cycle       647.67
    Total L2 Elapsed Cycles          cycle      298,240
    Average SM Active Cycles         cycle        99.13
    Total SM Elapsed Cycles          cycle      415,660
    Average SMSP Active Cycles       cycle        96.31
    Total SMSP Elapsed Cycles        cycle    1,662,640
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.957%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 51.56% above the average, while the minimum instance value is 85.33% below the      
          average.                                                                                                      

  void at::<unnamed>::vectorized_layer_norm_kernel<float, float, 0>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *) (8, 1, 1)x(32, 4, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle        8,672
    Memory Throughput                 %         0.69
    DRAM Throughput                   %         0.36
    Duration                         us         7.55
    L1/TEX Cache Throughput           %         5.85
    L2 Cache Throughput               %         0.80
    SM Active Cycles              cycle       491.71
    Compute (SM) Throughput           %         0.44
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              38
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block              24
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 92.59%                                                                                          
          The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           28
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %           75
    Achieved Occupancy                        %         5.93
    Achieved Active Warps Per SM           warp         3.79
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 92.09%                                                                                    
          The difference between calculated theoretical (75.0%) and measured achieved occupancy (5.9%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 25%                                                                                       
          The 12.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the      
          hardware maximum of 16. This kernel's theoretical occupancy (75.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        43.50
    Total DRAM Elapsed Cycles        cycle      477,952
    Average L1 Active Cycles         cycle       491.71
    Total L1 Elapsed Cycles          cycle      941,656
    Average L2 Active Cycles         cycle     1,018.41
    Total L2 Elapsed Cycles          cycle      657,760
    Average SM Active Cycles         cycle       491.71
    Total SM Elapsed Cycles          cycle      941,656
    Average SMSP Active Cycles       cycle       470.82
    Total SMSP Elapsed Cycles        cycle    3,766,624
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.225%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 92.65% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.027%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 93.10% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.225%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 92.65% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.215%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 50.18% above the average, while the minimum instance value is 62.39% below the      
          average.                                                                                                      

  triton_linear_kernel (1, 48, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle       21,201
    Memory Throughput                 %        25.12
    DRAM Throughput                   %        25.12
    Duration                         us        18.59
    L1/TEX Cache Throughput           %        35.35
    L2 Cache Throughput               %        23.37
    SM Active Cycles              cycle     8,148.38
    Compute (SM) Throughput           %         6.45
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     48
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           6,144
    Uses Green Context                                             0
    Waves Per SM                                                0.07
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 55.56%                                                                                          
          The grid for this launch is configured to execute only 48 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.25
    Achieved Active Warps Per SM           warp         4.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.32%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.3%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     7,408.30
    Total DRAM Elapsed Cycles        cycle    1,179,648
    Average L1 Active Cycles         cycle     8,148.38
    Total L1 Elapsed Cycles          cycle    2,286,292
    Average L2 Active Cycles         cycle    16,576.74
    Total L2 Elapsed Cycles          cycle    1,609,680
    Average SM Active Cycles         cycle     8,148.38
    Total SM Elapsed Cycles          cycle    2,286,292
    Average SMSP Active Cycles       cycle     8,197.75
    Total SMSP Elapsed Cycles        cycle    9,145,168
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 21.97%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 57.07% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.02%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 56.87% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.97%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 57.07% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  void at::vectorized_elementwise_kernel<4, at::<unnamed>::launch_clamp_scalar(at::TensorIteratorBase &, c10::Scalar, c10::Scalar, detail::ClampLimits)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3) (24, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.56
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        4,001
    Memory Throughput                 %         1.66
    DRAM Throughput                   %         1.50
    Duration                         us         3.52
    L1/TEX Cache Throughput           %         9.19
    L2 Cache Throughput               %         3.07
    SM Active Cycles              cycle       391.52
    Compute (SM) Throughput           %         0.24
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     24
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           3,072
    Uses Green Context                                             0
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 77.78%                                                                                          
          The grid for this launch is configured to execute only 24 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         6.53
    Achieved Active Warps Per SM           warp         4.18
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.47%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.5%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        82.50
    Total DRAM Elapsed Cycles        cycle      220,160
    Average L1 Active Cycles         cycle       391.52
    Total L1 Elapsed Cycles          cycle      450,800
    Average L2 Active Cycles         cycle     1,319.53
    Total L2 Elapsed Cycles          cycle      303,280
    Average SM Active Cycles         cycle       391.52
    Total SM Elapsed Cycles          cycle      450,800
    Average SMSP Active Cycles       cycle       382.25
    Total SMSP Elapsed Cycles        cycle    1,803,200
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.477%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 79.71% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.338%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 80.13% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.477%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 79.71% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 14.38%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 41.30% above the average, while the minimum instance value is 53.47% below the      
          average.                                                                                                      

  ampere_sgemm_128x32_sliced1x4_tn (6, 1, 9)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle       36,646
    Memory Throughput                 %        24.18
    DRAM Throughput                   %        14.66
    Duration                         us        32.03
    L1/TEX Cache Throughput           %        52.25
    L2 Cache Throughput               %        14.47
    SM Active Cycles              cycle    16,940.69
    Compute (SM) Throughput           %        31.49
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.5 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     54
    Registers Per Thread             register/thread             134
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.01
    # SMs                                         SM             108
    Threads                                   thread          13,824
    Uses Green Context                                             0
    Waves Per SM                                                0.50
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          The grid for this launch is configured to execute only 54 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        12.50
    Achieved Occupancy                        %        12.49
    Achieved Active Warps Per SM           warp         8.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 87.5%                                                                                     
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     7,467.40
    Total DRAM Elapsed Cycles        cycle    2,037,760
    Average L1 Active Cycles         cycle    16,940.69
    Total L1 Elapsed Cycles          cycle    3,953,246
    Average L2 Active Cycles         cycle    23,772.64
    Total L2 Elapsed Cycles          cycle    2,783,680
    Average SM Active Cycles         cycle    16,940.69
    Total SM Elapsed Cycles          cycle    3,953,246
    Average SMSP Active Cycles       cycle    16,946.63
    Total SMSP Elapsed Cycles        cycle   15,812,984
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.5%                                                                                           
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 50.79% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.68%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 51.15% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.5%                                                                                           
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 50.79% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T9 *, T8 *, T5 *, const T6 *, const T6 *, const T10 *, const T4 *, T10 *, void *, long, T6 *, int *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *) (24, 1, 1)x(32, 16, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        5,850
    Memory Throughput                 %         2.25
    DRAM Throughput                   %         2.25
    Duration                         us         5.12
    L1/TEX Cache Throughput           %         6.04
    L2 Cache Throughput               %         3.40
    SM Active Cycles              cycle       750.20
    Compute (SM) Throughput           %         1.84
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     24
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          12,288
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 77.78%                                                                                          
          The grid for this launch is configured to execute only 24 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        18.37
    Achieved Active Warps Per SM           warp        11.76
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 81.63%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (18.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       182.60
    Total DRAM Elapsed Cycles        cycle      324,096
    Average L1 Active Cycles         cycle       750.20
    Total L1 Elapsed Cycles          cycle      613,202
    Average L2 Active Cycles         cycle     1,879.46
    Total L2 Elapsed Cycles          cycle      443,440
    Average SM Active Cycles         cycle       750.20
    Total SM Elapsed Cycles          cycle      613,202
    Average SMSP Active Cycles       cycle       698.68
    Total SMSP Elapsed Cycles        cycle    2,452,808
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 10.56%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 79.89% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 9.859%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 80.12% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.56%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 79.89% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.23%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 30.18% above the average, while the minimum instance value is 43.12% below the      
          average.                                                                                                      

  void at::vectorized_elementwise_kernel<4, at::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3) (6, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.56
    SM Frequency                    Ghz         1.13
    Elapsed Cycles                cycle        3,866
    Memory Throughput                 %         0.92
    DRAM Throughput                   %         0.79
    Duration                         us         3.42
    L1/TEX Cache Throughput           %        13.22
    L2 Cache Throughput               %         1.49
    SM Active Cycles              cycle        97.53
    Compute (SM) Throughput           %         0.05
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      6
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             768
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 94.44%                                                                                          
          The grid for this launch is configured to execute only 6 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         6.48
    Achieved Active Warps Per SM           warp         4.15
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.52%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.5%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        42.50
    Total DRAM Elapsed Cycles        cycle      214,272
    Average L1 Active Cycles         cycle        97.53
    Total L1 Elapsed Cycles          cycle      432,974
    Average L2 Active Cycles         cycle       874.92
    Total L2 Elapsed Cycles          cycle      292,800
    Average SM Active Cycles         cycle        97.53
    Total SM Elapsed Cycles          cycle      432,974
    Average SMSP Active Cycles       cycle        97.26
    Total SMSP Elapsed Cycles        cycle    1,731,896
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.976%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 33.36% above the average, while the minimum instance value is 49.48% below the      
          average.                                                                                                      

  void at::<unnamed>::vectorized_layer_norm_kernel<float, float, 0>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *) (8, 1, 1)x(32, 4, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        8,736
    Memory Throughput                 %         0.68
    DRAM Throughput                   %         0.36
    Duration                         us         7.62
    L1/TEX Cache Throughput           %         5.80
    L2 Cache Throughput               %         0.81
    SM Active Cycles              cycle       493.96
    Compute (SM) Throughput           %         0.45
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              38
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block              24
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 92.59%                                                                                          
          The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           28
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %           75
    Achieved Occupancy                        %         5.84
    Achieved Active Warps Per SM           warp         3.74
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 92.21%                                                                                    
          The difference between calculated theoretical (75.0%) and measured achieved occupancy (5.8%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 25%                                                                                       
          The 12.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the      
          hardware maximum of 16. This kernel's theoretical occupancy (75.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        43.50
    Total DRAM Elapsed Cycles        cycle      482,304
    Average L1 Active Cycles         cycle       493.96
    Total L1 Elapsed Cycles          cycle      939,098
    Average L2 Active Cycles         cycle     1,029.59
    Total L2 Elapsed Cycles          cycle      662,080
    Average SM Active Cycles         cycle       493.96
    Total SM Elapsed Cycles          cycle      939,098
    Average SMSP Active Cycles       cycle       467.13
    Total SMSP Elapsed Cycles        cycle    3,756,392
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.262%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 92.62% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.262%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 92.62% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.125%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 49.23% above the average, while the minimum instance value is 92.72% below the      
          average.                                                                                                      

  triton_linear_kernel (1, 12, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle       20,260
    Memory Throughput                 %         6.69
    DRAM Throughput                   %         6.69
    Duration                         us        17.63
    L1/TEX Cache Throughput           %        36.22
    L2 Cache Throughput               %         6.40
    SM Active Cycles              cycle     1,974.68
    Compute (SM) Throughput           %         1.71
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.21
    Achieved Active Warps Per SM           warp         3.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.44%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,871.40
    Total DRAM Elapsed Cycles        cycle    1,118,976
    Average L1 Active Cycles         cycle     1,974.68
    Total L1 Elapsed Cycles          cycle    2,160,564
    Average L2 Active Cycles         cycle    14,930.88
    Total L2 Elapsed Cycles          cycle    1,536,160
    Average SM Active Cycles         cycle     1,974.68
    Total SM Elapsed Cycles          cycle    2,160,564
    Average SMSP Active Cycles       cycle     1,981.73
    Total SMSP Elapsed Cycles        cycle    8,642,256
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.789%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.04% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.83%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.13% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.789%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.04% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.332%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L2 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 6.86% above the average, while the minimum instance value is 5.22% below    
          the average.                                                                                                  

  void at::vectorized_elementwise_kernel<4, at::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3) (6, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.57
    SM Frequency                    Ghz         1.13
    Elapsed Cycles                cycle        3,746
    Memory Throughput                 %         0.76
    DRAM Throughput                   %         0.44
    Duration                         us         3.30
    L1/TEX Cache Throughput           %        12.73
    L2 Cache Throughput               %         0.93
    SM Active Cycles              cycle        94.28
    Compute (SM) Throughput           %         0.05
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      6
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             768
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 94.44%                                                                                          
          The grid for this launch is configured to execute only 6 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         5.80
    Achieved Active Warps Per SM           warp         3.71
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.2%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (5.8%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        22.90
    Total DRAM Elapsed Cycles        cycle      206,336
    Average L1 Active Cycles         cycle        94.28
    Total L1 Elapsed Cycles          cycle      403,894
    Average L2 Active Cycles         cycle       485.93
    Total L2 Elapsed Cycles          cycle      284,000
    Average SM Active Cycles         cycle        94.28
    Total SM Elapsed Cycles          cycle      403,894
    Average SMSP Active Cycles       cycle        88.23
    Total SMSP Elapsed Cycles        cycle    1,615,576
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.289%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 60.56% above the average, while the minimum instance value is 83.95% below the      
          average.                                                                                                      

  triton_linear_kernel (1, 12, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle       20,280
    Memory Throughput                 %         6.68
    DRAM Throughput                   %         6.68
    Duration                         us        17.63
    L1/TEX Cache Throughput           %        36.33
    L2 Cache Throughput               %         6.39
    SM Active Cycles              cycle     1,978.55
    Compute (SM) Throughput           %         1.65
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.26
    Achieved Active Warps Per SM           warp         4.01
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.3%                                                                                     
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.3%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,871.50
    Total DRAM Elapsed Cycles        cycle    1,120,512
    Average L1 Active Cycles         cycle     1,978.55
    Total L1 Elapsed Cycles          cycle    2,228,026
    Average L2 Active Cycles         cycle    14,949.59
    Total L2 Elapsed Cycles          cycle    1,537,280
    Average SM Active Cycles         cycle     1,978.55
    Total SM Elapsed Cycles          cycle    2,228,026
    Average SMSP Active Cycles       cycle     1,974.31
    Total SMSP Elapsed Cycles        cycle    8,912,104
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.546%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.11% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.546%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.30% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.546%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.11% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  triton_linear_kernel (1, 12, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle       20,426
    Memory Throughput                 %         6.63
    DRAM Throughput                   %         6.63
    Duration                         us        17.79
    L1/TEX Cache Throughput           %        36.40
    L2 Cache Throughput               %         6.33
    SM Active Cycles              cycle     1,975.09
    Compute (SM) Throughput           %         1.68
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.32
    Achieved Active Warps Per SM           warp         4.05
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.14%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.3%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,871.50
    Total DRAM Elapsed Cycles        cycle    1,129,472
    Average L1 Active Cycles         cycle     1,975.09
    Total L1 Elapsed Cycles          cycle    2,189,948
    Average L2 Active Cycles         cycle    14,951.36
    Total L2 Elapsed Cycles          cycle    1,548,400
    Average SM Active Cycles         cycle     1,975.09
    Total SM Elapsed Cycles          cycle    2,189,948
    Average SMSP Active Cycles       cycle     1,992.18
    Total SMSP Elapsed Cycles        cycle    8,759,792
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.687%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.19% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.751%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.07% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.687%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.19% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  void at::<unnamed>::CatArrayBatchedCopy<at::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, at::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, at::<unnamed>::TensorSizeStride<T2, 4>, int, T2) (216, 2, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.57
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        7,520
    Memory Throughput                 %         0.92
    DRAM Throughput                   %         0.24
    Duration                         us         6.59
    L1/TEX Cache Throughput           %         3.84
    L2 Cache Throughput               %         0.57
    SM Active Cycles              cycle     1,768.64
    Compute (SM) Throughput           %         5.10
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    432
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread         221,184
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        68.08
    Achieved Active Warps Per SM           warp        43.57
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 31.92%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (68.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24.60
    Total DRAM Elapsed Cycles        cycle      414,720
    Average L1 Active Cycles         cycle     1,768.64
    Total L1 Elapsed Cycles          cycle      798,422
    Average L2 Active Cycles         cycle       820.91
    Total L2 Elapsed Cycles          cycle      569,840
    Average SM Active Cycles         cycle     1,768.64
    Total SM Elapsed Cycles          cycle      798,422
    Average SMSP Active Cycles       cycle     1,711.01
    Total SMSP Elapsed Cycles        cycle    3,193,688
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 16.27%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 68.02% above the average, while the minimum instance value is 36.17% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 15.72%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 67.92% above the average, while the minimum instance value is 43.37% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 16.27%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 68.02% above the average, while the minimum instance value is 36.17% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.189%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 53.70% above the average, while the minimum instance value is 90.50% below the      
          average.                                                                                                      

  void at::<unnamed>::CatArrayBatchedCopy<at::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, at::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, at::<unnamed>::TensorSizeStride<T2, 4>, int, T2) (216, 2, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        7,441
    Memory Throughput                 %         0.90
    DRAM Throughput                   %         0.24
    Duration                         us         6.50
    L1/TEX Cache Throughput           %         3.84
    L2 Cache Throughput               %         0.64
    SM Active Cycles              cycle     1,764.46
    Compute (SM) Throughput           %         5.00
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    432
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread         221,184
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        68.58
    Achieved Active Warps Per SM           warp        43.89
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 31.42%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (68.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24.70
    Total DRAM Elapsed Cycles        cycle      410,752
    Average L1 Active Cycles         cycle     1,764.46
    Total L1 Elapsed Cycles          cycle      813,428
    Average L2 Active Cycles         cycle       913.44
    Total L2 Elapsed Cycles          cycle      563,920
    Average SM Active Cycles         cycle     1,764.46
    Total SM Elapsed Cycles          cycle      813,428
    Average SMSP Active Cycles       cycle     1,707.24
    Total SMSP Elapsed Cycles        cycle    3,253,712
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 15.78%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 67.37% above the average, while the minimum instance value is 36.52% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 15.58%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 68.73% above the average, while the minimum instance value is 37.44% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 15.78%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 67.37% above the average, while the minimum instance value is 36.52% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.181%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 39.98% above the average, while the minimum instance value is 91.46% below the      
          average.                                                                                                      

  fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::Sm80, 1, 64, 64, 64, 1, 1>::Params) (1, 12, 1)x(32, 4, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle       17,637
    Memory Throughput                 %         2.16
    DRAM Throughput                   %         0.45
    Duration                         us        15.33
    L1/TEX Cache Throughput           %        20.88
    L2 Cache Throughput               %         2.43
    SM Active Cycles              cycle     1,702.24
    Compute (SM) Throughput           %         2.32
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread             168
    Shared Memory Configuration Size           Kbyte          135.17
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           36.35
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.04
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %        18.75
    Achieved Occupancy                        %         6.33
    Achieved Active Warps Per SM           warp         4.05
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.22%                                                                                    
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.3%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 81.25%                                                                                    
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (18.8%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       110.10
    Total DRAM Elapsed Cycles        cycle      972,800
    Average L1 Active Cycles         cycle     1,702.24
    Total L1 Elapsed Cycles          cycle    1,915,820
    Average L2 Active Cycles         cycle     4,132.99
    Total L2 Elapsed Cycles          cycle    1,336,080
    Average SM Active Cycles         cycle     1,702.24
    Total SM Elapsed Cycles          cycle    1,915,820
    Average SMSP Active Cycles       cycle     1,735.85
    Total SMSP Elapsed Cycles        cycle    7,663,280
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.547%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.07% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.708%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 88.99% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.547%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.07% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.444%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 30.08% above the average, while the minimum instance value is 48.56% below the      
          average.                                                                                                      

  ampere_sgemm_128x32_tn (6, 1, 13)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.13
    Elapsed Cycles                cycle       13,787
    Memory Throughput                 %        23.49
    DRAM Throughput                   %         9.71
    Duration                         us        12.16
    L1/TEX Cache Throughput           %        41.65
    L2 Cache Throughput               %        13.47
    SM Active Cycles              cycle     7,742.04
    Compute (SM) Throughput           %        21.83
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     78
    Registers Per Thread             register/thread              57
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           16.38
    # SMs                                         SM             108
    Threads                                   thread          19,968
    Uses Green Context                                             0
    Waves Per SM                                                0.18
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 27.78%                                                                                          
          The grid for this launch is configured to execute only 78 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            5
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        12.42
    Achieved Active Warps Per SM           warp         7.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75.15%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (12.4%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,872.20
    Total DRAM Elapsed Cycles        cycle      771,072
    Average L1 Active Cycles         cycle     7,742.04
    Total L1 Elapsed Cycles          cycle    1,482,330
    Average L2 Active Cycles         cycle     8,800.75
    Total L2 Elapsed Cycles          cycle    1,049,600
    Average SM Active Cycles         cycle     7,742.04
    Total SM Elapsed Cycles          cycle    1,482,330
    Average SMSP Active Cycles       cycle     7,690.53
    Total SMSP Elapsed Cycles        cycle    5,929,320
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 18.59%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 32.95% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.29%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 32.63% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.59%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 32.95% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.25%                                                                                           
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L2 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 7.83% above the average, while the minimum instance value is 6.25% below    
          the average.                                                                                                  

  void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T9 *, T8 *, T5 *, const T6 *, const T6 *, const T10 *, const T4 *, T10 *, void *, long, T6 *, int *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *) (24, 1, 1)x(32, 16, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        6,435
    Memory Throughput                 %         2.92
    DRAM Throughput                   %         2.92
    Duration                         us         5.63
    L1/TEX Cache Throughput           %         5.93
    L2 Cache Throughput               %         3.98
    SM Active Cycles              cycle       888.75
    Compute (SM) Throughput           %         1.70
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     24
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          12,288
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 77.78%                                                                                          
          The grid for this launch is configured to execute only 24 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        17.70
    Achieved Active Warps Per SM           warp        11.33
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 82.3%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (17.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       259.40
    Total DRAM Elapsed Cycles        cycle      355,840
    Average L1 Active Cycles         cycle       888.75
    Total L1 Elapsed Cycles          cycle      717,718
    Average L2 Active Cycles         cycle     2,476.69
    Total L2 Elapsed Cycles          cycle      487,520
    Average SM Active Cycles         cycle       888.75
    Total SM Elapsed Cycles          cycle      717,718
    Average SMSP Active Cycles       cycle       882.99
    Total SMSP Elapsed Cycles        cycle    2,870,872
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 10.62%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 79.39% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.48%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.84% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.62%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 79.39% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.056%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L2 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 17.36% above the average, while the minimum instance value is 19.09% below  
          the average.                                                                                                  

  void at::vectorized_elementwise_kernel<4, at::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3) (6, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.57
    SM Frequency                    Ghz         1.13
    Elapsed Cycles                cycle        3,807
    Memory Throughput                 %         0.87
    DRAM Throughput                   %         0.81
    Duration                         us         3.36
    L1/TEX Cache Throughput           %        13.22
    L2 Cache Throughput               %         1.10
    SM Active Cycles              cycle        97.50
    Compute (SM) Throughput           %         0.05
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      6
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             768
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 94.44%                                                                                          
          The grid for this launch is configured to execute only 6 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         6.45
    Achieved Active Warps Per SM           warp         4.13
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.55%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.5%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        42.50
    Total DRAM Elapsed Cycles        cycle      210,432
    Average L1 Active Cycles         cycle        97.50
    Total L1 Elapsed Cycles          cycle      431,192
    Average L2 Active Cycles         cycle       694.65
    Total L2 Elapsed Cycles          cycle      288,800
    Average SM Active Cycles         cycle        97.50
    Total SM Elapsed Cycles          cycle      431,192
    Average SMSP Active Cycles       cycle        97.83
    Total SMSP Elapsed Cycles        cycle    1,724,768
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 9.023%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 46.89% above the average, while the minimum instance value is 89.20% below the      
          average.                                                                                                      

  void at::<unnamed>::vectorized_layer_norm_kernel<float, float, 0>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *) (8, 1, 1)x(32, 4, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        8,321
    Memory Throughput                 %         0.70
    DRAM Throughput                   %         0.38
    Duration                         us         7.26
    L1/TEX Cache Throughput           %         6.17
    L2 Cache Throughput               %         0.70
    SM Active Cycles              cycle       464.34
    Compute (SM) Throughput           %         0.46
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              38
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block              24
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 92.59%                                                                                          
          The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           28
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %           75
    Achieved Occupancy                        %         5.94
    Achieved Active Warps Per SM           warp         3.80
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 92.08%                                                                                    
          The difference between calculated theoretical (75.0%) and measured achieved occupancy (5.9%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 25%                                                                                       
          The 12.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the      
          hardware maximum of 16. This kernel's theoretical occupancy (75.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        43.50
    Total DRAM Elapsed Cycles        cycle      460,288
    Average L1 Active Cycles         cycle       464.34
    Total L1 Elapsed Cycles          cycle      901,870
    Average L2 Active Cycles         cycle       828.42
    Total L2 Elapsed Cycles          cycle      631,120
    Average SM Active Cycles         cycle       464.34
    Total SM Elapsed Cycles          cycle      901,870
    Average SMSP Active Cycles       cycle       451.38
    Total SMSP Elapsed Cycles        cycle    3,607,480
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.152%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 92.66% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.028%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 93.03% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.152%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 92.66% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.49%                                                                                           
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 52.28% above the average, while the minimum instance value is 70.67% below the      
          average.                                                                                                      

  triton_linear_kernel (1, 48, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle       21,417
    Memory Throughput                 %        24.84
    DRAM Throughput                   %        24.84
    Duration                         us        18.78
    L1/TEX Cache Throughput           %        35.13
    L2 Cache Throughput               %        23.05
    SM Active Cycles              cycle     8,185.92
    Compute (SM) Throughput           %         6.38
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     48
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           6,144
    Uses Green Context                                             0
    Waves Per SM                                                0.07
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 55.56%                                                                                          
          The grid for this launch is configured to execute only 48 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.26
    Achieved Active Warps Per SM           warp         4.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.31%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.3%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     7,408.30
    Total DRAM Elapsed Cycles        cycle    1,192,960
    Average L1 Active Cycles         cycle     8,185.92
    Total L1 Elapsed Cycles          cycle    2,309,638
    Average L2 Active Cycles         cycle    16,416.99
    Total L2 Elapsed Cycles          cycle    1,626,560
    Average SM Active Cycles         cycle     8,185.92
    Total SM Elapsed Cycles          cycle    2,309,638
    Average SMSP Active Cycles       cycle     8,152.65
    Total SMSP Elapsed Cycles        cycle    9,238,552
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 21.96%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 57.36% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.02%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 57.77% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.96%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 57.36% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  void at::vectorized_elementwise_kernel<4, at::<unnamed>::launch_clamp_scalar(at::TensorIteratorBase &, c10::Scalar, c10::Scalar, detail::ClampLimits)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3) (24, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        4,133
    Memory Throughput                 %         1.62
    DRAM Throughput                   %         1.44
    Duration                         us         3.62
    L1/TEX Cache Throughput           %         8.76
    L2 Cache Throughput               %         3.46
    SM Active Cycles              cycle       410.97
    Compute (SM) Throughput           %         0.24
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     24
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           3,072
    Uses Green Context                                             0
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 77.78%                                                                                          
          The grid for this launch is configured to execute only 24 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         5.98
    Achieved Active Warps Per SM           warp         3.83
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.02%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.0%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        82.50
    Total DRAM Elapsed Cycles        cycle      229,504
    Average L1 Active Cycles         cycle       410.97
    Total L1 Elapsed Cycles          cycle      447,380
    Average L2 Active Cycles         cycle     1,174.51
    Total L2 Elapsed Cycles          cycle      313,200
    Average SM Active Cycles         cycle       410.97
    Total SM Elapsed Cycles          cycle      447,380
    Average SMSP Active Cycles       cycle       398.01
    Total SMSP Elapsed Cycles        cycle    1,789,520
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.991%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 80.54% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.668%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 79.81% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.991%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 80.54% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.26%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 37.53% above the average, while the minimum instance value is 42.87% below the      
          average.                                                                                                      

  ampere_sgemm_128x32_sliced1x4_tn (6, 1, 9)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle       36,986
    Memory Throughput                 %        24.10
    DRAM Throughput                   %        14.52
    Duration                         us        32.35
    L1/TEX Cache Throughput           %        52.07
    L2 Cache Throughput               %        14.31
    SM Active Cycles              cycle    16,997.97
    Compute (SM) Throughput           %        31.38
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.5 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     54
    Registers Per Thread             register/thread             134
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.01
    # SMs                                         SM             108
    Threads                                   thread          13,824
    Uses Green Context                                             0
    Waves Per SM                                                0.50
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          The grid for this launch is configured to execute only 54 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        12.50
    Achieved Occupancy                        %        12.41
    Achieved Active Warps Per SM           warp         7.94
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 87.5%                                                                                     
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     7,467.40
    Total DRAM Elapsed Cycles        cycle    2,057,216
    Average L1 Active Cycles         cycle    16,997.97
    Total L1 Elapsed Cycles          cycle    3,966,796
    Average L2 Active Cycles         cycle    22,923.70
    Total L2 Elapsed Cycles          cycle    2,811,600
    Average SM Active Cycles         cycle    16,997.97
    Total SM Elapsed Cycles          cycle    3,966,796
    Average SMSP Active Cycles       cycle    16,947.49
    Total SMSP Elapsed Cycles        cycle   15,867,184
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.65%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 51.11% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.51%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 50.95% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.65%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 51.11% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.203%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 7.98% above the average, while the minimum instance value is 5.67% below the        
          average.                                                                                                      

  void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T9 *, T8 *, T5 *, const T6 *, const T6 *, const T10 *, const T4 *, T10 *, void *, long, T6 *, int *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *) (24, 1, 1)x(32, 16, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.57
    SM Frequency                    Ghz         1.13
    Elapsed Cycles                cycle        5,486
    Memory Throughput                 %         2.40
    DRAM Throughput                   %         2.40
    Duration                         us         4.83
    L1/TEX Cache Throughput           %         6.49
    L2 Cache Throughput               %         3.68
    SM Active Cycles              cycle       697.95
    Compute (SM) Throughput           %         1.87
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     24
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          12,288
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 77.78%                                                                                          
          The grid for this launch is configured to execute only 24 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        19.71
    Achieved Active Warps Per SM           warp        12.61
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 80.29%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (19.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       182.60
    Total DRAM Elapsed Cycles        cycle      304,384
    Average L1 Active Cycles         cycle       697.95
    Total L1 Elapsed Cycles          cycle      603,226
    Average L2 Active Cycles         cycle     1,865.33
    Total L2 Elapsed Cycles          cycle      415,680
    Average SM Active Cycles         cycle       697.95
    Total SM Elapsed Cycles          cycle      603,226
    Average SMSP Active Cycles       cycle       691.65
    Total SMSP Elapsed Cycles        cycle    2,412,904
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 9.911%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 79.31% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 9.83%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 79.38% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 9.911%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 79.31% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.171%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L2 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 22.76% above the average, while the minimum instance value is 22.00% below  
          the average.                                                                                                  

  void at::vectorized_elementwise_kernel<4, at::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3) (6, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        3,870
    Memory Throughput                 %         0.85
    DRAM Throughput                   %         0.81
    Duration                         us         3.39
    L1/TEX Cache Throughput           %        13.37
    L2 Cache Throughput               %         0.98
    SM Active Cycles              cycle        96.37
    Compute (SM) Throughput           %         0.05
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      6
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             768
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 94.44%                                                                                          
          The grid for this launch is configured to execute only 6 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         5.98
    Achieved Active Warps Per SM           warp         3.83
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.02%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.0%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        43.10
    Total DRAM Elapsed Cycles        cycle      214,016
    Average L1 Active Cycles         cycle        96.37
    Total L1 Elapsed Cycles          cycle      411,940
    Average L2 Active Cycles         cycle       653.98
    Total L2 Elapsed Cycles          cycle      293,360
    Average SM Active Cycles         cycle        96.37
    Total SM Elapsed Cycles          cycle      411,940
    Average SMSP Active Cycles       cycle        93.87
    Total SMSP Elapsed Cycles        cycle    1,647,760
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.26%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 63.11% above the average, while the minimum instance value is 85.47% below the      
          average.                                                                                                      

  void at::<unnamed>::vectorized_layer_norm_kernel<float, float, 0>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *) (8, 1, 1)x(32, 4, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        8,588
    Memory Throughput                 %         0.70
    DRAM Throughput                   %         0.37
    Duration                         us         7.49
    L1/TEX Cache Throughput           %         5.97
    L2 Cache Throughput               %         0.86
    SM Active Cycles              cycle       480.05
    Compute (SM) Throughput           %         0.46
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              38
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block              24
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 92.59%                                                                                          
          The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           28
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %           75
    Achieved Occupancy                        %         5.84
    Achieved Active Warps Per SM           warp         3.74
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 92.22%                                                                                    
          The difference between calculated theoretical (75.0%) and measured achieved occupancy (5.8%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 25%                                                                                       
          The 12.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the      
          hardware maximum of 16. This kernel's theoretical occupancy (75.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        43.50
    Total DRAM Elapsed Cycles        cycle      473,472
    Average L1 Active Cycles         cycle       480.05
    Total L1 Elapsed Cycles          cycle      906,594
    Average L2 Active Cycles         cycle     1,026.64
    Total L2 Elapsed Cycles          cycle      651,120
    Average SM Active Cycles         cycle       480.05
    Total SM Elapsed Cycles          cycle      906,594
    Average SMSP Active Cycles       cycle       448.12
    Total SMSP Elapsed Cycles        cycle    3,626,376
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.299%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 92.66% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.299%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 92.66% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.722%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 53.29% above the average, while the minimum instance value is 62.69% below the      
          average.                                                                                                      

  triton_linear_kernel (1, 12, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle       20,545
    Memory Throughput                 %         6.59
    DRAM Throughput                   %         6.59
    Duration                         us        17.86
    L1/TEX Cache Throughput           %        36.51
    L2 Cache Throughput               %         6.30
    SM Active Cycles              cycle     1,969.59
    Compute (SM) Throughput           %         1.67
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.29
    Achieved Active Warps Per SM           warp         4.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.23%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.3%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,871.50
    Total DRAM Elapsed Cycles        cycle    1,135,104
    Average L1 Active Cycles         cycle     1,969.59
    Total L1 Elapsed Cycles          cycle    2,201,486
    Average L2 Active Cycles         cycle    14,988.09
    Total L2 Elapsed Cycles          cycle    1,557,840
    Average SM Active Cycles         cycle     1,969.59
    Total SM Elapsed Cycles          cycle    2,201,486
    Average SMSP Active Cycles       cycle     1,970.25
    Total SMSP Elapsed Cycles        cycle    8,805,944
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.632%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.33% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.627%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.26% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.632%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.33% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  void at::vectorized_elementwise_kernel<4, at::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3) (6, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        3,720
    Memory Throughput                 %         0.77
    DRAM Throughput                   %         0.44
    Duration                         us         3.26
    L1/TEX Cache Throughput           %        13.20
    L2 Cache Throughput               %         0.99
    SM Active Cycles              cycle        90.88
    Compute (SM) Throughput           %         0.05
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      6
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             768
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 94.44%                                                                                          
          The grid for this launch is configured to execute only 6 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         6.64
    Achieved Active Warps Per SM           warp         4.25
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.36%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.6%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        22.90
    Total DRAM Elapsed Cycles        cycle      205,952
    Average L1 Active Cycles         cycle        90.88
    Total L1 Elapsed Cycles          cycle      419,912
    Average L2 Active Cycles         cycle       555.69
    Total L2 Elapsed Cycles          cycle      282,080
    Average SM Active Cycles         cycle        90.88
    Total SM Elapsed Cycles          cycle      419,912
    Average SMSP Active Cycles       cycle        90.76
    Total SMSP Elapsed Cycles        cycle    1,679,648
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 9.753%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 61.89% above the average, while the minimum instance value is 86.50% below the      
          average.                                                                                                      

  triton_linear_kernel (1, 12, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle       20,281
    Memory Throughput                 %         6.69
    DRAM Throughput                   %         6.69
    Duration                         us        17.63
    L1/TEX Cache Throughput           %        36.45
    L2 Cache Throughput               %         6.38
    SM Active Cycles              cycle     1,977.63
    Compute (SM) Throughput           %         1.70
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.24
    Achieved Active Warps Per SM           warp         3.99
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.36%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,871.50
    Total DRAM Elapsed Cycles        cycle    1,118,208
    Average L1 Active Cycles         cycle     1,977.63
    Total L1 Elapsed Cycles          cycle    2,173,362
    Average L2 Active Cycles         cycle    14,817.08
    Total L2 Elapsed Cycles          cycle    1,536,480
    Average SM Active Cycles         cycle     1,977.63
    Total SM Elapsed Cycles          cycle    2,173,362
    Average SMSP Active Cycles       cycle     1,963.79
    Total SMSP Elapsed Cycles        cycle    8,693,448
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.754%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.08% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.69%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.05% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.754%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.08% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  triton_linear_kernel (1, 12, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle       20,327
    Memory Throughput                 %         6.66
    DRAM Throughput                   %         6.66
    Duration                         us        17.70
    L1/TEX Cache Throughput           %        36.50
    L2 Cache Throughput               %         6.36
    SM Active Cycles              cycle     1,969.07
    Compute (SM) Throughput           %         1.67
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.26
    Achieved Active Warps Per SM           warp         4.01
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.31%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.3%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,871.50
    Total DRAM Elapsed Cycles        cycle    1,124,352
    Average L1 Active Cycles         cycle     1,969.07
    Total L1 Elapsed Cycles          cycle    2,210,432
    Average L2 Active Cycles         cycle    14,805.41
    Total L2 Elapsed Cycles          cycle    1,540,720
    Average SM Active Cycles         cycle     1,969.07
    Total SM Elapsed Cycles          cycle    2,210,432
    Average SMSP Active Cycles       cycle     1,968.91
    Total SMSP Elapsed Cycles        cycle    8,841,728
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.573%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.11% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.576%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.15% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.573%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.11% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  void at::<unnamed>::CatArrayBatchedCopy<at::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, at::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, at::<unnamed>::TensorSizeStride<T2, 4>, int, T2) (216, 2, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        7,663
    Memory Throughput                 %         0.93
    DRAM Throughput                   %         0.23
    Duration                         us         6.69
    L1/TEX Cache Throughput           %         3.86
    L2 Cache Throughput               %         0.55
    SM Active Cycles              cycle     1,760.38
    Compute (SM) Throughput           %         5.17
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    432
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread         221,184
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        68.86
    Achieved Active Warps Per SM           warp        44.07
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 31.14%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (68.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24.60
    Total DRAM Elapsed Cycles        cycle      422,400
    Average L1 Active Cycles         cycle     1,760.38
    Total L1 Elapsed Cycles          cycle      787,664
    Average L2 Active Cycles         cycle          884
    Total L2 Elapsed Cycles          cycle      580,560
    Average SM Active Cycles         cycle     1,760.38
    Total SM Elapsed Cycles          cycle      787,664
    Average SMSP Active Cycles       cycle     1,706.16
    Total SMSP Elapsed Cycles        cycle    3,150,656
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 16.58%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 68.70% above the average, while the minimum instance value is 36.49% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 15.94%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 68.13% above the average, while the minimum instance value is 43.79% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 16.58%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 68.70% above the average, while the minimum instance value is 36.49% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.392%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 44.26% above the average, while the minimum instance value is 91.18% below the      
          average.                                                                                                      

  void at::<unnamed>::CatArrayBatchedCopy<at::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, at::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, at::<unnamed>::TensorSizeStride<T2, 4>, int, T2) (216, 2, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        7,659
    Memory Throughput                 %         0.93
    DRAM Throughput                   %         0.23
    Duration                         us         6.69
    L1/TEX Cache Throughput           %         3.84
    L2 Cache Throughput               %         0.61
    SM Active Cycles              cycle     1,769.41
    Compute (SM) Throughput           %         5.17
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    432
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread         221,184
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        68.36
    Achieved Active Warps Per SM           warp        43.75
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 31.64%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (68.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24.70
    Total DRAM Elapsed Cycles        cycle      422,400
    Average L1 Active Cycles         cycle     1,769.41
    Total L1 Elapsed Cycles          cycle      786,392
    Average L2 Active Cycles         cycle       807.40
    Total L2 Elapsed Cycles          cycle      580,720
    Average SM Active Cycles         cycle     1,769.41
    Total SM Elapsed Cycles          cycle      786,392
    Average SMSP Active Cycles       cycle     1,715.21
    Total SMSP Elapsed Cycles        cycle    3,145,568
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 16.66%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 68.54% above the average, while the minimum instance value is 36.93% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 15.98%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 67.84% above the average, while the minimum instance value is 43.16% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 16.66%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 68.54% above the average, while the minimum instance value is 36.93% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.028%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 63.18% above the average, while the minimum instance value is 90.71% below the      
          average.                                                                                                      

  fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::Sm80, 1, 64, 64, 64, 1, 1>::Params) (1, 12, 1)x(32, 4, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle       18,082
    Memory Throughput                 %         2.11
    DRAM Throughput                   %         0.44
    Duration                         us        15.71
    L1/TEX Cache Throughput           %        20.12
    L2 Cache Throughput               %         2.36
    SM Active Cycles              cycle     1,767.04
    Compute (SM) Throughput           %         2.33
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread             168
    Shared Memory Configuration Size           Kbyte          135.17
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           36.35
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.04
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %        18.75
    Achieved Occupancy                        %         6.02
    Achieved Active Warps Per SM           warp         3.85
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 67.88%                                                                                    
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.0%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 81.25%                                                                                    
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (18.8%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       110.20
    Total DRAM Elapsed Cycles        cycle      997,376
    Average L1 Active Cycles         cycle     1,767.04
    Total L1 Elapsed Cycles          cycle    1,903,850
    Average L2 Active Cycles         cycle     4,308.29
    Total L2 Elapsed Cycles          cycle    1,370,320
    Average SM Active Cycles         cycle     1,767.04
    Total SM Elapsed Cycles          cycle    1,903,850
    Average SMSP Active Cycles       cycle     1,719.47
    Total SMSP Elapsed Cycles        cycle    7,615,400
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.921%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.00% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.685%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.04% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.921%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.00% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.788%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 34.94% above the average, while the minimum instance value is 51.65% below the      
          average.                                                                                                      

  ampere_sgemm_128x32_tn (6, 1, 13)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.13
    Elapsed Cycles                cycle       13,812
    Memory Throughput                 %        23.35
    DRAM Throughput                   %         9.69
    Duration                         us        12.19
    L1/TEX Cache Throughput           %        41.71
    L2 Cache Throughput               %        13.55
    SM Active Cycles              cycle     7,729.91
    Compute (SM) Throughput           %        21.70
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     78
    Registers Per Thread             register/thread              57
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           16.38
    # SMs                                         SM             108
    Threads                                   thread          19,968
    Uses Green Context                                             0
    Waves Per SM                                                0.18
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 27.78%                                                                                          
          The grid for this launch is configured to execute only 78 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            5
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        12.46
    Achieved Active Warps Per SM           warp         7.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75.08%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (12.5%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,872.20
    Total DRAM Elapsed Cycles        cycle      773,120
    Average L1 Active Cycles         cycle     7,729.91
    Total L1 Elapsed Cycles          cycle    1,491,338
    Average L2 Active Cycles         cycle     8,624.10
    Total L2 Elapsed Cycles          cycle    1,051,200
    Average SM Active Cycles         cycle     7,729.91
    Total SM Elapsed Cycles          cycle    1,491,338
    Average SMSP Active Cycles       cycle     7,739.93
    Total SMSP Elapsed Cycles        cycle    5,965,352
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 18.42%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 32.91% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.38%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 32.80% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.42%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 32.91% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.552%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L2 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 8.46% above the average, while the minimum instance value is 8.73% below    
          the average.                                                                                                  

  void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T9 *, T8 *, T5 *, const T6 *, const T6 *, const T10 *, const T4 *, T10 *, void *, long, T6 *, int *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *) (24, 1, 1)x(32, 16, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        6,623
    Memory Throughput                 %         2.82
    DRAM Throughput                   %         2.82
    Duration                         us         5.79
    L1/TEX Cache Throughput           %         5.52
    L2 Cache Throughput               %         3.88
    SM Active Cycles              cycle       952.48
    Compute (SM) Throughput           %         1.72
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     24
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          12,288
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 77.78%                                                                                          
          The grid for this launch is configured to execute only 24 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.66
    Achieved Active Warps Per SM           warp        10.66
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.34%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (16.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       259.40
    Total DRAM Elapsed Cycles        cycle      368,128
    Average L1 Active Cycles         cycle       952.48
    Total L1 Elapsed Cycles          cycle      706,798
    Average L2 Active Cycles         cycle     2,544.59
    Total L2 Elapsed Cycles          cycle      502,000
    Average SM Active Cycles         cycle       952.48
    Total SM Elapsed Cycles          cycle      706,798
    Average SMSP Active Cycles       cycle       883.27
    Total SMSP Elapsed Cycles        cycle    2,827,192
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.52%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 79.12% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.78%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 79.90% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.52%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 79.12% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.631%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 16.35% above the average, while the minimum instance value is 21.28% below the      
          average.                                                                                                      

  void at::vectorized_elementwise_kernel<4, at::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3) (6, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.55
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        4,047
    Memory Throughput                 %         0.83
    DRAM Throughput                   %         0.77
    Duration                         us         3.55
    L1/TEX Cache Throughput           %        12.56
    L2 Cache Throughput               %         1.08
    SM Active Cycles              cycle       102.61
    Compute (SM) Throughput           %         0.05
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      6
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             768
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 94.44%                                                                                          
          The grid for this launch is configured to execute only 6 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         5.87
    Achieved Active Warps Per SM           warp         3.76
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.13%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (5.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        42.50
    Total DRAM Elapsed Cycles        cycle      220,928
    Average L1 Active Cycles         cycle       102.61
    Total L1 Elapsed Cycles          cycle      412,756
    Average L2 Active Cycles         cycle       749.16
    Total L2 Elapsed Cycles          cycle      306,880
    Average SM Active Cycles         cycle       102.61
    Total SM Elapsed Cycles          cycle      412,756
    Average SMSP Active Cycles       cycle        97.61
    Total SMSP Elapsed Cycles        cycle    1,651,024
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 9.866%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 50.52% above the average, while the minimum instance value is 87.32% below the      
          average.                                                                                                      

  void at::<unnamed>::vectorized_layer_norm_kernel<float, float, 0>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *) (8, 1, 1)x(32, 4, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        8,579
    Memory Throughput                 %         0.69
    DRAM Throughput                   %         0.37
    Duration                         us         7.49
    L1/TEX Cache Throughput           %         5.96
    L2 Cache Throughput               %         0.79
    SM Active Cycles              cycle       480.62
    Compute (SM) Throughput           %         0.46
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              38
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block              24
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 92.59%                                                                                          
          The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           28
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %           75
    Achieved Occupancy                        %         5.80
    Achieved Active Warps Per SM           warp         3.71
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 92.26%                                                                                    
          The difference between calculated theoretical (75.0%) and measured achieved occupancy (5.8%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 25%                                                                                       
          The 12.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the      
          hardware maximum of 16. This kernel's theoretical occupancy (75.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        43.50
    Total DRAM Elapsed Cycles        cycle      474,880
    Average L1 Active Cycles         cycle       480.62
    Total L1 Elapsed Cycles          cycle      899,584
    Average L2 Active Cycles         cycle       886.41
    Total L2 Elapsed Cycles          cycle      650,480
    Average SM Active Cycles         cycle       480.62
    Total SM Elapsed Cycles          cycle      899,584
    Average SMSP Active Cycles       cycle       453.05
    Total SMSP Elapsed Cycles        cycle    3,598,336
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.347%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 92.66% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.06%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 93.03% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.347%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 92.66% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.034%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 46.18% above the average, while the minimum instance value is 91.54% below the      
          average.                                                                                                      

  triton_linear_kernel (1, 48, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle       21,378
    Memory Throughput                 %        24.87
    DRAM Throughput                   %        24.87
    Duration                         us        18.75
    L1/TEX Cache Throughput           %        35.21
    L2 Cache Throughput               %        23.14
    SM Active Cycles              cycle     8,174.61
    Compute (SM) Throughput           %         6.41
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     48
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           6,144
    Uses Green Context                                             0
    Waves Per SM                                                0.07
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 55.56%                                                                                          
          The grid for this launch is configured to execute only 48 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.23
    Achieved Active Warps Per SM           warp         3.99
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.39%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     7,408.30
    Total DRAM Elapsed Cycles        cycle    1,191,424
    Average L1 Active Cycles         cycle     8,174.61
    Total L1 Elapsed Cycles          cycle    2,300,900
    Average L2 Active Cycles         cycle    16,606.30
    Total L2 Elapsed Cycles          cycle    1,623,120
    Average SM Active Cycles         cycle     8,174.61
    Total SM Elapsed Cycles          cycle    2,300,900
    Average SMSP Active Cycles       cycle     8,133.96
    Total SMSP Elapsed Cycles        cycle    9,203,600
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 22.08%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 57.55% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.77%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 57.03% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.08%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 57.55% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  void at::vectorized_elementwise_kernel<4, at::<unnamed>::launch_clamp_scalar(at::TensorIteratorBase &, c10::Scalar, c10::Scalar, detail::ClampLimits)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3) (24, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        4,061
    Memory Throughput                 %         1.65
    DRAM Throughput                   %         1.47
    Duration                         us         3.55
    L1/TEX Cache Throughput           %         9.25
    L2 Cache Throughput               %         3.50
    SM Active Cycles              cycle       389.08
    Compute (SM) Throughput           %         0.25
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     24
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           3,072
    Uses Green Context                                             0
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 77.78%                                                                                          
          The grid for this launch is configured to execute only 24 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         6.33
    Achieved Active Warps Per SM           warp         4.05
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.67%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.3%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        82.50
    Total DRAM Elapsed Cycles        cycle      224,512
    Average L1 Active Cycles         cycle       389.08
    Total L1 Elapsed Cycles          cycle      437,698
    Average L2 Active Cycles         cycle     1,069.17
    Total L2 Elapsed Cycles          cycle      307,760
    Average SM Active Cycles         cycle       389.08
    Total SM Elapsed Cycles          cycle      437,698
    Average SMSP Active Cycles       cycle       378.88
    Total SMSP Elapsed Cycles        cycle    1,750,792
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.638%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 79.55% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.5%                                                                                            
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 80.23% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.638%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 79.55% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.19%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 36.66% above the average, while the minimum instance value is 39.02% below the      
          average.                                                                                                      

  ampere_sgemm_128x32_sliced1x4_tn (6, 1, 9)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle       36,729
    Memory Throughput                 %        24.10
    DRAM Throughput                   %        14.61
    Duration                         us        32.13
    L1/TEX Cache Throughput           %        52.26
    L2 Cache Throughput               %        14.40
    SM Active Cycles              cycle    16,934.56
    Compute (SM) Throughput           %        31.39
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.5 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     54
    Registers Per Thread             register/thread             134
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.01
    # SMs                                         SM             108
    Threads                                   thread          13,824
    Uses Green Context                                             0
    Waves Per SM                                                0.50
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          The grid for this launch is configured to execute only 54 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        12.50
    Achieved Occupancy                        %        12.52
    Achieved Active Warps Per SM           warp         8.02
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 87.5%                                                                                     
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     7,467.40
    Total DRAM Elapsed Cycles        cycle    2,044,416
    Average L1 Active Cycles         cycle    16,934.56
    Total L1 Elapsed Cycles          cycle    3,966,048
    Average L2 Active Cycles         cycle    23,161.92
    Total L2 Elapsed Cycles          cycle    2,794,160
    Average SM Active Cycles         cycle    16,934.56
    Total SM Elapsed Cycles          cycle    3,966,048
    Average SMSP Active Cycles       cycle    16,939.99
    Total SMSP Elapsed Cycles        cycle   15,864,192
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.41%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 50.76% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.54%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 51.04% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.41%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 50.76% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T9 *, T8 *, T5 *, const T6 *, const T6 *, const T10 *, const T4 *, T10 *, void *, long, T6 *, int *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *) (24, 1, 1)x(32, 16, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        5,961
    Memory Throughput                 %         2.22
    DRAM Throughput                   %         2.22
    Duration                         us         5.22
    L1/TEX Cache Throughput           %         5.83
    L2 Cache Throughput               %         3.38
    SM Active Cycles              cycle       780.56
    Compute (SM) Throughput           %         1.85
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     24
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          12,288
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 77.78%                                                                                          
          The grid for this launch is configured to execute only 24 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        17.43
    Achieved Active Warps Per SM           warp        11.15
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 82.57%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (17.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       182.60
    Total DRAM Elapsed Cycles        cycle      328,704
    Average L1 Active Cycles         cycle       780.56
    Total L1 Elapsed Cycles          cycle      610,864
    Average L2 Active Cycles         cycle     1,870.36
    Total L2 Elapsed Cycles          cycle      451,760
    Average SM Active Cycles         cycle       780.56
    Total SM Elapsed Cycles          cycle      610,864
    Average SMSP Active Cycles       cycle       757.03
    Total SMSP Elapsed Cycles        cycle    2,443,456
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.03%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 79.91% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.66%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 79.64% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.03%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 79.91% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.33%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 37.24% above the average, while the minimum instance value is 44.02% below the      
          average.                                                                                                      

  void at::vectorized_elementwise_kernel<4, at::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3) (6, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.56
    SM Frequency                    Ghz         1.13
    Elapsed Cycles                cycle        4,057
    Memory Throughput                 %         0.83
    DRAM Throughput                   %         0.76
    Duration                         us         3.58
    L1/TEX Cache Throughput           %        12.66
    L2 Cache Throughput               %         1.14
    SM Active Cycles              cycle       101.81
    Compute (SM) Throughput           %         0.05
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      6
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             768
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 94.44%                                                                                          
          The grid for this launch is configured to execute only 6 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         6.54
    Achieved Active Warps Per SM           warp         4.19
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.46%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.5%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        42.50
    Total DRAM Elapsed Cycles        cycle      223,232
    Average L1 Active Cycles         cycle       101.81
    Total L1 Elapsed Cycles          cycle      429,610
    Average L2 Active Cycles         cycle       828.70
    Total L2 Elapsed Cycles          cycle      307,600
    Average SM Active Cycles         cycle       101.81
    Total SM Elapsed Cycles          cycle      429,610
    Average SMSP Active Cycles       cycle       100.25
    Total SMSP Elapsed Cycles        cycle    1,718,440
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.381%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 38.89% above the average, while the minimum instance value is 90.95% below the      
          average.                                                                                                      

  void at::<unnamed>::vectorized_layer_norm_kernel<float, float, 0>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *) (8, 1, 1)x(32, 4, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        8,535
    Memory Throughput                 %         0.70
    DRAM Throughput                   %         0.37
    Duration                         us         7.46
    L1/TEX Cache Throughput           %         6.00
    L2 Cache Throughput               %         0.80
    SM Active Cycles              cycle       478.12
    Compute (SM) Throughput           %         0.46
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              38
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block              24
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 92.59%                                                                                          
          The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           28
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %           75
    Achieved Occupancy                        %         5.80
    Achieved Active Warps Per SM           warp         3.71
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 92.26%                                                                                    
          The difference between calculated theoretical (75.0%) and measured achieved occupancy (5.8%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 25%                                                                                       
          The 12.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the      
          hardware maximum of 16. This kernel's theoretical occupancy (75.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        43.50
    Total DRAM Elapsed Cycles        cycle      471,040
    Average L1 Active Cycles         cycle       478.12
    Total L1 Elapsed Cycles          cycle      911,668
    Average L2 Active Cycles         cycle       880.74
    Total L2 Elapsed Cycles          cycle      647,040
    Average SM Active Cycles         cycle       478.12
    Total SM Elapsed Cycles          cycle      911,668
    Average SMSP Active Cycles       cycle       436.83
    Total SMSP Elapsed Cycles        cycle    3,646,672
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.247%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 92.64% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.247%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 92.64% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.415%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 49.73% above the average, while the minimum instance value is 91.48% below the      
          average.                                                                                                      

  triton_linear_kernel (1, 12, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle       20,154
    Memory Throughput                 %         6.71
    DRAM Throughput                   %         6.71
    Duration                         us        17.57
    L1/TEX Cache Throughput           %        36.36
    L2 Cache Throughput               %         6.43
    SM Active Cycles              cycle     1,970.79
    Compute (SM) Throughput           %         1.68
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.29
    Achieved Active Warps Per SM           warp         4.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.22%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.3%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,871.50
    Total DRAM Elapsed Cycles        cycle    1,116,160
    Average L1 Active Cycles         cycle     1,970.79
    Total L1 Elapsed Cycles          cycle    2,187,860
    Average L2 Active Cycles         cycle    14,725.46
    Total L2 Elapsed Cycles          cycle    1,527,760
    Average SM Active Cycles         cycle     1,970.79
    Total SM Elapsed Cycles          cycle    2,187,860
    Average SMSP Active Cycles       cycle     1,982.09
    Total SMSP Elapsed Cycles        cycle    8,751,440
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.667%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.09% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.717%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.09% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.667%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.09% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  void at::vectorized_elementwise_kernel<4, at::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3) (6, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.57
    SM Frequency                    Ghz         1.13
    Elapsed Cycles                cycle        3,966
    Memory Throughput                 %         0.72
    DRAM Throughput                   %         0.42
    Duration                         us         3.49
    L1/TEX Cache Throughput           %        12.25
    L2 Cache Throughput               %         0.89
    SM Active Cycles              cycle        97.94
    Compute (SM) Throughput           %         0.05
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      6
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             768
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 94.44%                                                                                          
          The grid for this launch is configured to execute only 6 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         5.61
    Achieved Active Warps Per SM           warp         3.59
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.39%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (5.6%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        22.90
    Total DRAM Elapsed Cycles        cycle      219,520
    Average L1 Active Cycles         cycle        97.94
    Total L1 Elapsed Cycles          cycle      401,640
    Average L2 Active Cycles         cycle       493.40
    Total L2 Elapsed Cycles          cycle      300,400
    Average SM Active Cycles         cycle        97.94
    Total SM Elapsed Cycles          cycle      401,640
    Average SMSP Active Cycles       cycle        86.39
    Total SMSP Elapsed Cycles        cycle    1,606,560
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.978%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 60.72% above the average, while the minimum instance value is 84.19% below the      
          average.                                                                                                      

  triton_linear_kernel (1, 12, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle       20,213
    Memory Throughput                 %         6.70
    DRAM Throughput                   %         6.70
    Duration                         us        17.57
    L1/TEX Cache Throughput           %        36.42
    L2 Cache Throughput               %         6.41
    SM Active Cycles              cycle     1,974.17
    Compute (SM) Throughput           %         1.68
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.21
    Achieved Active Warps Per SM           warp         3.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.44%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,871.50
    Total DRAM Elapsed Cycles        cycle    1,116,672
    Average L1 Active Cycles         cycle     1,974.17
    Total L1 Elapsed Cycles          cycle    2,195,476
    Average L2 Active Cycles         cycle    14,714.50
    Total L2 Elapsed Cycles          cycle    1,532,160
    Average SM Active Cycles         cycle     1,974.17
    Total SM Elapsed Cycles          cycle    2,195,476
    Average SMSP Active Cycles       cycle     1,974.68
    Total SMSP Elapsed Cycles        cycle    8,781,904
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.652%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.09% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.652%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.07% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.652%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.09% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  triton_linear_kernel (1, 12, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle       20,338
    Memory Throughput                 %         6.65
    DRAM Throughput                   %         6.65
    Duration                         us        17.66
    L1/TEX Cache Throughput           %        36.32
    L2 Cache Throughput               %         6.35
    SM Active Cycles              cycle     1,976.94
    Compute (SM) Throughput           %         1.70
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.21
    Achieved Active Warps Per SM           warp         3.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.44%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,871.50
    Total DRAM Elapsed Cycles        cycle    1,125,888
    Average L1 Active Cycles         cycle     1,976.94
    Total L1 Elapsed Cycles          cycle    2,172,376
    Average L2 Active Cycles         cycle    14,908.86
    Total L2 Elapsed Cycles          cycle    1,542,320
    Average SM Active Cycles         cycle     1,976.94
    Total SM Elapsed Cycles          cycle    2,172,376
    Average SMSP Active Cycles       cycle     1,994.19
    Total SMSP Elapsed Cycles        cycle    8,689,504
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.753%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.06% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.837%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.14% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.753%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.06% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  void at::<unnamed>::CatArrayBatchedCopy<at::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, at::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, at::<unnamed>::TensorSizeStride<T2, 4>, int, T2) (216, 2, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        7,549
    Memory Throughput                 %         0.92
    DRAM Throughput                   %         0.24
    Duration                         us         6.59
    L1/TEX Cache Throughput           %         3.88
    L2 Cache Throughput               %         0.58
    SM Active Cycles              cycle     1,759.23
    Compute (SM) Throughput           %         5.11
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    432
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread         221,184
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        68.39
    Achieved Active Warps Per SM           warp        43.77
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 31.61%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (68.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24.70
    Total DRAM Elapsed Cycles        cycle      416,768
    Average L1 Active Cycles         cycle     1,759.23
    Total L1 Elapsed Cycles          cycle      796,192
    Average L2 Active Cycles         cycle       918.73
    Total L2 Elapsed Cycles          cycle      571,840
    Average SM Active Cycles         cycle     1,759.23
    Total SM Elapsed Cycles          cycle      796,192
    Average SMSP Active Cycles       cycle     1,713.28
    Total SMSP Elapsed Cycles        cycle    3,184,768
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 16.26%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 68.13% above the average, while the minimum instance value is 36.39% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 15.97%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 68.74% above the average, while the minimum instance value is 44.43% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 16.26%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 68.13% above the average, while the minimum instance value is 36.39% below the      
          average.                                                                                                      

  void at::<unnamed>::CatArrayBatchedCopy<at::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, at::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, at::<unnamed>::TensorSizeStride<T2, 4>, int, T2) (216, 2, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        7,536
    Memory Throughput                 %         0.90
    DRAM Throughput                   %         0.24
    Duration                         us         6.59
    L1/TEX Cache Throughput           %         3.55
    L2 Cache Throughput               %         0.56
    SM Active Cycles              cycle     1,920.55
    Compute (SM) Throughput           %         4.95
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    432
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread         221,184
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        63.56
    Achieved Active Warps Per SM           warp        40.68
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 36.44%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (63.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24.60
    Total DRAM Elapsed Cycles        cycle      416,768
    Average L1 Active Cycles         cycle     1,920.55
    Total L1 Elapsed Cycles          cycle      823,560
    Average L2 Active Cycles         cycle       931.24
    Total L2 Elapsed Cycles          cycle      571,040
    Average SM Active Cycles         cycle     1,920.55
    Total SM Elapsed Cycles          cycle      823,560
    Average SMSP Active Cycles       cycle     1,712.81
    Total SMSP Elapsed Cycles        cycle    3,294,240
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 16.46%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 65.34% above the average, while the minimum instance value is 32.94% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 15.61%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 69.51% above the average, while the minimum instance value is 43.08% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 16.46%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 65.34% above the average, while the minimum instance value is 32.94% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.3%                                                                                            
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 48.29% above the average, while the minimum instance value is 68.64% below the      
          average.                                                                                                      

  fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::Sm80, 1, 64, 64, 64, 1, 1>::Params) (1, 12, 1)x(32, 4, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle       17,837
    Memory Throughput                 %         2.14
    DRAM Throughput                   %         0.45
    Duration                         us        15.49
    L1/TEX Cache Throughput           %        20.64
    L2 Cache Throughput               %         2.39
    SM Active Cycles              cycle     1,722.73
    Compute (SM) Throughput           %         2.30
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread             168
    Shared Memory Configuration Size           Kbyte          135.17
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           36.35
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.04
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %        18.75
    Achieved Occupancy                        %         6.26
    Achieved Active Warps Per SM           warp         4.01
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.6%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.3%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 81.25%                                                                                    
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (18.8%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       110.10
    Total DRAM Elapsed Cycles        cycle      983,040
    Average L1 Active Cycles         cycle     1,722.73
    Total L1 Elapsed Cycles          cycle    1,929,564
    Average L2 Active Cycles         cycle     4,390.02
    Total L2 Elapsed Cycles          cycle    1,351,600
    Average SM Active Cycles         cycle     1,722.73
    Total SM Elapsed Cycles          cycle    1,929,564
    Average SMSP Active Cycles       cycle     1,702.73
    Total SMSP Elapsed Cycles        cycle    7,718,256
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.586%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.04% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.491%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.09% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.586%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.04% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.671%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 29.52% above the average, while the minimum instance value is 39.36% below the      
          average.                                                                                                      

  ampere_sgemm_128x32_tn (6, 1, 13)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.13
    Elapsed Cycles                cycle       13,743
    Memory Throughput                 %        23.45
    DRAM Throughput                   %         9.71
    Duration                         us        12.16
    L1/TEX Cache Throughput           %        41.59
    L2 Cache Throughput               %        13.56
    SM Active Cycles              cycle     7,753.30
    Compute (SM) Throughput           %        21.79
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     78
    Registers Per Thread             register/thread              57
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           16.38
    # SMs                                         SM             108
    Threads                                   thread          19,968
    Uses Green Context                                             0
    Waves Per SM                                                0.18
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 27.78%                                                                                          
          The grid for this launch is configured to execute only 78 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            5
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        12.43
    Achieved Active Warps Per SM           warp         7.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75.15%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (12.4%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,872.20
    Total DRAM Elapsed Cycles        cycle      771,072
    Average L1 Active Cycles         cycle     7,753.30
    Total L1 Elapsed Cycles          cycle    1,485,116
    Average L2 Active Cycles         cycle     8,874.66
    Total L2 Elapsed Cycles          cycle    1,047,520
    Average SM Active Cycles         cycle     7,753.30
    Total SM Elapsed Cycles          cycle    1,485,116
    Average SMSP Active Cycles       cycle     7,728.83
    Total SMSP Elapsed Cycles        cycle    5,940,464
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 18.2%                                                                                           
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 32.27% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.31%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 32.58% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.2%                                                                                           
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 32.27% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.015%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 8.88% above the average, while the minimum instance value is 6.08% below the        
          average.                                                                                                      

  void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T9 *, T8 *, T5 *, const T6 *, const T6 *, const T10 *, const T4 *, T10 *, void *, long, T6 *, int *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *) (24, 1, 1)x(32, 16, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.57
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        6,545
    Memory Throughput                 %         2.88
    DRAM Throughput                   %         2.88
    Duration                         us         5.73
    L1/TEX Cache Throughput           %         5.72
    L2 Cache Throughput               %         3.92
    SM Active Cycles              cycle       919.42
    Compute (SM) Throughput           %         1.75
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     24
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          12,288
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 77.78%                                                                                          
          The grid for this launch is configured to execute only 24 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        17.36
    Achieved Active Warps Per SM           warp        11.11
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 82.64%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (17.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       259.40
    Total DRAM Elapsed Cycles        cycle      360,576
    Average L1 Active Cycles         cycle       919.42
    Total L1 Elapsed Cycles          cycle      698,488
    Average L2 Active Cycles         cycle     2,683.38
    Total L2 Elapsed Cycles          cycle      496,160
    Average SM Active Cycles         cycle       919.42
    Total SM Elapsed Cycles          cycle      698,488
    Average SMSP Active Cycles       cycle       884.03
    Total SMSP Elapsed Cycles        cycle    2,793,952
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.29%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 79.40% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.81%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 79.09% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.29%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 79.40% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 9.129%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L2 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 21.10% above the average, while the minimum instance value is 22.52% below  
          the average.                                                                                                  

  void at::vectorized_elementwise_kernel<4, at::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3) (6, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        4,003
    Memory Throughput                 %         0.88
    DRAM Throughput                   %         0.77
    Duration                         us         3.49
    L1/TEX Cache Throughput           %        12.79
    L2 Cache Throughput               %         1.41
    SM Active Cycles              cycle       100.81
    Compute (SM) Throughput           %         0.05
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      6
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             768
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 94.44%                                                                                          
          The grid for this launch is configured to execute only 6 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         6.12
    Achieved Active Warps Per SM           warp         3.91
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.88%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        42.50
    Total DRAM Elapsed Cycles        cycle      221,312
    Average L1 Active Cycles         cycle       100.81
    Total L1 Elapsed Cycles          cycle      411,786
    Average L2 Active Cycles         cycle       684.25
    Total L2 Elapsed Cycles          cycle      303,200
    Average SM Active Cycles         cycle       100.81
    Total SM Elapsed Cycles          cycle      411,786
    Average SMSP Active Cycles       cycle        97.60
    Total SMSP Elapsed Cycles        cycle    1,647,144
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.468%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 41.37% above the average, while the minimum instance value is 89.04% below the      
          average.                                                                                                      

  void at::<unnamed>::vectorized_layer_norm_kernel<float, float, 0>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *) (8, 1, 1)x(32, 4, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        8,591
    Memory Throughput                 %         0.71
    DRAM Throughput                   %         0.37
    Duration                         us         7.49
    L1/TEX Cache Throughput           %         5.98
    L2 Cache Throughput               %         0.94
    SM Active Cycles              cycle       479.06
    Compute (SM) Throughput           %         0.45
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              38
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block              24
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 92.59%                                                                                          
          The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           28
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %           75
    Achieved Occupancy                        %         5.91
    Achieved Active Warps Per SM           warp         3.78
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 92.12%                                                                                    
          The difference between calculated theoretical (75.0%) and measured achieved occupancy (5.9%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 25%                                                                                       
          The 12.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the      
          hardware maximum of 16. This kernel's theoretical occupancy (75.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        43.50
    Total DRAM Elapsed Cycles        cycle      473,088
    Average L1 Active Cycles         cycle       479.06
    Total L1 Elapsed Cycles          cycle      925,100
    Average L2 Active Cycles         cycle       974.62
    Total L2 Elapsed Cycles          cycle      651,200
    Average SM Active Cycles         cycle       479.06
    Total SM Elapsed Cycles          cycle      925,100
    Average SMSP Active Cycles       cycle       457.15
    Total SMSP Elapsed Cycles        cycle    3,700,400
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.182%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 92.66% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.182%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 92.66% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.083%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 50.80% above the average, while the minimum instance value is 92.30% below the      
          average.                                                                                                      

  triton_linear_kernel (1, 48, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle       21,179
    Memory Throughput                 %        25.10
    DRAM Throughput                   %        25.10
    Duration                         us        18.56
    L1/TEX Cache Throughput           %        35.35
    L2 Cache Throughput               %        23.35
    SM Active Cycles              cycle     8,145.16
    Compute (SM) Throughput           %         6.45
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     48
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           6,144
    Uses Green Context                                             0
    Waves Per SM                                                0.07
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 55.56%                                                                                          
          The grid for this launch is configured to execute only 48 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.25
    Achieved Active Warps Per SM           warp         4.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.32%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.3%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     7,408.30
    Total DRAM Elapsed Cycles        cycle    1,180,672
    Average L1 Active Cycles         cycle     8,145.16
    Total L1 Elapsed Cycles          cycle    2,287,188
    Average L2 Active Cycles         cycle    16,616.86
    Total L2 Elapsed Cycles          cycle    1,608,320
    Average SM Active Cycles         cycle     8,145.16
    Total SM Elapsed Cycles          cycle    2,287,188
    Average SMSP Active Cycles       cycle     8,153.09
    Total SMSP Elapsed Cycles        cycle    9,148,752
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 21.97%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 57.13% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.02%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 57.20% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.97%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 57.13% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  void at::vectorized_elementwise_kernel<4, at::<unnamed>::launch_clamp_scalar(at::TensorIteratorBase &, c10::Scalar, c10::Scalar, detail::ClampLimits)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3) (24, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        4,194
    Memory Throughput                 %         1.60
    DRAM Throughput                   %         1.42
    Duration                         us         3.68
    L1/TEX Cache Throughput           %         8.86
    L2 Cache Throughput               %         3.35
    SM Active Cycles              cycle       406.18
    Compute (SM) Throughput           %         0.26
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     24
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           3,072
    Uses Green Context                                             0
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 77.78%                                                                                          
          The grid for this launch is configured to execute only 24 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         5.89
    Achieved Active Warps Per SM           warp         3.77
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.11%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (5.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        82.50
    Total DRAM Elapsed Cycles        cycle      231,936
    Average L1 Active Cycles         cycle       406.18
    Total L1 Elapsed Cycles          cycle      422,288
    Average L2 Active Cycles         cycle     1,221.41
    Total L2 Elapsed Cycles          cycle      317,760
    Average SM Active Cycles         cycle       406.18
    Total SM Elapsed Cycles          cycle      422,288
    Average SMSP Active Cycles       cycle       381.44
    Total SMSP Elapsed Cycles        cycle    1,689,152
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.388%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 80.75% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.918%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 81.16% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.388%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 80.75% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.17%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 33.07% above the average, while the minimum instance value is 29.59% below the      
          average.                                                                                                      

  ampere_sgemm_128x32_sliced1x4_tn (6, 1, 9)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle       36,756
    Memory Throughput                 %        24.20
    DRAM Throughput                   %        14.60
    Duration                         us        32.16
    L1/TEX Cache Throughput           %        52.29
    L2 Cache Throughput               %        14.38
    SM Active Cycles              cycle    16,935.58
    Compute (SM) Throughput           %        31.49
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.5 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     54
    Registers Per Thread             register/thread             134
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.01
    # SMs                                         SM             108
    Threads                                   thread          13,824
    Uses Green Context                                             0
    Waves Per SM                                                0.50
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          The grid for this launch is configured to execute only 54 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        12.50
    Achieved Occupancy                        %        12.48
    Achieved Active Warps Per SM           warp         7.98
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 87.5%                                                                                     
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     7,467.40
    Total DRAM Elapsed Cycles        cycle    2,046,464
    Average L1 Active Cycles         cycle    16,935.58
    Total L1 Elapsed Cycles          cycle    3,953,192
    Average L2 Active Cycles         cycle    22,749.25
    Total L2 Elapsed Cycles          cycle    2,794,240
    Average SM Active Cycles         cycle    16,935.58
    Total SM Elapsed Cycles          cycle    3,953,192
    Average SMSP Active Cycles       cycle    16,927.50
    Total SMSP Elapsed Cycles        cycle   15,812,768
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.61%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 51.02% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.54%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 50.90% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.61%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 51.02% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T9 *, T8 *, T5 *, const T6 *, const T6 *, const T10 *, const T4 *, T10 *, void *, long, T6 *, int *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *) (24, 1, 1)x(32, 16, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        5,687
    Memory Throughput                 %         2.31
    DRAM Throughput                   %         2.31
    Duration                         us         4.99
    L1/TEX Cache Throughput           %         6.12
    L2 Cache Throughput               %         3.52
    SM Active Cycles              cycle       745.53
    Compute (SM) Throughput           %         1.88
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     24
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          12,288
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 77.78%                                                                                          
          The grid for this launch is configured to execute only 24 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        18.27
    Achieved Active Warps Per SM           warp        11.69
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 81.73%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (18.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       182.60
    Total DRAM Elapsed Cycles        cycle      316,672
    Average L1 Active Cycles         cycle       745.53
    Total L1 Elapsed Cycles          cycle      600,182
    Average L2 Active Cycles         cycle     1,836.49
    Total L2 Elapsed Cycles          cycle      430,800
    Average SM Active Cycles         cycle       745.53
    Total SM Elapsed Cycles          cycle      600,182
    Average SMSP Active Cycles       cycle       741.47
    Total SMSP Elapsed Cycles        cycle    2,400,728
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 10.62%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 79.13% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.6%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 79.42% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.62%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 79.13% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 13.16%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 38.60% above the average, while the minimum instance value is 40.87% below the      
          average.                                                                                                      

  void at::vectorized_elementwise_kernel<4, at::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3) (6, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.57
    SM Frequency                    Ghz         1.13
    Elapsed Cycles                cycle        3,892
    Memory Throughput                 %         0.91
    DRAM Throughput                   %         0.79
    Duration                         us         3.42
    L1/TEX Cache Throughput           %        13.68
    L2 Cache Throughput               %         1.56
    SM Active Cycles              cycle       101.54
    Compute (SM) Throughput           %         0.05
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      6
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             768
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 94.44%                                                                                          
          The grid for this launch is configured to execute only 6 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         6.30
    Achieved Active Warps Per SM           warp         4.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.7%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.3%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        42.50
    Total DRAM Elapsed Cycles        cycle      215,040
    Average L1 Active Cycles         cycle       101.54
    Total L1 Elapsed Cycles          cycle      431,370
    Average L2 Active Cycles         cycle       716.26
    Total L2 Elapsed Cycles          cycle      294,960
    Average SM Active Cycles         cycle       101.54
    Total SM Elapsed Cycles          cycle      431,370
    Average SMSP Active Cycles       cycle       100.47
    Total SMSP Elapsed Cycles        cycle    1,725,480
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.753%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 39.91% above the average, while the minimum instance value is 86.74% below the      
          average.                                                                                                      

  void at::<unnamed>::vectorized_layer_norm_kernel<float, float, 0>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *) (8, 1, 1)x(32, 4, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle        8,603
    Memory Throughput                 %         0.69
    DRAM Throughput                   %         0.37
    Duration                         us         7.49
    L1/TEX Cache Throughput           %         5.95
    L2 Cache Throughput               %         0.76
    SM Active Cycles              cycle       481.63
    Compute (SM) Throughput           %         0.45
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              38
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block              24
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 92.59%                                                                                          
          The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           28
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %           75
    Achieved Occupancy                        %         6.01
    Achieved Active Warps Per SM           warp         3.85
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.98%                                                                                    
          The difference between calculated theoretical (75.0%) and measured achieved occupancy (6.0%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 25%                                                                                       
          The 12.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the      
          hardware maximum of 16. This kernel's theoretical occupancy (75.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        43.50
    Total DRAM Elapsed Cycles        cycle      474,752
    Average L1 Active Cycles         cycle       481.63
    Total L1 Elapsed Cycles          cycle      937,002
    Average L2 Active Cycles         cycle       899.92
    Total L2 Elapsed Cycles          cycle      652,320
    Average SM Active Cycles         cycle       481.63
    Total SM Elapsed Cycles          cycle      937,002
    Average SMSP Active Cycles       cycle       445.64
    Total SMSP Elapsed Cycles        cycle    3,748,008
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.143%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 92.65% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.143%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 92.65% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.041%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 54.73% above the average, while the minimum instance value is 91.67% below the      
          average.                                                                                                      

  triton_linear_kernel (1, 12, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle       20,375
    Memory Throughput                 %         6.64
    DRAM Throughput                   %         6.64
    Duration                         us        17.73
    L1/TEX Cache Throughput           %        36.14
    L2 Cache Throughput               %         6.35
    SM Active Cycles              cycle     1,982.90
    Compute (SM) Throughput           %         1.67
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.26
    Achieved Active Warps Per SM           warp         4.01
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.31%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.3%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,871.50
    Total DRAM Elapsed Cycles        cycle    1,126,912
    Average L1 Active Cycles         cycle     1,982.90
    Total L1 Elapsed Cycles          cycle    2,206,730
    Average L2 Active Cycles         cycle    14,791.69
    Total L2 Elapsed Cycles          cycle    1,545,040
    Average SM Active Cycles         cycle     1,982.90
    Total SM Elapsed Cycles          cycle    2,206,730
    Average SMSP Active Cycles       cycle     1,982.84
    Total SMSP Elapsed Cycles        cycle    8,826,920
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.65%                                                                                           
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.14% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.647%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.11% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.65%                                                                                           
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.14% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  void at::vectorized_elementwise_kernel<4, at::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3) (6, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.56
    SM Frequency                    Ghz         1.13
    Elapsed Cycles                cycle        3,889
    Memory Throughput                 %         0.74
    DRAM Throughput                   %         0.43
    Duration                         us         3.42
    L1/TEX Cache Throughput           %        12.03
    L2 Cache Throughput               %         0.98
    SM Active Cycles              cycle        99.71
    Compute (SM) Throughput           %         0.05
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      6
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             768
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 94.44%                                                                                          
          The grid for this launch is configured to execute only 6 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         5.78
    Achieved Active Warps Per SM           warp         3.70
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.22%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (5.8%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        22.90
    Total DRAM Elapsed Cycles        cycle      214,016
    Average L1 Active Cycles         cycle        99.71
    Total L1 Elapsed Cycles          cycle      407,666
    Average L2 Active Cycles         cycle       625.31
    Total L2 Elapsed Cycles          cycle      294,640
    Average SM Active Cycles         cycle        99.71
    Total SM Elapsed Cycles          cycle      407,666
    Average SMSP Active Cycles       cycle        92.18
    Total SMSP Elapsed Cycles        cycle    1,630,664
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.211%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 42.47% above the average, while the minimum instance value is 87.53% below the      
          average.                                                                                                      

  triton_linear_kernel (1, 12, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle       20,380
    Memory Throughput                 %         6.64
    DRAM Throughput                   %         6.64
    Duration                         us        17.76
    L1/TEX Cache Throughput           %        36.49
    L2 Cache Throughput               %         6.35
    SM Active Cycles              cycle     1,968.19
    Compute (SM) Throughput           %         1.68
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.31
    Achieved Active Warps Per SM           warp         4.04
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.17%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.3%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,871.50
    Total DRAM Elapsed Cycles        cycle    1,126,912
    Average L1 Active Cycles         cycle     1,968.19
    Total L1 Elapsed Cycles          cycle    2,189,842
    Average L2 Active Cycles         cycle    14,613.02
    Total L2 Elapsed Cycles          cycle    1,544,960
    Average SM Active Cycles         cycle     1,968.19
    Total SM Elapsed Cycles          cycle    2,189,842
    Average SMSP Active Cycles       cycle     1,997.16
    Total SMSP Elapsed Cycles        cycle    8,759,368
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.66%                                                                                           
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.21% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.787%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.22% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.66%                                                                                           
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.21% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  triton_linear_kernel (1, 12, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle       20,323
    Memory Throughput                 %         6.67
    DRAM Throughput                   %         6.67
    Duration                         us        17.66
    L1/TEX Cache Throughput           %        36.17
    L2 Cache Throughput               %         6.35
    SM Active Cycles              cycle     1,976.13
    Compute (SM) Throughput           %         1.68
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.34
    Achieved Active Warps Per SM           warp         4.06
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.1%                                                                                     
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.3%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,871.40
    Total DRAM Elapsed Cycles        cycle    1,121,792
    Average L1 Active Cycles         cycle     1,976.13
    Total L1 Elapsed Cycles          cycle    2,200,208
    Average L2 Active Cycles         cycle    14,820.96
    Total L2 Elapsed Cycles          cycle    1,541,840
    Average SM Active Cycles         cycle     1,976.13
    Total SM Elapsed Cycles          cycle    2,200,208
    Average SMSP Active Cycles       cycle     1,978.65
    Total SMSP Elapsed Cycles        cycle    8,800,832
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.644%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.11% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.666%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.22% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.644%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.11% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.884%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 7.65% above the average, while the minimum instance value is 4.51% below the        
          average.                                                                                                      

  void at::<unnamed>::CatArrayBatchedCopy<at::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, at::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, at::<unnamed>::TensorSizeStride<T2, 4>, int, T2) (216, 2, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        7,473
    Memory Throughput                 %         0.91
    DRAM Throughput                   %         0.24
    Duration                         us         6.53
    L1/TEX Cache Throughput           %         3.83
    L2 Cache Throughput               %         0.58
    SM Active Cycles              cycle     1,774.84
    Compute (SM) Throughput           %         5.06
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    432
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread         221,184
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.48
    Achieved Active Warps Per SM           warp        45.75
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.52%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24.70
    Total DRAM Elapsed Cycles        cycle      413,184
    Average L1 Active Cycles         cycle     1,774.84
    Total L1 Elapsed Cycles          cycle      806,064
    Average L2 Active Cycles         cycle       766.21
    Total L2 Elapsed Cycles          cycle      566,000
    Average SM Active Cycles         cycle     1,774.84
    Total SM Elapsed Cycles          cycle      806,064
    Average SMSP Active Cycles       cycle     1,926.19
    Total SMSP Elapsed Cycles        cycle    3,224,256
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 16.11%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 67.73% above the average, while the minimum instance value is 36.67% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 16.84%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 65.26% above the average, while the minimum instance value is 38.17% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 16.11%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 67.73% above the average, while the minimum instance value is 36.67% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.293%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 58.11% above the average, while the minimum instance value is 89.82% below the      
          average.                                                                                                      

  void at::<unnamed>::CatArrayBatchedCopy<at::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, at::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, at::<unnamed>::TensorSizeStride<T2, 4>, int, T2) (216, 2, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.57
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        7,677
    Memory Throughput                 %         0.90
    DRAM Throughput                   %         0.23
    Duration                         us         6.72
    L1/TEX Cache Throughput           %         3.83
    L2 Cache Throughput               %         0.50
    SM Active Cycles              cycle     1,775.96
    Compute (SM) Throughput           %         4.97
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    432
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread         221,184
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        68.00
    Achieved Active Warps Per SM           warp        43.52
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 32%                                                                                       
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (68.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24.70
    Total DRAM Elapsed Cycles        cycle      423,168
    Average L1 Active Cycles         cycle     1,775.96
    Total L1 Elapsed Cycles          cycle      819,310
    Average L2 Active Cycles         cycle       906.48
    Total L2 Elapsed Cycles          cycle      581,760
    Average SM Active Cycles         cycle     1,775.96
    Total SM Elapsed Cycles          cycle      819,310
    Average SMSP Active Cycles       cycle     1,714.23
    Total SMSP Elapsed Cycles        cycle    3,277,240
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 16.04%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 68.51% above the average, while the minimum instance value is 36.82% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 15.36%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 67.96% above the average, while the minimum instance value is 37.64% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 16.04%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 68.51% above the average, while the minimum instance value is 36.82% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.227%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 57.98% above the average, while the minimum instance value is 90.07% below the      
          average.                                                                                                      

  fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::Sm80, 1, 64, 64, 64, 1, 1>::Params) (1, 12, 1)x(32, 4, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle       17,898
    Memory Throughput                 %         2.13
    DRAM Throughput                   %         0.45
    Duration                         us        15.55
    L1/TEX Cache Throughput           %        20.44
    L2 Cache Throughput               %         2.39
    SM Active Cycles              cycle     1,739.26
    Compute (SM) Throughput           %         2.30
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread             168
    Shared Memory Configuration Size           Kbyte          135.17
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           36.35
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.04
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %        18.75
    Achieved Occupancy                        %         6.24
    Achieved Active Warps Per SM           warp         3.99
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.71%                                                                                    
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 81.25%                                                                                    
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (18.8%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       110.10
    Total DRAM Elapsed Cycles        cycle      988,160
    Average L1 Active Cycles         cycle     1,739.26
    Total L1 Elapsed Cycles          cycle    1,927,212
    Average L2 Active Cycles         cycle     4,320.51
    Total L2 Elapsed Cycles          cycle    1,356,960
    Average SM Active Cycles         cycle     1,739.26
    Total SM Elapsed Cycles          cycle    1,927,212
    Average SMSP Active Cycles       cycle     1,722.98
    Total SMSP Elapsed Cycles        cycle    7,708,848
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.677%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.02% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.609%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.16% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.677%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.02% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.439%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 29.21% above the average, while the minimum instance value is 40.24% below the      
          average.                                                                                                      

  ampere_sgemm_128x32_tn (6, 1, 13)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle       13,696
    Memory Throughput                 %        23.55
    DRAM Throughput                   %         9.82
    Duration                         us           12
    L1/TEX Cache Throughput           %        41.60
    L2 Cache Throughput               %        13.60
    SM Active Cycles              cycle     7,752.02
    Compute (SM) Throughput           %        21.88
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     78
    Registers Per Thread             register/thread              57
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           16.38
    # SMs                                         SM             108
    Threads                                   thread          19,968
    Uses Green Context                                             0
    Waves Per SM                                                0.18
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 27.78%                                                                                          
          The grid for this launch is configured to execute only 78 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            5
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        12.46
    Achieved Active Warps Per SM           warp         7.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75.08%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (12.5%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,872.20
    Total DRAM Elapsed Cycles        cycle      762,368
    Average L1 Active Cycles         cycle     7,752.02
    Total L1 Elapsed Cycles          cycle    1,478,892
    Average L2 Active Cycles         cycle     8,685.04
    Total L2 Elapsed Cycles          cycle    1,041,200
    Average SM Active Cycles         cycle     7,752.02
    Total SM Elapsed Cycles          cycle    1,478,892
    Average SMSP Active Cycles       cycle     7,722.58
    Total SMSP Elapsed Cycles        cycle    5,915,568
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 18.26%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 32.26% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.95%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 33.61% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.26%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 32.26% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.623%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L2 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 9.92% above the average, while the minimum instance value is 8.85% below    
          the average.                                                                                                  

  void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T9 *, T8 *, T5 *, const T6 *, const T6 *, const T10 *, const T4 *, T10 *, void *, long, T6 *, int *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *) (24, 1, 1)x(32, 16, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        6,396
    Memory Throughput                 %         2.94
    DRAM Throughput                   %         2.94
    Duration                         us         5.60
    L1/TEX Cache Throughput           %         6.00
    L2 Cache Throughput               %         4.02
    SM Active Cycles              cycle       873.69
    Compute (SM) Throughput           %         1.79
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     24
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          12,288
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 77.78%                                                                                          
          The grid for this launch is configured to execute only 24 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        17.68
    Achieved Active Warps Per SM           warp        11.31
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 82.32%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (17.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       259.40
    Total DRAM Elapsed Cycles        cycle      353,280
    Average L1 Active Cycles         cycle       873.69
    Total L1 Elapsed Cycles          cycle      679,912
    Average L2 Active Cycles         cycle     2,598.62
    Total L2 Elapsed Cycles          cycle      484,400
    Average SM Active Cycles         cycle       873.69
    Total SM Elapsed Cycles          cycle      679,912
    Average SMSP Active Cycles       cycle       867.08
    Total SMSP Elapsed Cycles        cycle    2,719,648
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.02%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 79.42% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.91%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 79.19% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.02%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 79.42% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.915%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L2 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 20.77% above the average, while the minimum instance value is 20.23% below  
          the average.                                                                                                  

  void at::vectorized_elementwise_kernel<4, at::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3) (6, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.57
    SM Frequency                    Ghz         1.13
    Elapsed Cycles                cycle        3,966
    Memory Throughput                 %         0.89
    DRAM Throughput                   %         0.78
    Duration                         us         3.49
    L1/TEX Cache Throughput           %        12.50
    L2 Cache Throughput               %         1.44
    SM Active Cycles              cycle       103.08
    Compute (SM) Throughput           %         0.05
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      6
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             768
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 94.44%                                                                                          
          The grid for this launch is configured to execute only 6 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         6.33
    Achieved Active Warps Per SM           warp         4.05
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.67%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.3%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        42.50
    Total DRAM Elapsed Cycles        cycle      219,136
    Average L1 Active Cycles         cycle       103.08
    Total L1 Elapsed Cycles          cycle      430,070
    Average L2 Active Cycles         cycle       785.10
    Total L2 Elapsed Cycles          cycle      300,560
    Average SM Active Cycles         cycle       103.08
    Total SM Elapsed Cycles          cycle      430,070
    Average SMSP Active Cycles       cycle        95.30
    Total SMSP Elapsed Cycles        cycle    1,720,280
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 9.945%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 47.59% above the average, while the minimum instance value is 90.45% below the      
          average.                                                                                                      

  void at::<unnamed>::vectorized_layer_norm_kernel<float, float, 0>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *) (8, 1, 1)x(32, 4, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        8,681
    Memory Throughput                 %         0.71
    DRAM Throughput                   %         0.36
    Duration                         us         7.58
    L1/TEX Cache Throughput           %         5.87
    L2 Cache Throughput               %         0.94
    SM Active Cycles              cycle       488.25
    Compute (SM) Throughput           %         0.45
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              38
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block              24
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 92.59%                                                                                          
          The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           28
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %           75
    Achieved Occupancy                        %         5.98
    Achieved Active Warps Per SM           warp         3.83
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 92.03%                                                                                    
          The difference between calculated theoretical (75.0%) and measured achieved occupancy (6.0%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 25%                                                                                       
          The 12.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the      
          hardware maximum of 16. This kernel's theoretical occupancy (75.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        43.50
    Total DRAM Elapsed Cycles        cycle      478,720
    Average L1 Active Cycles         cycle       488.25
    Total L1 Elapsed Cycles          cycle      935,258
    Average L2 Active Cycles         cycle     1,001.61
    Total L2 Elapsed Cycles          cycle      658,240
    Average SM Active Cycles         cycle       488.25
    Total SM Elapsed Cycles          cycle      935,258
    Average SMSP Active Cycles       cycle       463.37
    Total SMSP Elapsed Cycles        cycle    3,741,032
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.225%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 92.67% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.225%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 92.67% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.167%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 50.66% above the average, while the minimum instance value is 72.94% below the      
          average.                                                                                                      

  triton_linear_kernel (1, 48, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle       21,637
    Memory Throughput                 %        24.62
    DRAM Throughput                   %        24.62
    Duration                         us        18.98
    L1/TEX Cache Throughput           %        34.96
    L2 Cache Throughput               %        22.87
    SM Active Cycles              cycle     8,215.64
    Compute (SM) Throughput           %         6.47
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     48
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           6,144
    Uses Green Context                                             0
    Waves Per SM                                                0.07
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 55.56%                                                                                          
          The grid for this launch is configured to execute only 48 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.23
    Achieved Active Warps Per SM           warp         3.99
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.39%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     7,408.30
    Total DRAM Elapsed Cycles        cycle    1,203,712
    Average L1 Active Cycles         cycle     8,215.64
    Total L1 Elapsed Cycles          cycle    2,280,614
    Average L2 Active Cycles         cycle    16,432.60
    Total L2 Elapsed Cycles          cycle    1,644,080
    Average SM Active Cycles         cycle     8,215.64
    Total SM Elapsed Cycles          cycle    2,280,614
    Average SMSP Active Cycles       cycle     8,158.36
    Total SMSP Elapsed Cycles        cycle    9,122,456
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 22.46%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 57.74% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.03%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 57.03% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.46%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 57.74% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.951%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L2 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 7.44% above the average, while the minimum instance value is 5.97% below    
          the average.                                                                                                  

  void at::vectorized_elementwise_kernel<4, at::<unnamed>::launch_clamp_scalar(at::TensorIteratorBase &, c10::Scalar, c10::Scalar, detail::ClampLimits)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3) (24, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        3,981
    Memory Throughput                 %         1.68
    DRAM Throughput                   %         1.50
    Duration                         us         3.49
    L1/TEX Cache Throughput           %         9.38
    L2 Cache Throughput               %         3.30
    SM Active Cycles              cycle       394.46
    Compute (SM) Throughput           %         0.24
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     24
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           3,072
    Uses Green Context                                             0
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 77.78%                                                                                          
          The grid for this launch is configured to execute only 24 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         6.91
    Achieved Active Warps Per SM           warp         4.42
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.09%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        82.50
    Total DRAM Elapsed Cycles        cycle      220,160
    Average L1 Active Cycles         cycle       394.46
    Total L1 Elapsed Cycles          cycle      454,886
    Average L2 Active Cycles         cycle     1,020.06
    Total L2 Elapsed Cycles          cycle      302,000
    Average SM Active Cycles         cycle       394.46
    Total SM Elapsed Cycles          cycle      454,886
    Average SMSP Active Cycles       cycle       385.45
    Total SMSP Elapsed Cycles        cycle    1,819,544
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.459%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 79.65% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.313%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 79.91% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.459%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 79.65% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.22%                                                                                           
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 30.42% above the average, while the minimum instance value is 52.26% below the      
          average.                                                                                                      

  ampere_sgemm_128x32_sliced1x4_tn (6, 1, 9)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle       36,956
    Memory Throughput                 %        24.01
    DRAM Throughput                   %        14.52
    Duration                         us        32.32
    L1/TEX Cache Throughput           %        52.25
    L2 Cache Throughput               %        14.33
    SM Active Cycles              cycle    16,943.22
    Compute (SM) Throughput           %        31.26
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.5 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     54
    Registers Per Thread             register/thread             134
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.01
    # SMs                                         SM             108
    Threads                                   thread          13,824
    Uses Green Context                                             0
    Waves Per SM                                                0.50
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          The grid for this launch is configured to execute only 54 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        12.50
    Achieved Occupancy                        %        12.49
    Achieved Active Warps Per SM           warp         7.99
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 87.5%                                                                                     
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     7,467.40
    Total DRAM Elapsed Cycles        cycle    2,057,728
    Average L1 Active Cycles         cycle    16,943.22
    Total L1 Elapsed Cycles          cycle    3,982,432
    Average L2 Active Cycles         cycle    23,159.90
    Total L2 Elapsed Cycles          cycle    2,808,640
    Average SM Active Cycles         cycle    16,943.22
    Total SM Elapsed Cycles          cycle    3,982,432
    Average SMSP Active Cycles       cycle    16,920.45
    Total SMSP Elapsed Cycles        cycle   15,929,728
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.49%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 51.13% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.3%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 50.78% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.49%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 51.13% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T9 *, T8 *, T5 *, const T6 *, const T6 *, const T10 *, const T4 *, T10 *, void *, long, T6 *, int *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *) (24, 1, 1)x(32, 16, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        5,586
    Memory Throughput                 %         2.36
    DRAM Throughput                   %         2.36
    Duration                         us         4.90
    L1/TEX Cache Throughput           %         6.19
    L2 Cache Throughput               %         3.59
    SM Active Cycles              cycle          735
    Compute (SM) Throughput           %         1.80
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     24
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          12,288
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 77.78%                                                                                          
          The grid for this launch is configured to execute only 24 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        18.96
    Achieved Active Warps Per SM           warp        12.13
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 81.04%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (19.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       182.60
    Total DRAM Elapsed Cycles        cycle      309,248
    Average L1 Active Cycles         cycle          735
    Total L1 Elapsed Cycles          cycle      627,312
    Average L2 Active Cycles         cycle     1,799.92
    Total L2 Elapsed Cycles          cycle      423,280
    Average SM Active Cycles         cycle          735
    Total SM Elapsed Cycles          cycle      627,312
    Average SMSP Active Cycles       cycle       739.03
    Total SMSP Elapsed Cycles        cycle    2,509,248
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 10.02%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 79.18% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.24%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 80.48% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.02%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 79.18% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.973%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 20.50% above the average, while the minimum instance value is 24.94% below the      
          average.                                                                                                      

  void at::vectorized_elementwise_kernel<4, at::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3) (6, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        3,982
    Memory Throughput                 %         0.89
    DRAM Throughput                   %         0.77
    Duration                         us         3.49
    L1/TEX Cache Throughput           %        12.30
    L2 Cache Throughput               %         1.48
    SM Active Cycles              cycle       104.77
    Compute (SM) Throughput           %         0.05
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      6
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             768
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 94.44%                                                                                          
          The grid for this launch is configured to execute only 6 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         6.16
    Achieved Active Warps Per SM           warp         3.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.84%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        42.50
    Total DRAM Elapsed Cycles        cycle      220,160
    Average L1 Active Cycles         cycle       104.77
    Total L1 Elapsed Cycles          cycle      437,806
    Average L2 Active Cycles         cycle       940.39
    Total L2 Elapsed Cycles          cycle      301,920
    Average SM Active Cycles         cycle       104.77
    Total SM Elapsed Cycles          cycle      437,806
    Average SMSP Active Cycles       cycle        99.67
    Total SMSP Elapsed Cycles        cycle    1,751,224
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.839%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 31.46% above the average, while the minimum instance value is 37.37% below the      
          average.                                                                                                      

  void at::<unnamed>::vectorized_layer_norm_kernel<float, float, 0>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *) (8, 1, 1)x(32, 4, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle        8,667
    Memory Throughput                 %         0.68
    DRAM Throughput                   %         0.36
    Duration                         us         7.55
    L1/TEX Cache Throughput           %         5.90
    L2 Cache Throughput               %         0.77
    SM Active Cycles              cycle       485.98
    Compute (SM) Throughput           %         0.46
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              38
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block              24
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 92.59%                                                                                          
          The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           28
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %           75
    Achieved Occupancy                        %         5.73
    Achieved Active Warps Per SM           warp         3.66
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 92.37%                                                                                    
          The difference between calculated theoretical (75.0%) and measured achieved occupancy (5.7%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 25%                                                                                       
          The 12.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the      
          hardware maximum of 16. This kernel's theoretical occupancy (75.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        43.50
    Total DRAM Elapsed Cycles        cycle      477,824
    Average L1 Active Cycles         cycle       485.98
    Total L1 Elapsed Cycles          cycle      902,330
    Average L2 Active Cycles         cycle       820.20
    Total L2 Elapsed Cycles          cycle      657,120
    Average SM Active Cycles         cycle       485.98
    Total SM Elapsed Cycles          cycle      902,330
    Average SMSP Active Cycles       cycle       439.35
    Total SMSP Elapsed Cycles        cycle    3,609,320
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.39%                                                                                           
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 92.67% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.39%                                                                                           
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 92.67% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.164%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 61.73% above the average, while the minimum instance value is 90.86% below the      
          average.                                                                                                      

  triton_linear_kernel (1, 12, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle       20,387
    Memory Throughput                 %         6.65
    DRAM Throughput                   %         6.65
    Duration                         us        17.70
    L1/TEX Cache Throughput           %        36.10
    L2 Cache Throughput               %         6.36
    SM Active Cycles              cycle     1,989.46
    Compute (SM) Throughput           %         1.68
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.25
    Achieved Active Warps Per SM           warp         4.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.34%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,871.50
    Total DRAM Elapsed Cycles        cycle    1,125,888
    Average L1 Active Cycles         cycle     1,989.46
    Total L1 Elapsed Cycles          cycle    2,197,956
    Average L2 Active Cycles         cycle    14,840.50
    Total L2 Elapsed Cycles          cycle    1,546,080
    Average SM Active Cycles         cycle     1,989.46
    Total SM Elapsed Cycles          cycle    2,197,956
    Average SMSP Active Cycles       cycle     1,982.48
    Total SMSP Elapsed Cycles        cycle    8,791,824
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.712%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.12% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.677%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.08% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.712%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.12% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.171%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L2 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 6.73% above the average, while the minimum instance value is 5.70% below    
          the average.                                                                                                  

  void at::vectorized_elementwise_kernel<4, at::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3) (6, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.57
    SM Frequency                    Ghz         1.13
    Elapsed Cycles                cycle        3,994
    Memory Throughput                 %         0.72
    DRAM Throughput                   %         0.42
    Duration                         us         3.52
    L1/TEX Cache Throughput           %        12.19
    L2 Cache Throughput               %         1.02
    SM Active Cycles              cycle        98.44
    Compute (SM) Throughput           %         0.05
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      6
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             768
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 94.44%                                                                                          
          The grid for this launch is configured to execute only 6 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         6.05
    Achieved Active Warps Per SM           warp         3.87
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.95%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.0%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        22.90
    Total DRAM Elapsed Cycles        cycle      220,672
    Average L1 Active Cycles         cycle        98.44
    Total L1 Elapsed Cycles          cycle      414,668
    Average L2 Active Cycles         cycle       649.15
    Total L2 Elapsed Cycles          cycle      302,640
    Average SM Active Cycles         cycle        98.44
    Total SM Elapsed Cycles          cycle      414,668
    Average SMSP Active Cycles       cycle        97.35
    Total SMSP Elapsed Cycles        cycle    1,658,672
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.169%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 41.78% above the average, while the minimum instance value is 61.33% below the      
          average.                                                                                                      

  triton_linear_kernel (1, 12, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle       20,322
    Memory Throughput                 %         6.68
    DRAM Throughput                   %         6.68
    Duration                         us        17.63
    L1/TEX Cache Throughput           %        36.13
    L2 Cache Throughput               %         6.36
    SM Active Cycles              cycle     1,985.12
    Compute (SM) Throughput           %         1.68
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.23
    Achieved Active Warps Per SM           warp         3.99
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.39%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,871.50
    Total DRAM Elapsed Cycles        cycle    1,121,280
    Average L1 Active Cycles         cycle     1,985.12
    Total L1 Elapsed Cycles          cycle    2,188,006
    Average L2 Active Cycles         cycle    14,741.65
    Total L2 Elapsed Cycles          cycle    1,541,120
    Average SM Active Cycles         cycle     1,985.12
    Total SM Elapsed Cycles          cycle    2,188,006
    Average SMSP Active Cycles       cycle     1,962.98
    Total SMSP Elapsed Cycles        cycle    8,752,024
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.732%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.12% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.65%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.28% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.732%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.12% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  triton_linear_kernel (1, 12, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle       20,531
    Memory Throughput                 %         6.61
    DRAM Throughput                   %         6.61
    Duration                         us        17.86
    L1/TEX Cache Throughput           %        35.91
    L2 Cache Throughput               %         6.30
    SM Active Cycles              cycle     1,993.90
    Compute (SM) Throughput           %         1.67
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.19
    Achieved Active Warps Per SM           warp         3.96
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.49%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,871.50
    Total DRAM Elapsed Cycles        cycle    1,132,032
    Average L1 Active Cycles         cycle     1,993.90
    Total L1 Elapsed Cycles          cycle    2,207,076
    Average L2 Active Cycles         cycle    14,863.84
    Total L2 Elapsed Cycles          cycle    1,555,840
    Average SM Active Cycles         cycle     1,993.90
    Total SM Elapsed Cycles          cycle    2,207,076
    Average SMSP Active Cycles       cycle     1,962.77
    Total SMSP Elapsed Cycles        cycle    8,828,304
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.694%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.10% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.559%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.12% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.694%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.10% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.405%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 7.07% above the average, while the minimum instance value is 4.86% below the        
          average.                                                                                                      

  void at::<unnamed>::CatArrayBatchedCopy<at::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, at::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, at::<unnamed>::TensorSizeStride<T2, 4>, int, T2) (216, 2, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        7,640
    Memory Throughput                 %         0.93
    DRAM Throughput                   %         0.23
    Duration                         us         6.69
    L1/TEX Cache Throughput           %         3.86
    L2 Cache Throughput               %         0.60
    SM Active Cycles              cycle     1,766.01
    Compute (SM) Throughput           %         5.13
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    432
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread         221,184
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.63
    Achieved Active Warps Per SM           warp        45.84
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.37%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24.70
    Total DRAM Elapsed Cycles        cycle      424,192
    Average L1 Active Cycles         cycle     1,766.01
    Total L1 Elapsed Cycles          cycle      793,150
    Average L2 Active Cycles         cycle       897.29
    Total L2 Elapsed Cycles          cycle      578,720
    Average SM Active Cycles         cycle     1,766.01
    Total SM Elapsed Cycles          cycle      793,150
    Average SMSP Active Cycles       cycle     1,711.91
    Total SMSP Elapsed Cycles        cycle    3,172,600
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 16.47%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 68.50% above the average, while the minimum instance value is 36.64% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 16.05%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 68.84% above the average, while the minimum instance value is 43.16% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 16.47%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 68.50% above the average, while the minimum instance value is 36.64% below the      
          average.                                                                                                      

  void at::<unnamed>::CatArrayBatchedCopy<at::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, at::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, at::<unnamed>::TensorSizeStride<T2, 4>, int, T2) (216, 2, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        7,575
    Memory Throughput                 %         0.92
    DRAM Throughput                   %         0.24
    Duration                         us         6.62
    L1/TEX Cache Throughput           %         3.83
    L2 Cache Throughput               %         0.58
    SM Active Cycles              cycle     1,770.24
    Compute (SM) Throughput           %         5.10
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    432
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread         221,184
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        68.17
    Achieved Active Warps Per SM           warp        43.63
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 31.83%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (68.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24.60
    Total DRAM Elapsed Cycles        cycle      418,304
    Average L1 Active Cycles         cycle     1,770.24
    Total L1 Elapsed Cycles          cycle      798,724
    Average L2 Active Cycles         cycle       810.17
    Total L2 Elapsed Cycles          cycle      574,160
    Average SM Active Cycles         cycle     1,770.24
    Total SM Elapsed Cycles          cycle      798,724
    Average SMSP Active Cycles       cycle     1,727.34
    Total SMSP Elapsed Cycles        cycle    3,194,896
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 16.35%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 68.30% above the average, while the minimum instance value is 36.45% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 16.14%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 69.09% above the average, while the minimum instance value is 42.92% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 16.35%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 68.30% above the average, while the minimum instance value is 36.45% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.718%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 59.51% above the average, while the minimum instance value is 90.74% below the      
          average.                                                                                                      

  fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::Sm80, 1, 64, 64, 64, 1, 1>::Params) (1, 12, 1)x(32, 4, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle       17,675
    Memory Throughput                 %         2.16
    DRAM Throughput                   %         0.45
    Duration                         us        15.36
    L1/TEX Cache Throughput           %        20.81
    L2 Cache Throughput               %         2.42
    SM Active Cycles              cycle     1,709.19
    Compute (SM) Throughput           %         2.30
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread             168
    Shared Memory Configuration Size           Kbyte          135.17
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           36.35
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.04
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %        18.75
    Achieved Occupancy                        %         6.26
    Achieved Active Warps Per SM           warp         4.01
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.6%                                                                                     
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.3%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 81.25%                                                                                    
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (18.8%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       110.10
    Total DRAM Elapsed Cycles        cycle      975,104
    Average L1 Active Cycles         cycle     1,709.19
    Total L1 Elapsed Cycles          cycle    1,933,430
    Average L2 Active Cycles         cycle     4,386.70
    Total L2 Elapsed Cycles          cycle    1,339,120
    Average SM Active Cycles         cycle     1,709.19
    Total SM Elapsed Cycles          cycle    1,933,430
    Average SMSP Active Cycles       cycle     1,747.94
    Total SMSP Elapsed Cycles        cycle    7,733,720
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.505%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.08% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.694%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.04% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.505%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.08% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 9.426%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 35.97% above the average, while the minimum instance value is 38.70% below the      
          average.                                                                                                      

  ampere_sgemm_128x32_tn (6, 1, 13)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.13
    Elapsed Cycles                cycle       13,682
    Memory Throughput                 %        23.41
    DRAM Throughput                   %         9.77
    Duration                         us        12.10
    L1/TEX Cache Throughput           %        41.75
    L2 Cache Throughput               %        13.62
    SM Active Cycles              cycle     7,724.21
    Compute (SM) Throughput           %        21.76
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     78
    Registers Per Thread             register/thread              57
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           16.38
    # SMs                                         SM             108
    Threads                                   thread          19,968
    Uses Green Context                                             0
    Waves Per SM                                                0.18
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 27.78%                                                                                          
          The grid for this launch is configured to execute only 78 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            5
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        12.50
    Achieved Active Warps Per SM           warp         8.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75.01%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (12.5%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,872.20
    Total DRAM Elapsed Cycles        cycle      766,592
    Average L1 Active Cycles         cycle     7,724.21
    Total L1 Elapsed Cycles          cycle    1,487,608
    Average L2 Active Cycles         cycle     8,744.19
    Total L2 Elapsed Cycles          cycle    1,042,080
    Average SM Active Cycles         cycle     7,724.21
    Total SM Elapsed Cycles          cycle    1,487,608
    Average SMSP Active Cycles       cycle     7,686.05
    Total SMSP Elapsed Cycles        cycle    5,950,432
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 18.03%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 32.15% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.73%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 33.56% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.03%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 32.15% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.23%                                                                                           
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L2 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 7.79% above the average, while the minimum instance value is 6.04% below    
          the average.                                                                                                  

  void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T9 *, T8 *, T5 *, const T6 *, const T6 *, const T10 *, const T4 *, T10 *, void *, long, T6 *, int *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *) (24, 1, 1)x(32, 16, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        6,549
    Memory Throughput                 %         2.86
    DRAM Throughput                   %         2.86
    Duration                         us         5.73
    L1/TEX Cache Throughput           %         5.66
    L2 Cache Throughput               %         3.97
    SM Active Cycles              cycle       929.08
    Compute (SM) Throughput           %         1.80
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     24
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          12,288
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 77.78%                                                                                          
          The grid for this launch is configured to execute only 24 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.91
    Achieved Active Warps Per SM           warp        10.82
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.09%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (16.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       259.40
    Total DRAM Elapsed Cycles        cycle      363,008
    Average L1 Active Cycles         cycle       929.08
    Total L1 Elapsed Cycles          cycle      677,206
    Average L2 Active Cycles         cycle     2,618.07
    Total L2 Elapsed Cycles          cycle      495,680
    Average SM Active Cycles         cycle       929.08
    Total SM Elapsed Cycles          cycle      677,206
    Average SMSP Active Cycles       cycle       883.73
    Total SMSP Elapsed Cycles        cycle    2,708,824
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.72%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 79.11% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.16%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 79.19% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.72%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 79.11% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.38%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 29.30% above the average, while the minimum instance value is 25.67% below the      
          average.                                                                                                      

  void at::vectorized_elementwise_kernel<4, at::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3) (6, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.57
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        4,046
    Memory Throughput                 %         0.87
    DRAM Throughput                   %         0.76
    Duration                         us         3.55
    L1/TEX Cache Throughput           %        12.37
    L2 Cache Throughput               %         1.40
    SM Active Cycles              cycle       104.16
    Compute (SM) Throughput           %         0.05
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      6
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             768
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 94.44%                                                                                          
          The grid for this launch is configured to execute only 6 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         6.63
    Achieved Active Warps Per SM           warp         4.25
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.37%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.6%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        42.50
    Total DRAM Elapsed Cycles        cycle      223,616
    Average L1 Active Cycles         cycle       104.16
    Total L1 Elapsed Cycles          cycle      435,008
    Average L2 Active Cycles         cycle       678.99
    Total L2 Elapsed Cycles          cycle      306,640
    Average SM Active Cycles         cycle       104.16
    Total SM Elapsed Cycles          cycle      435,008
    Average SMSP Active Cycles       cycle       100.12
    Total SMSP Elapsed Cycles        cycle    1,740,032
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.741%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 43.70% above the average, while the minimum instance value is 88.95% below the      
          average.                                                                                                      

  void at::<unnamed>::vectorized_layer_norm_kernel<float, float, 0>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *) (8, 1, 1)x(32, 4, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle        8,571
    Memory Throughput                 %         0.71
    DRAM Throughput                   %         0.37
    Duration                         us         7.46
    L1/TEX Cache Throughput           %         5.97
    L2 Cache Throughput               %         0.95
    SM Active Cycles              cycle       480.56
    Compute (SM) Throughput           %         0.44
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              38
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block              24
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 92.59%                                                                                          
          The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           28
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %           75
    Achieved Occupancy                        %         6.14
    Achieved Active Warps Per SM           warp         3.93
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.82%                                                                                    
          The difference between calculated theoretical (75.0%) and measured achieved occupancy (6.1%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 25%                                                                                       
          The 12.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the      
          hardware maximum of 16. This kernel's theoretical occupancy (75.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        43.50
    Total DRAM Elapsed Cycles        cycle      472,064
    Average L1 Active Cycles         cycle       480.56
    Total L1 Elapsed Cycles          cycle      943,084
    Average L2 Active Cycles         cycle       974.67
    Total L2 Elapsed Cycles          cycle      650,000
    Average SM Active Cycles         cycle       480.56
    Total SM Elapsed Cycles          cycle      943,084
    Average SMSP Active Cycles       cycle       461.79
    Total SMSP Elapsed Cycles        cycle    3,772,336
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.1%                                                                                            
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 92.67% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.1%                                                                                            
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 92.67% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.552%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 62.95% above the average, while the minimum instance value is 92.31% below the      
          average.                                                                                                      

  triton_linear_kernel (1, 48, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle       21,161
    Memory Throughput                 %        25.14
    DRAM Throughput                   %        25.14
    Duration                         us        18.56
    L1/TEX Cache Throughput           %        35.37
    L2 Cache Throughput               %        23.37
    SM Active Cycles              cycle     8,124.74
    Compute (SM) Throughput           %         6.42
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     48
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           6,144
    Uses Green Context                                             0
    Waves Per SM                                                0.07
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 55.56%                                                                                          
          The grid for this launch is configured to execute only 48 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.26
    Achieved Active Warps Per SM           warp         4.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.31%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.3%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     7,408.30
    Total DRAM Elapsed Cycles        cycle    1,178,752
    Average L1 Active Cycles         cycle     8,124.74
    Total L1 Elapsed Cycles          cycle    2,297,980
    Average L2 Active Cycles         cycle    16,646.90
    Total L2 Elapsed Cycles          cycle    1,606,720
    Average SM Active Cycles         cycle     8,124.74
    Total SM Elapsed Cycles          cycle    2,297,980
    Average SMSP Active Cycles       cycle     8,239.12
    Total SMSP Elapsed Cycles        cycle    9,191,920
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 21.85%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 57.22% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.19%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 57.29% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.85%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 57.22% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  void at::vectorized_elementwise_kernel<4, at::<unnamed>::launch_clamp_scalar(at::TensorIteratorBase &, c10::Scalar, c10::Scalar, detail::ClampLimits)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3) (24, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.56
    SM Frequency                    Ghz         1.13
    Elapsed Cycles                cycle        3,984
    Memory Throughput                 %         1.68
    DRAM Throughput                   %         1.50
    Duration                         us         3.52
    L1/TEX Cache Throughput           %         9.31
    L2 Cache Throughput               %         3.38
    SM Active Cycles              cycle       386.79
    Compute (SM) Throughput           %         0.24
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     24
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           3,072
    Uses Green Context                                             0
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 77.78%                                                                                          
          The grid for this launch is configured to execute only 24 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         6.35
    Achieved Active Warps Per SM           warp         4.06
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.65%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.4%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        82.50
    Total DRAM Elapsed Cycles        cycle      219,520
    Average L1 Active Cycles         cycle       386.79
    Total L1 Elapsed Cycles          cycle      442,734
    Average L2 Active Cycles         cycle     1,097.91
    Total L2 Elapsed Cycles          cycle      302,000
    Average SM Active Cycles         cycle       386.79
    Total SM Elapsed Cycles          cycle      442,734
    Average SMSP Active Cycles       cycle       375.02
    Total SMSP Elapsed Cycles        cycle    1,770,936
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.499%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 79.48% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.307%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 79.87% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.499%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 79.48% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.62%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L2 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 36.50% above the average, while the minimum instance value is 35.97% below  
          the average.                                                                                                  

  ampere_sgemm_128x32_sliced1x4_tn (6, 1, 9)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle       37,356
    Memory Throughput                 %        23.99
    DRAM Throughput                   %        14.44
    Duration                         us        32.48
    L1/TEX Cache Throughput           %        51.37
    L2 Cache Throughput               %        14.21
    SM Active Cycles              cycle    17,230.81
    Compute (SM) Throughput           %        31.24
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.5 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     54
    Registers Per Thread             register/thread             134
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.01
    # SMs                                         SM             108
    Threads                                   thread          13,824
    Uses Green Context                                             0
    Waves Per SM                                                0.50
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          The grid for this launch is configured to execute only 54 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        12.50
    Achieved Occupancy                        %        12.29
    Achieved Active Warps Per SM           warp         7.86
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 87.5%                                                                                     
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     7,467.40
    Total DRAM Elapsed Cycles        cycle    2,067,968
    Average L1 Active Cycles         cycle    17,230.81
    Total L1 Elapsed Cycles          cycle    3,984,378
    Average L2 Active Cycles         cycle    23,481.65
    Total L2 Elapsed Cycles          cycle    2,834,000
    Average SM Active Cycles         cycle    17,230.81
    Total SM Elapsed Cycles          cycle    3,984,378
    Average SMSP Active Cycles       cycle    16,963.24
    Total SMSP Elapsed Cycles        cycle   15,937,512
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.78%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 50.92% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.44%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 50.98% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.78%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 50.92% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T9 *, T8 *, T5 *, const T6 *, const T6 *, const T10 *, const T4 *, T10 *, void *, long, T6 *, int *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *) (24, 1, 1)x(32, 16, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        5,438
    Memory Throughput                 %         2.42
    DRAM Throughput                   %         2.42
    Duration                         us         4.77
    L1/TEX Cache Throughput           %         6.56
    L2 Cache Throughput               %         3.73
    SM Active Cycles              cycle       696.10
    Compute (SM) Throughput           %         1.88
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     24
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          12,288
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 77.78%                                                                                          
          The grid for this launch is configured to execute only 24 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        19.55
    Achieved Active Warps Per SM           warp        12.51
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 80.45%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (19.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       182.60
    Total DRAM Elapsed Cycles        cycle      301,568
    Average L1 Active Cycles         cycle       696.10
    Total L1 Elapsed Cycles          cycle      598,756
    Average L2 Active Cycles         cycle     1,857.44
    Total L2 Elapsed Cycles          cycle      412,080
    Average SM Active Cycles         cycle       696.10
    Total SM Elapsed Cycles          cycle      598,756
    Average SMSP Active Cycles       cycle       736.53
    Total SMSP Elapsed Cycles        cycle    2,395,024
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 9.975%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 79.44% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.63%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 80.04% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 9.975%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 79.44% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 14.74%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L2 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 40.86% above the average, while the minimum instance value is 41.37% below  
          the average.                                                                                                  

  void at::vectorized_elementwise_kernel<4, at::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3) (6, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        3,998
    Memory Throughput                 %         0.89
    DRAM Throughput                   %         0.77
    Duration                         us         3.49
    L1/TEX Cache Throughput           %        12.67
    L2 Cache Throughput               %         1.52
    SM Active Cycles              cycle       101.70
    Compute (SM) Throughput           %         0.05
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      6
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             768
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 94.44%                                                                                          
          The grid for this launch is configured to execute only 6 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         6.31
    Achieved Active Warps Per SM           warp         4.04
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.69%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.3%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        42.50
    Total DRAM Elapsed Cycles        cycle      220,160
    Average L1 Active Cycles         cycle       101.70
    Total L1 Elapsed Cycles          cycle      431,746
    Average L2 Active Cycles         cycle       765.71
    Total L2 Elapsed Cycles          cycle      302,960
    Average SM Active Cycles         cycle       101.70
    Total SM Elapsed Cycles          cycle      431,746
    Average SMSP Active Cycles       cycle        97.27
    Total SMSP Elapsed Cycles        cycle    1,726,984
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.663%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 37.90% above the average, while the minimum instance value is 87.59% below the      
          average.                                                                                                      

  void at::<unnamed>::vectorized_layer_norm_kernel<float, float, 0>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *) (8, 1, 1)x(32, 4, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle        8,717
    Memory Throughput                 %         0.68
    DRAM Throughput                   %         0.36
    Duration                         us         7.58
    L1/TEX Cache Throughput           %         5.82
    L2 Cache Throughput               %         0.77
    SM Active Cycles              cycle       495.37
    Compute (SM) Throughput           %         0.46
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              38
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block              24
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 92.59%                                                                                          
          The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           28
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %           75
    Achieved Occupancy                        %         5.60
    Achieved Active Warps Per SM           warp         3.58
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 92.54%                                                                                    
          The difference between calculated theoretical (75.0%) and measured achieved occupancy (5.6%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 25%                                                                                       
          The 12.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the      
          hardware maximum of 16. This kernel's theoretical occupancy (75.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        43.50
    Total DRAM Elapsed Cycles        cycle      481,280
    Average L1 Active Cycles         cycle       495.37
    Total L1 Elapsed Cycles          cycle      907,020
    Average L2 Active Cycles         cycle       846.34
    Total L2 Elapsed Cycles          cycle      661,120
    Average SM Active Cycles         cycle       495.37
    Total SM Elapsed Cycles          cycle      907,020
    Average SMSP Active Cycles       cycle       436.00
    Total SMSP Elapsed Cycles        cycle    3,628,080
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.465%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 92.65% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.465%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 92.65% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.079%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 49.59% above the average, while the minimum instance value is 91.14% below the      
          average.                                                                                                      

  triton_linear_kernel (1, 12, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle       20,212
    Memory Throughput                 %         6.71
    DRAM Throughput                   %         6.71
    Duration                         us        17.57
    L1/TEX Cache Throughput           %        36.38
    L2 Cache Throughput               %         6.40
    SM Active Cycles              cycle     1,966.63
    Compute (SM) Throughput           %         1.70
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.31
    Achieved Active Warps Per SM           warp         4.04
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.18%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.3%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,871.50
    Total DRAM Elapsed Cycles        cycle    1,115,904
    Average L1 Active Cycles         cycle     1,966.63
    Total L1 Elapsed Cycles          cycle    2,172,438
    Average L2 Active Cycles         cycle       14,836
    Total L2 Elapsed Cycles          cycle    1,531,440
    Average SM Active Cycles         cycle     1,966.63
    Total SM Elapsed Cycles          cycle    2,172,438
    Average SMSP Active Cycles       cycle     1,973.24
    Total SMSP Elapsed Cycles        cycle    8,689,752
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.716%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.15% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.742%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.12% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.716%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.15% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  void at::vectorized_elementwise_kernel<4, at::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3) (6, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.57
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        3,958
    Memory Throughput                 %         0.73
    DRAM Throughput                   %         0.42
    Duration                         us         3.46
    L1/TEX Cache Throughput           %        12.02
    L2 Cache Throughput               %         0.96
    SM Active Cycles              cycle        99.80
    Compute (SM) Throughput           %         0.05
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      6
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             768
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 94.44%                                                                                          
          The grid for this launch is configured to execute only 6 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         5.92
    Achieved Active Warps Per SM           warp         3.79
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.08%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (5.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        22.90
    Total DRAM Elapsed Cycles        cycle      216,576
    Average L1 Active Cycles         cycle        99.80
    Total L1 Elapsed Cycles          cycle      413,206
    Average L2 Active Cycles         cycle       621.86
    Total L2 Elapsed Cycles          cycle      300,000
    Average SM Active Cycles         cycle        99.80
    Total SM Elapsed Cycles          cycle      413,206
    Average SMSP Active Cycles       cycle        94.21
    Total SMSP Elapsed Cycles        cycle    1,652,824
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.165%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 43.21% above the average, while the minimum instance value is 87.46% below the      
          average.                                                                                                      

  triton_linear_kernel (1, 12, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle       20,222
    Memory Throughput                 %         6.71
    DRAM Throughput                   %         6.71
    Duration                         us        17.57
    L1/TEX Cache Throughput           %        36.17
    L2 Cache Throughput               %         6.40
    SM Active Cycles              cycle     1,982.70
    Compute (SM) Throughput           %         1.68
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.30
    Achieved Active Warps Per SM           warp         4.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.2%                                                                                     
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.3%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,871.50
    Total DRAM Elapsed Cycles        cycle    1,116,160
    Average L1 Active Cycles         cycle     1,982.70
    Total L1 Elapsed Cycles          cycle    2,197,502
    Average L2 Active Cycles         cycle    14,750.85
    Total L2 Elapsed Cycles          cycle    1,530,400
    Average SM Active Cycles         cycle     1,982.70
    Total SM Elapsed Cycles          cycle    2,197,502
    Average SMSP Active Cycles       cycle     1,981.68
    Total SMSP Elapsed Cycles        cycle    8,790,008
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.679%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.07% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.677%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.10% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.679%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.07% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  triton_linear_kernel (1, 12, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle       20,419
    Memory Throughput                 %         6.65
    DRAM Throughput                   %         6.65
    Duration                         us        17.73
    L1/TEX Cache Throughput           %        35.90
    L2 Cache Throughput               %         6.32
    SM Active Cycles              cycle     1,987.69
    Compute (SM) Throughput           %         1.68
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.22
    Achieved Active Warps Per SM           warp         3.98
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.42%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,871.50
    Total DRAM Elapsed Cycles        cycle    1,126,400
    Average L1 Active Cycles         cycle     1,987.69
    Total L1 Elapsed Cycles          cycle    2,192,324
    Average L2 Active Cycles         cycle    14,961.98
    Total L2 Elapsed Cycles          cycle    1,548,480
    Average SM Active Cycles         cycle     1,987.69
    Total SM Elapsed Cycles          cycle    2,192,324
    Average SMSP Active Cycles       cycle     1,976.20
    Total SMSP Elapsed Cycles        cycle    8,769,296
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.727%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.12% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.672%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.07% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.727%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.12% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.806%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 7.51% above the average, while the minimum instance value is 5.05% below the        
          average.                                                                                                      

  void at::<unnamed>::CatArrayBatchedCopy<at::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, at::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, at::<unnamed>::TensorSizeStride<T2, 4>, int, T2) (216, 2, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        7,624
    Memory Throughput                 %         0.91
    DRAM Throughput                   %         0.23
    Duration                         us         6.66
    L1/TEX Cache Throughput           %         3.84
    L2 Cache Throughput               %         0.57
    SM Active Cycles              cycle     1,767.82
    Compute (SM) Throughput           %         5.05
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    432
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread         221,184
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        68.58
    Achieved Active Warps Per SM           warp        43.89
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 31.42%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (68.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24.70
    Total DRAM Elapsed Cycles        cycle      421,248
    Average L1 Active Cycles         cycle     1,767.82
    Total L1 Elapsed Cycles          cycle      806,086
    Average L2 Active Cycles         cycle       915.27
    Total L2 Elapsed Cycles          cycle      577,840
    Average SM Active Cycles         cycle     1,767.82
    Total SM Elapsed Cycles          cycle      806,086
    Average SMSP Active Cycles       cycle     1,838.48
    Total SMSP Elapsed Cycles        cycle    3,224,344
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 16.25%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 68.61% above the average, while the minimum instance value is 36.76% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 16.34%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 66.33% above the average, while the minimum instance value is 39.95% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 16.25%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 68.61% above the average, while the minimum instance value is 36.76% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.106%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 40.30% above the average, while the minimum instance value is 69.74% below the      
          average.                                                                                                      

  void at::<unnamed>::CatArrayBatchedCopy<at::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, at::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, at::<unnamed>::TensorSizeStride<T2, 4>, int, T2) (216, 2, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle        7,607
    Memory Throughput                 %         0.91
    DRAM Throughput                   %         0.24
    Duration                         us         6.62
    L1/TEX Cache Throughput           %         3.85
    L2 Cache Throughput               %         0.60
    SM Active Cycles              cycle     1,763.57
    Compute (SM) Throughput           %         5.07
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    432
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread         221,184
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        68.68
    Achieved Active Warps Per SM           warp        43.96
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 31.32%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (68.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24.70
    Total DRAM Elapsed Cycles        cycle      419,712
    Average L1 Active Cycles         cycle     1,763.57
    Total L1 Elapsed Cycles          cycle      802,644
    Average L2 Active Cycles         cycle       846.59
    Total L2 Elapsed Cycles          cycle      576,960
    Average SM Active Cycles         cycle     1,763.57
    Total SM Elapsed Cycles          cycle      802,644
    Average SMSP Active Cycles       cycle     1,715.48
    Total SMSP Elapsed Cycles        cycle    3,210,576
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 16.3%                                                                                           
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 68.69% above the average, while the minimum instance value is 36.21% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 15.82%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 68.52% above the average, while the minimum instance value is 42.87% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 16.3%                                                                                           
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 68.69% above the average, while the minimum instance value is 36.21% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.981%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 59.47% above the average, while the minimum instance value is 91.14% below the      
          average.                                                                                                      

  fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::Sm80, 1, 64, 64, 64, 1, 1>::Params) (1, 12, 1)x(32, 4, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle       17,890
    Memory Throughput                 %         2.13
    DRAM Throughput                   %         0.45
    Duration                         us        15.55
    L1/TEX Cache Throughput           %        20.73
    L2 Cache Throughput               %         2.39
    SM Active Cycles              cycle     1,715.05
    Compute (SM) Throughput           %         2.30
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread             168
    Shared Memory Configuration Size           Kbyte          135.17
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           36.35
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.04
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %        18.75
    Achieved Occupancy                        %         6.30
    Achieved Active Warps Per SM           warp         4.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.41%                                                                                    
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.3%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 81.25%                                                                                    
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (18.8%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       110.10
    Total DRAM Elapsed Cycles        cycle      988,160
    Average L1 Active Cycles         cycle     1,715.05
    Total L1 Elapsed Cycles          cycle    1,933,140
    Average L2 Active Cycles         cycle     4,245.39
    Total L2 Elapsed Cycles          cycle    1,354,960
    Average SM Active Cycles         cycle     1,715.05
    Total SM Elapsed Cycles          cycle    1,933,140
    Average SMSP Active Cycles       cycle     1,702.57
    Total SMSP Elapsed Cycles        cycle    7,732,560
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.542%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.15% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.474%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.09% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.542%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.15% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 9.655%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 38.52% above the average, while the minimum instance value is 60.99% below the      
          average.                                                                                                      

  ampere_sgemm_128x32_tn (6, 1, 13)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.13
    Elapsed Cycles                cycle       13,832
    Memory Throughput                 %        23.51
    DRAM Throughput                   %         9.68
    Duration                         us        12.22
    L1/TEX Cache Throughput           %        41.67
    L2 Cache Throughput               %        13.49
    SM Active Cycles              cycle     7,737.98
    Compute (SM) Throughput           %        21.85
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     78
    Registers Per Thread             register/thread              57
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           16.38
    # SMs                                         SM             108
    Threads                                   thread          19,968
    Uses Green Context                                             0
    Waves Per SM                                                0.18
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 27.78%                                                                                          
          The grid for this launch is configured to execute only 78 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            5
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        12.44
    Achieved Active Warps Per SM           warp         7.96
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75.12%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (12.4%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,872.20
    Total DRAM Elapsed Cycles        cycle      773,888
    Average L1 Active Cycles         cycle     7,737.98
    Total L1 Elapsed Cycles          cycle    1,480,930
    Average L2 Active Cycles         cycle     8,765.74
    Total L2 Elapsed Cycles          cycle    1,052,800
    Average SM Active Cycles         cycle     7,737.98
    Total SM Elapsed Cycles          cycle    1,480,930
    Average SMSP Active Cycles       cycle     7,699.71
    Total SMSP Elapsed Cycles        cycle    5,923,720
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 18.81%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 33.34% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.23%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 32.46% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.81%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 33.34% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.953%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L2 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 8.94% above the average, while the minimum instance value is 8.34% below    
          the average.                                                                                                  

  void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T9 *, T8 *, T5 *, const T6 *, const T6 *, const T10 *, const T4 *, T10 *, void *, long, T6 *, int *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *) (24, 1, 1)x(32, 16, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        6,648
    Memory Throughput                 %         2.82
    DRAM Throughput                   %         2.82
    Duration                         us         5.82
    L1/TEX Cache Throughput           %         5.85
    L2 Cache Throughput               %         3.85
    SM Active Cycles              cycle       898.74
    Compute (SM) Throughput           %         1.75
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     24
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          12,288
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 77.78%                                                                                          
          The grid for this launch is configured to execute only 24 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        17.40
    Achieved Active Warps Per SM           warp        11.13
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 82.6%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (17.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       259.40
    Total DRAM Elapsed Cycles        cycle      368,128
    Average L1 Active Cycles         cycle       898.74
    Total L1 Elapsed Cycles          cycle      696,454
    Average L2 Active Cycles         cycle     2,568.86
    Total L2 Elapsed Cycles          cycle      503,600
    Average SM Active Cycles         cycle       898.74
    Total SM Elapsed Cycles          cycle      696,454
    Average SMSP Active Cycles       cycle       910.04
    Total SMSP Elapsed Cycles        cycle    2,785,816
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.17%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 80.16% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.17%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 79.13% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.17%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 80.16% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.981%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L2 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 17.11% above the average, while the minimum instance value is 18.76% below  
          the average.                                                                                                  

  void at::vectorized_elementwise_kernel<4, at::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3) (6, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.56
    SM Frequency                    Ghz         1.13
    Elapsed Cycles                cycle        3,888
    Memory Throughput                 %         0.91
    DRAM Throughput                   %         0.79
    Duration                         us         3.42
    L1/TEX Cache Throughput           %        13.21
    L2 Cache Throughput               %         1.46
    SM Active Cycles              cycle        97.58
    Compute (SM) Throughput           %         0.05
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      6
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             768
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 94.44%                                                                                          
          The grid for this launch is configured to execute only 6 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         6.22
    Achieved Active Warps Per SM           warp         3.98
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.78%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        42.50
    Total DRAM Elapsed Cycles        cycle      214,144
    Average L1 Active Cycles         cycle        97.58
    Total L1 Elapsed Cycles          cycle      416,678
    Average L2 Active Cycles         cycle       856.98
    Total L2 Elapsed Cycles          cycle      294,560
    Average SM Active Cycles         cycle        97.58
    Total SM Elapsed Cycles          cycle      416,678
    Average SMSP Active Cycles       cycle       101.07
    Total SMSP Elapsed Cycles        cycle    1,666,712
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.44%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 49.17% above the average, while the minimum instance value is 91.25% below the      
          average.                                                                                                      

  void at::<unnamed>::vectorized_layer_norm_kernel<float, float, 0>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *) (8, 1, 1)x(32, 4, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        8,659
    Memory Throughput                 %         0.71
    DRAM Throughput                   %         0.37
    Duration                         us         7.55
    L1/TEX Cache Throughput           %         5.89
    L2 Cache Throughput               %         0.96
    SM Active Cycles              cycle       486.63
    Compute (SM) Throughput           %         0.44
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              38
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block              24
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 92.59%                                                                                          
          The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           28
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %           75
    Achieved Occupancy                        %         5.94
    Achieved Active Warps Per SM           warp         3.80
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 92.08%                                                                                    
          The difference between calculated theoretical (75.0%) and measured achieved occupancy (5.9%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 25%                                                                                       
          The 12.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the      
          hardware maximum of 16. This kernel's theoretical occupancy (75.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        43.50
    Total DRAM Elapsed Cycles        cycle      476,672
    Average L1 Active Cycles         cycle       486.63
    Total L1 Elapsed Cycles          cycle      946,264
    Average L2 Active Cycles         cycle     1,041.80
    Total L2 Elapsed Cycles          cycle      656,480
    Average SM Active Cycles         cycle       486.63
    Total SM Elapsed Cycles          cycle      946,264
    Average SMSP Active Cycles       cycle       465.98
    Total SMSP Elapsed Cycles        cycle    3,785,056
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.145%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 92.64% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.145%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 92.64% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.151%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 48.45% above the average, while the minimum instance value is 73.99% below the      
          average.                                                                                                      

  triton_linear_kernel (1, 48, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle       21,387
    Memory Throughput                 %        25.13
    DRAM Throughput                   %        25.13
    Duration                         us        18.56
    L1/TEX Cache Throughput           %        35.17
    L2 Cache Throughput               %        23.15
    SM Active Cycles              cycle     8,194.12
    Compute (SM) Throughput           %         6.42
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     48
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           6,144
    Uses Green Context                                             0
    Waves Per SM                                                0.07
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 55.56%                                                                                          
          The grid for this launch is configured to execute only 48 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.25
    Achieved Active Warps Per SM           warp         4.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.34%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     7,408.30
    Total DRAM Elapsed Cycles        cycle    1,179,136
    Average L1 Active Cycles         cycle     8,194.12
    Total L1 Elapsed Cycles          cycle    2,296,248
    Average L2 Active Cycles         cycle    16,583.91
    Total L2 Elapsed Cycles          cycle    1,621,920
    Average SM Active Cycles         cycle     8,194.12
    Total SM Elapsed Cycles          cycle    2,296,248
    Average SMSP Active Cycles       cycle     8,198.01
    Total SMSP Elapsed Cycles        cycle    9,184,992
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 22.05%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 57.21% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.1%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 57.32% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.05%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 57.21% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  void at::vectorized_elementwise_kernel<4, at::<unnamed>::launch_clamp_scalar(at::TensorIteratorBase &, c10::Scalar, c10::Scalar, detail::ClampLimits)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3) (24, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.57
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        3,952
    Memory Throughput                 %         1.70
    DRAM Throughput                   %         1.52
    Duration                         us         3.46
    L1/TEX Cache Throughput           %         9.34
    L2 Cache Throughput               %         3.02
    SM Active Cycles              cycle       385.60
    Compute (SM) Throughput           %         0.25
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     24
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           3,072
    Uses Green Context                                             0
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 77.78%                                                                                          
          The grid for this launch is configured to execute only 24 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         6.34
    Achieved Active Warps Per SM           warp         4.05
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.66%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.3%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        82.50
    Total DRAM Elapsed Cycles        cycle      216,960
    Average L1 Active Cycles         cycle       385.60
    Total L1 Elapsed Cycles          cycle      435,562
    Average L2 Active Cycles         cycle     1,131.14
    Total L2 Elapsed Cycles          cycle      299,280
    Average SM Active Cycles         cycle       385.60
    Total SM Elapsed Cycles          cycle      435,562
    Average SMSP Active Cycles       cycle       374.87
    Total SMSP Elapsed Cycles        cycle    1,742,248
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.622%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 79.72% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.444%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 80.08% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.622%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 79.72% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.88%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 42.61% above the average, while the minimum instance value is 57.83% below the      
          average.                                                                                                      

  ampere_sgemm_128x32_sliced1x4_tn (6, 1, 9)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle       36,523
    Memory Throughput                 %        24.18
    DRAM Throughput                   %        14.71
    Duration                         us        31.94
    L1/TEX Cache Throughput           %        52.53
    L2 Cache Throughput               %        14.52
    SM Active Cycles              cycle    16,853.31
    Compute (SM) Throughput           %        31.48
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.5 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     54
    Registers Per Thread             register/thread             134
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.01
    # SMs                                         SM             108
    Threads                                   thread          13,824
    Uses Green Context                                             0
    Waves Per SM                                                0.50
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          The grid for this launch is configured to execute only 54 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        12.50
    Achieved Occupancy                        %        12.55
    Achieved Active Warps Per SM           warp         8.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 87.5%                                                                                     
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     7,467.40
    Total DRAM Elapsed Cycles        cycle    2,030,592
    Average L1 Active Cycles         cycle    16,853.31
    Total L1 Elapsed Cycles          cycle    3,954,484
    Average L2 Active Cycles         cycle    23,565.04
    Total L2 Elapsed Cycles          cycle    2,774,080
    Average SM Active Cycles         cycle    16,853.31
    Total SM Elapsed Cycles          cycle    3,954,484
    Average SMSP Active Cycles       cycle    16,877.14
    Total SMSP Elapsed Cycles        cycle   15,817,936
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.47%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 50.98% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.65%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 51.31% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.47%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 50.98% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T9 *, T8 *, T5 *, const T6 *, const T6 *, const T10 *, const T4 *, T10 *, void *, long, T6 *, int *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *) (24, 1, 1)x(32, 16, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        5,547
    Memory Throughput                 %         2.38
    DRAM Throughput                   %         2.38
    Duration                         us         4.83
    L1/TEX Cache Throughput           %         6.42
    L2 Cache Throughput               %         3.64
    SM Active Cycles              cycle       706.77
    Compute (SM) Throughput           %         1.76
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     24
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          12,288
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 77.78%                                                                                          
          The grid for this launch is configured to execute only 24 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        20.40
    Achieved Active Warps Per SM           warp        13.05
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 79.6%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (20.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       182.60
    Total DRAM Elapsed Cycles        cycle      307,200
    Average L1 Active Cycles         cycle       706.77
    Total L1 Elapsed Cycles          cycle      641,368
    Average L2 Active Cycles         cycle     1,847.24
    Total L2 Elapsed Cycles          cycle      419,920
    Average SM Active Cycles         cycle       706.77
    Total SM Elapsed Cycles          cycle      641,368
    Average SMSP Active Cycles       cycle       710.09
    Total SMSP Elapsed Cycles        cycle    2,565,472
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 9.435%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 79.27% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 9.577%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 80.09% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 9.435%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 79.27% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.327%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 20.82% above the average, while the minimum instance value is 30.17% below the      
          average.                                                                                                      

  void at::vectorized_elementwise_kernel<4, at::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3) (6, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        3,989
    Memory Throughput                 %         0.89
    DRAM Throughput                   %         0.77
    Duration                         us         3.49
    L1/TEX Cache Throughput           %        12.28
    L2 Cache Throughput               %         1.48
    SM Active Cycles              cycle       104.99
    Compute (SM) Throughput           %         0.05
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      6
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             768
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 94.44%                                                                                          
          The grid for this launch is configured to execute only 6 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         5.83
    Achieved Active Warps Per SM           warp         3.73
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.17%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (5.8%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        42.50
    Total DRAM Elapsed Cycles        cycle      220,160
    Average L1 Active Cycles         cycle       104.99
    Total L1 Elapsed Cycles          cycle      418,962
    Average L2 Active Cycles         cycle       930.71
    Total L2 Elapsed Cycles          cycle      302,320
    Average SM Active Cycles         cycle       104.99
    Total SM Elapsed Cycles          cycle      418,962
    Average SMSP Active Cycles       cycle        99.72
    Total SMSP Elapsed Cycles        cycle    1,675,848
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.306%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 25.60% above the average, while the minimum instance value is 42.95% below the      
          average.                                                                                                      

  void at::<unnamed>::vectorized_layer_norm_kernel<float, float, 0>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *) (8, 1, 1)x(32, 4, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle        8,707
    Memory Throughput                 %         0.68
    DRAM Throughput                   %         0.36
    Duration                         us         7.58
    L1/TEX Cache Throughput           %         5.90
    L2 Cache Throughput               %         0.78
    SM Active Cycles              cycle       485.97
    Compute (SM) Throughput           %         0.45
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              38
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block              24
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 92.59%                                                                                          
          The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           28
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %           75
    Achieved Occupancy                        %         5.83
    Achieved Active Warps Per SM           warp         3.73
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 92.22%                                                                                    
          The difference between calculated theoretical (75.0%) and measured achieved occupancy (5.8%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 25%                                                                                       
          The 12.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the      
          hardware maximum of 16. This kernel's theoretical occupancy (75.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        43.50
    Total DRAM Elapsed Cycles        cycle      481,280
    Average L1 Active Cycles         cycle       485.97
    Total L1 Elapsed Cycles          cycle      930,228
    Average L2 Active Cycles         cycle       908.84
    Total L2 Elapsed Cycles          cycle      660,160
    Average SM Active Cycles         cycle       485.97
    Total SM Elapsed Cycles          cycle      930,228
    Average SMSP Active Cycles       cycle       449.75
    Total SMSP Elapsed Cycles        cycle    3,720,912
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.228%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 92.66% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.228%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 92.66% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.204%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 56.33% above the average, while the minimum instance value is 91.75% below the      
          average.                                                                                                      

  triton_linear_kernel (1, 12, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle       20,553
    Memory Throughput                 %         6.59
    DRAM Throughput                   %         6.59
    Duration                         us        17.86
    L1/TEX Cache Throughput           %        36.11
    L2 Cache Throughput               %         6.28
    SM Active Cycles              cycle     1,989.68
    Compute (SM) Throughput           %         1.69
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.21
    Achieved Active Warps Per SM           warp         3.98
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.43%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,871.50
    Total DRAM Elapsed Cycles        cycle    1,136,640
    Average L1 Active Cycles         cycle     1,989.68
    Total L1 Elapsed Cycles          cycle    2,184,204
    Average L2 Active Cycles         cycle    14,723.01
    Total L2 Elapsed Cycles          cycle    1,558,160
    Average SM Active Cycles         cycle     1,989.68
    Total SM Elapsed Cycles          cycle    2,184,204
    Average SMSP Active Cycles       cycle     1,990.18
    Total SMSP Elapsed Cycles        cycle    8,736,816
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.772%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.17% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.777%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.19% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.772%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.17% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.397%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 7.14% above the average, while the minimum instance value is 5.02% below the        
          average.                                                                                                      

  void at::vectorized_elementwise_kernel<4, at::AUnaryFunctor<float, float, float, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3) (6, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.54
    SM Frequency                    Ghz         1.13
    Elapsed Cycles                cycle        3,761
    Memory Throughput                 %         0.77
    DRAM Throughput                   %         0.45
    Duration                         us         3.33
    L1/TEX Cache Throughput           %        12.84
    L2 Cache Throughput               %         1.08
    SM Active Cycles              cycle        93.45
    Compute (SM) Throughput           %         0.05
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      6
    Registers Per Thread             register/thread              28
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             768
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 94.44%                                                                                          
          The grid for this launch is configured to execute only 6 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         6.15
    Achieved Active Warps Per SM           warp         3.93
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.85%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        22.90
    Total DRAM Elapsed Cycles        cycle      205,312
    Average L1 Active Cycles         cycle        93.45
    Total L1 Elapsed Cycles          cycle      407,228
    Average L2 Active Cycles         cycle       639.10
    Total L2 Elapsed Cycles          cycle      285,200
    Average SM Active Cycles         cycle        93.45
    Total SM Elapsed Cycles          cycle      407,228
    Average SMSP Active Cycles       cycle        93.51
    Total SMSP Elapsed Cycles        cycle    1,628,912
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.707%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 42.99% above the average, while the minimum instance value is 58.38% below the      
          average.                                                                                                      

  triton_linear_kernel (1, 12, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle       20,585
    Memory Throughput                 %         6.59
    DRAM Throughput                   %         6.59
    Duration                         us        17.89
    L1/TEX Cache Throughput           %        35.89
    L2 Cache Throughput               %         6.30
    SM Active Cycles              cycle     1,998.58
    Compute (SM) Throughput           %         1.66
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.28
    Achieved Active Warps Per SM           warp         4.02
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.25%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.3%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,871.50
    Total DRAM Elapsed Cycles        cycle    1,136,640
    Average L1 Active Cycles         cycle     1,998.58
    Total L1 Elapsed Cycles          cycle    2,224,518
    Average L2 Active Cycles         cycle    14,913.14
    Total L2 Elapsed Cycles          cycle    1,557,200
    Average SM Active Cycles         cycle     1,998.58
    Total SM Elapsed Cycles          cycle    2,224,518
    Average SMSP Active Cycles       cycle     1,979.69
    Total SMSP Elapsed Cycles        cycle    8,898,072
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.643%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.08% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.559%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.05% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.643%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.08% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.134%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 8.01% above the average, while the minimum instance value is 2.86% below the        
          average.                                                                                                      

  triton_linear_kernel (1, 12, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle       20,328
    Memory Throughput                 %         6.68
    DRAM Throughput                   %         6.68
    Duration                         us        17.63
    L1/TEX Cache Throughput           %        36.27
    L2 Cache Throughput               %         6.37
    SM Active Cycles              cycle     1,977.79
    Compute (SM) Throughput           %         1.68
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.25
    Achieved Active Warps Per SM           warp         4.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.32%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.3%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,871.50
    Total DRAM Elapsed Cycles        cycle    1,121,280
    Average L1 Active Cycles         cycle     1,977.79
    Total L1 Elapsed Cycles          cycle    2,195,116
    Average L2 Active Cycles         cycle    14,838.21
    Total L2 Elapsed Cycles          cycle    1,541,520
    Average SM Active Cycles         cycle     1,977.79
    Total SM Elapsed Cycles          cycle    2,195,116
    Average SMSP Active Cycles       cycle     1,979.85
    Total SMSP Elapsed Cycles        cycle    8,780,464
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.679%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.19% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.683%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.14% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.679%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.19% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  void at::<unnamed>::CatArrayBatchedCopy<at::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, at::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, at::<unnamed>::TensorSizeStride<T2, 4>, int, T2) (216, 2, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        7,469
    Memory Throughput                 %         0.90
    DRAM Throughput                   %         0.24
    Duration                         us         6.53
    L1/TEX Cache Throughput           %         3.85
    L2 Cache Throughput               %         0.57
    SM Active Cycles              cycle     1,768.30
    Compute (SM) Throughput           %         5.01
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    432
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread         221,184
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        68.56
    Achieved Active Warps Per SM           warp        43.88
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 31.44%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (68.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24.70
    Total DRAM Elapsed Cycles        cycle      412,672
    Average L1 Active Cycles         cycle     1,768.30
    Total L1 Elapsed Cycles          cycle      813,878
    Average L2 Active Cycles         cycle       853.40
    Total L2 Elapsed Cycles          cycle      565,680
    Average SM Active Cycles         cycle     1,768.30
    Total SM Elapsed Cycles          cycle      813,878
    Average SMSP Active Cycles       cycle     1,716.01
    Total SMSP Elapsed Cycles        cycle    3,255,512
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 15.91%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 67.81% above the average, while the minimum instance value is 36.15% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 15.75%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 69.18% above the average, while the minimum instance value is 42.95% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 15.91%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 67.81% above the average, while the minimum instance value is 36.15% below the      
          average.                                                                                                      

  void at::<unnamed>::CatArrayBatchedCopy<at::<unnamed>::OpaqueType<4>, unsigned int, 4, 64, 64>(T1 *, at::<unnamed>::CatArrInputTensorMetadata<T1, T2, T4, T5>, at::<unnamed>::TensorSizeStride<T2, 4>, int, T2) (216, 2, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.58
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        7,414
    Memory Throughput                 %         0.93
    DRAM Throughput                   %         0.24
    Duration                         us         6.46
    L1/TEX Cache Throughput           %         3.86
    L2 Cache Throughput               %         0.59
    SM Active Cycles              cycle     1,757.54
    Compute (SM) Throughput           %         5.15
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    432
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread         221,184
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            5
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        69.49
    Achieved Active Warps Per SM           warp        44.47
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 30.51%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (69.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24.60
    Total DRAM Elapsed Cycles        cycle      408,576
    Average L1 Active Cycles         cycle     1,757.54
    Total L1 Elapsed Cycles          cycle      789,770
    Average L2 Active Cycles         cycle       917.58
    Total L2 Elapsed Cycles          cycle      561,520
    Average SM Active Cycles         cycle     1,757.54
    Total SM Elapsed Cycles          cycle      789,770
    Average SMSP Active Cycles       cycle     1,709.35
    Total SMSP Elapsed Cycles        cycle    3,159,080
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 16.17%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 67.28% above the average, while the minimum instance value is 35.65% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 15.94%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 68.17% above the average, while the minimum instance value is 37.58% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 16.17%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 67.28% above the average, while the minimum instance value is 35.65% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.222%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 47.60% above the average, while the minimum instance value is 67.31% below the      
          average.                                                                                                      

  fmha_cutlassF_f32_aligned_64x64_rf_sm80(PyTorchMemEffAttention::AttentionKernel<float, cutlass::Sm80, 1, 64, 64, 64, 1, 1>::Params) (1, 12, 1)x(32, 4, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle       17,894
    Memory Throughput                 %         2.13
    DRAM Throughput                   %         0.45
    Duration                         us        15.55
    L1/TEX Cache Throughput           %        20.55
    L2 Cache Throughput               %         2.38
    SM Active Cycles              cycle     1,730.81
    Compute (SM) Throughput           %         2.30
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     12
    Registers Per Thread             register/thread             168
    Shared Memory Configuration Size           Kbyte          135.17
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           36.35
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,536
    Uses Green Context                                             0
    Waves Per SM                                                0.04
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 88.89%                                                                                          
          The grid for this launch is configured to execute only 12 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            3
    Block Limit Shared Mem                block            3
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           12
    Theoretical Occupancy                     %        18.75
    Achieved Occupancy                        %         6.21
    Achieved Active Warps Per SM           warp         3.98
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.87%                                                                                    
          The difference between calculated theoretical (18.8%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 81.25%                                                                                    
          The 3.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (18.8%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (18.8%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       110.10
    Total DRAM Elapsed Cycles        cycle      988,160
    Average L1 Active Cycles         cycle     1,730.81
    Total L1 Elapsed Cycles          cycle    1,925,154
    Average L2 Active Cycles         cycle     4,346.36
    Total L2 Elapsed Cycles          cycle    1,357,040
    Average SM Active Cycles         cycle     1,730.81
    Total SM Elapsed Cycles          cycle    1,925,154
    Average SMSP Active Cycles       cycle     1,712.09
    Total SMSP Elapsed Cycles        cycle    7,700,616
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.652%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 89.10% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.551%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 89.03% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.652%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 89.10% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.112%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 31.66% above the average, while the minimum instance value is 40.25% below the      
          average.                                                                                                      

  ampere_sgemm_128x32_tn (6, 1, 13)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.13
    Elapsed Cycles                cycle       13,858
    Memory Throughput                 %        23.46
    DRAM Throughput                   %         9.64
    Duration                         us        12.22
    L1/TEX Cache Throughput           %        41.58
    L2 Cache Throughput               %        13.46
    SM Active Cycles              cycle     7,755.35
    Compute (SM) Throughput           %        21.81
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     78
    Registers Per Thread             register/thread              57
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           16.38
    # SMs                                         SM             108
    Threads                                   thread          19,968
    Uses Green Context                                             0
    Waves Per SM                                                0.18
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 27.78%                                                                                          
          The grid for this launch is configured to execute only 78 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            5
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        12.49
    Achieved Active Warps Per SM           warp         7.99
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75.03%                                                                                    
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (12.5%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 50%                                                                                       
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (50.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,872.20
    Total DRAM Elapsed Cycles        cycle      776,704
    Average L1 Active Cycles         cycle     7,755.35
    Total L1 Elapsed Cycles          cycle    1,484,062
    Average L2 Active Cycles         cycle        8,637
    Total L2 Elapsed Cycles          cycle    1,055,360
    Average SM Active Cycles         cycle     7,755.35
    Total SM Elapsed Cycles          cycle    1,484,062
    Average SMSP Active Cycles       cycle     7,875.70
    Total SMSP Elapsed Cycles        cycle    5,936,248
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 18.57%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 32.89% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 19.04%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 33.22% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 18.57%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 32.89% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.289%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L2 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 8.08% above the average, while the minimum instance value is 6.62% below    
          the average.                                                                                                  

  void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T9 *, T8 *, T5 *, const T6 *, const T6 *, const T10 *, const T4 *, T10 *, void *, long, T6 *, int *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *) (24, 1, 1)x(32, 16, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        6,364
    Memory Throughput                 %         2.94
    DRAM Throughput                   %         2.94
    Duration                         us         5.57
    L1/TEX Cache Throughput           %         5.80
    L2 Cache Throughput               %         4.08
    SM Active Cycles              cycle       907.69
    Compute (SM) Throughput           %         1.70
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     24
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          12,288
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 77.78%                                                                                          
          The grid for this launch is configured to execute only 24 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        18.39
    Achieved Active Warps Per SM           warp        11.77
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 81.61%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (18.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       259.40
    Total DRAM Elapsed Cycles        cycle      353,280
    Average L1 Active Cycles         cycle       907.69
    Total L1 Elapsed Cycles          cycle      716,254
    Average L2 Active Cycles         cycle     2,661.40
    Total L2 Elapsed Cycles          cycle      482,080
    Average SM Active Cycles         cycle       907.69
    Total SM Elapsed Cycles          cycle      716,254
    Average SMSP Active Cycles       cycle       893.79
    Total SMSP Elapsed Cycles        cycle    2,865,016
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 10.77%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 78.69% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.63%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 78.86% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.77%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 78.69% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.654%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 19.60% above the average, while the minimum instance value is 28.42% below the      
          average.                                                                                                      

  void at::vectorized_elementwise_kernel<4, at::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3) (6, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.57
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        3,966
    Memory Throughput                 %         0.89
    DRAM Throughput                   %         0.77
    Duration                         us         3.49
    L1/TEX Cache Throughput           %        12.57
    L2 Cache Throughput               %         1.42
    SM Active Cycles              cycle       102.54
    Compute (SM) Throughput           %         0.05
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      6
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             768
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 94.44%                                                                                          
          The grid for this launch is configured to execute only 6 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         6.25
    Achieved Active Warps Per SM           warp         4.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.75%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.3%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        42.50
    Total DRAM Elapsed Cycles        cycle      219,648
    Average L1 Active Cycles         cycle       102.54
    Total L1 Elapsed Cycles          cycle      434,266
    Average L2 Active Cycles         cycle       708.84
    Total L2 Elapsed Cycles          cycle      300,800
    Average SM Active Cycles         cycle       102.54
    Total SM Elapsed Cycles          cycle      434,266
    Average SMSP Active Cycles       cycle       101.56
    Total SMSP Elapsed Cycles        cycle    1,737,064
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.067%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 42.79% above the average, while the minimum instance value is 89.42% below the      
          average.                                                                                                      

  void at::<unnamed>::vectorized_layer_norm_kernel<float, float, 0>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *) (8, 1, 1)x(32, 4, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle        8,637
    Memory Throughput                 %         0.71
    DRAM Throughput                   %         0.36
    Duration                         us         7.52
    L1/TEX Cache Throughput           %         5.92
    L2 Cache Throughput               %         0.95
    SM Active Cycles              cycle       483.44
    Compute (SM) Throughput           %         0.45
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              38
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block              24
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 92.59%                                                                                          
          The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           28
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %           75
    Achieved Occupancy                        %         5.93
    Achieved Active Warps Per SM           warp         3.80
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 92.09%                                                                                    
          The difference between calculated theoretical (75.0%) and measured achieved occupancy (5.9%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 25%                                                                                       
          The 12.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the      
          hardware maximum of 16. This kernel's theoretical occupancy (75.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        43.50
    Total DRAM Elapsed Cycles        cycle      476,928
    Average L1 Active Cycles         cycle       483.44
    Total L1 Elapsed Cycles          cycle      929,400
    Average L2 Active Cycles         cycle       994.52
    Total L2 Elapsed Cycles          cycle      654,400
    Average SM Active Cycles         cycle       483.44
    Total SM Elapsed Cycles          cycle      929,400
    Average SMSP Active Cycles       cycle       457.73
    Total SMSP Elapsed Cycles        cycle    3,717,600
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.203%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 92.61% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.203%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 92.61% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.578%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 54.11% above the average, while the minimum instance value is 92.46% below the      
          average.                                                                                                      

  triton_linear_kernel (1, 48, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle       21,416
    Memory Throughput                 %        24.84
    DRAM Throughput                   %        24.84
    Duration                         us        18.78
    L1/TEX Cache Throughput           %        35.30
    L2 Cache Throughput               %        23.11
    SM Active Cycles              cycle     8,156.56
    Compute (SM) Throughput           %         6.49
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     48
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          167.94
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           24.58
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           6,144
    Uses Green Context                                             0
    Waves Per SM                                                0.07
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 55.56%                                                                                          
          The grid for this launch is configured to execute only 48 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block            6
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %        37.50
    Achieved Occupancy                        %         6.22
    Achieved Active Warps Per SM           warp         3.98
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 83.42%                                                                                    
          The difference between calculated theoretical (37.5%) and measured achieved occupancy (6.2%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 62.5%                                                                                     
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (37.5%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     7,408.30
    Total DRAM Elapsed Cycles        cycle    1,192,960
    Average L1 Active Cycles         cycle     8,156.56
    Total L1 Elapsed Cycles          cycle    2,270,848
    Average L2 Active Cycles         cycle    16,366.66
    Total L2 Elapsed Cycles          cycle    1,625,920
    Average SM Active Cycles         cycle     8,156.56
    Total SM Elapsed Cycles          cycle    2,270,848
    Average SMSP Active Cycles       cycle     8,152.30
    Total SMSP Elapsed Cycles        cycle    9,083,392
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 22.25%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 57.36% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.19%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 57.22% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 22.25%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 57.36% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.284%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L2 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 7.80% above the average, while the minimum instance value is 6.02% below    
          the average.                                                                                                  

  void at::vectorized_elementwise_kernel<4, at::<unnamed>::launch_clamp_scalar(at::TensorIteratorBase &, c10::Scalar, c10::Scalar, detail::ClampLimits)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 7)]::operator ()() const::[lambda(float) (instance 1)], std::array<char *, 2>>(int, T2, T3) (24, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.57
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        4,080
    Memory Throughput                 %         1.64
    DRAM Throughput                   %         1.46
    Duration                         us         3.58
    L1/TEX Cache Throughput           %         9.05
    L2 Cache Throughput               %         3.63
    SM Active Cycles              cycle       397.65
    Compute (SM) Throughput           %         0.24
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     24
    Registers Per Thread             register/thread              24
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           3,072
    Uses Green Context                                             0
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 77.78%                                                                                          
          The grid for this launch is configured to execute only 24 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         6.16
    Achieved Active Warps Per SM           warp         3.94
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.84%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        82.50
    Total DRAM Elapsed Cycles        cycle      225,280
    Average L1 Active Cycles         cycle       397.65
    Total L1 Elapsed Cycles          cycle      442,586
    Average L2 Active Cycles         cycle     1,066.20
    Total L2 Elapsed Cycles          cycle      309,520
    Average SM Active Cycles         cycle       397.65
    Total SM Elapsed Cycles          cycle      442,586
    Average SMSP Active Cycles       cycle       385.55
    Total SMSP Elapsed Cycles        cycle    1,770,344
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.829%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 80.69% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.501%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 79.73% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.829%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 80.69% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.97%                                                                                           
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L2 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 28.92% above the average, while the minimum instance value is 29.28% below  
          the average.                                                                                                  

  ampere_sgemm_128x32_sliced1x4_tn (6, 1, 9)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle       36,760
    Memory Throughput                 %        24.20
    DRAM Throughput                   %        14.61
    Duration                         us        32.16
    L1/TEX Cache Throughput           %        52.37
    L2 Cache Throughput               %        14.40
    SM Active Cycles              cycle    16,904.76
    Compute (SM) Throughput           %        31.50
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.5 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     54
    Registers Per Thread             register/thread             134
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.01
    # SMs                                         SM             108
    Threads                                   thread          13,824
    Uses Green Context                                             0
    Waves Per SM                                                0.50
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          The grid for this launch is configured to execute only 54 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            1
    Block Limit Shared Mem                block            1
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        12.50
    Achieved Occupancy                        %        12.50
    Achieved Active Warps Per SM           warp         8.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 87.5%                                                                                     
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the number of required      
          registers. This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory.    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     7,467.40
    Total DRAM Elapsed Cycles        cycle    2,044,416
    Average L1 Active Cycles         cycle    16,904.76
    Total L1 Elapsed Cycles          cycle    3,951,536
    Average L2 Active Cycles         cycle    22,479.94
    Total L2 Elapsed Cycles          cycle    2,791,360
    Average SM Active Cycles         cycle    16,904.76
    Total SM Elapsed Cycles          cycle    3,951,536
    Average SMSP Active Cycles       cycle    16,989.23
    Total SMSP Elapsed Cycles        cycle   15,806,144
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.6%                                                                                           
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 51.08% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.52%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 50.65% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.6%                                                                                           
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 51.08% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  void cublasLt::splitKreduce_kernel<32, 16, int, float, float, float, 0, float, float, float, 1, 1, 0>(cublasLt::cublasSplitKParams<T6>, const T4 *, const T9 *, T8 *, T5 *, const T6 *, const T6 *, const T10 *, const T4 *, T10 *, void *, long, T6 *, int *, T6 *, const T6 *, const T6 *, const T6 *, const T6 *) (24, 1, 1)x(32, 16, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.57
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        5,771
    Memory Throughput                 %         2.30
    DRAM Throughput                   %         2.30
    Duration                         us         5.06
    L1/TEX Cache Throughput           %         6.06
    L2 Cache Throughput               %         3.52
    SM Active Cycles              cycle       753.42
    Compute (SM) Throughput           %         1.82
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     24
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread          12,288
    Uses Green Context                                             0
    Waves Per SM                                                0.06
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 77.78%                                                                                          
          The grid for this launch is configured to execute only 24 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        18.21
    Achieved Active Warps Per SM           warp        11.65
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 81.79%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (18.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       182.60
    Total DRAM Elapsed Cycles        cycle      317,696
    Average L1 Active Cycles         cycle       753.42
    Total L1 Elapsed Cycles          cycle      620,508
    Average L2 Active Cycles         cycle     1,758.83
    Total L2 Elapsed Cycles          cycle      437,440
    Average SM Active Cycles         cycle       753.42
    Total SM Elapsed Cycles          cycle      620,508
    Average SMSP Active Cycles       cycle       721.75
    Total SMSP Elapsed Cycles        cycle    2,482,032
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 10.46%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 79.74% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 9.986%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 79.50% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.46%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 79.74% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 13.1%                                                                                           
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 40.74% above the average, while the minimum instance value is 45.13% below the      
          average.                                                                                                      

  void at::vectorized_elementwise_kernel<4, at::CUDAFunctor_add<float>, std::array<char *, 3>>(int, T2, T3) (6, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.57
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle        4,196
    Memory Throughput                 %         0.85
    DRAM Throughput                   %         0.74
    Duration                         us         3.68
    L1/TEX Cache Throughput           %        12.44
    L2 Cache Throughput               %         1.44
    SM Active Cycles              cycle       103.57
    Compute (SM) Throughput           %         0.05
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      6
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread             768
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 94.44%                                                                                          
          The grid for this launch is configured to execute only 6 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         6.23
    Achieved Active Warps Per SM           warp         3.98
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.77%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        42.50
    Total DRAM Elapsed Cycles        cycle      230,400
    Average L1 Active Cycles         cycle       103.57
    Total L1 Elapsed Cycles          cycle      430,032
    Average L2 Active Cycles         cycle       708.02
    Total L2 Elapsed Cycles          cycle      318,000
    Average SM Active Cycles         cycle       103.57
    Total SM Elapsed Cycles          cycle      430,032
    Average SMSP Active Cycles       cycle       105.17
    Total SMSP Elapsed Cycles        cycle    1,720,128
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.337%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 35.58% above the average, while the minimum instance value is 86.58% below the      
          average.                                                                                                      

  void at::<unnamed>::vectorized_layer_norm_kernel<float, float, 0>(int, T2, const T1 *, const T1 *, const T1 *, T2 *, T2 *, T1 *) (8, 1, 1)x(32, 4, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle        8,359
    Memory Throughput                 %         0.71
    DRAM Throughput                   %         0.38
    Duration                         us         7.26
    L1/TEX Cache Throughput           %         6.20
    L2 Cache Throughput               %         0.80
    SM Active Cycles              cycle       462.59
    Compute (SM) Throughput           %         0.46
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              38
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block              24
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread           1,024
    Uses Green Context                                             0
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 92.59%                                                                                          
          The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 108             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           28
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %           75
    Achieved Occupancy                        %         5.99
    Achieved Active Warps Per SM           warp         3.84
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 92.01%                                                                                    
          The difference between calculated theoretical (75.0%) and measured achieved occupancy (6.0%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 25%                                                                                       
          The 12.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the      
          hardware maximum of 16. This kernel's theoretical occupancy (75.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        43.50
    Total DRAM Elapsed Cycles        cycle      460,800
    Average L1 Active Cycles         cycle       462.59
    Total L1 Elapsed Cycles          cycle      900,766
    Average L2 Active Cycles         cycle       855.66
    Total L2 Elapsed Cycles          cycle      633,920
    Average SM Active Cycles         cycle       462.59
    Total SM Elapsed Cycles          cycle      900,766
    Average SMSP Active Cycles       cycle       448.84
    Total SMSP Elapsed Cycles        cycle    3,603,064
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.142%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 92.71% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.009%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 93.07% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.142%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 92.71% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      

  void scal_64addr_kernel<float, float, 1, 1, 6, 5, 5, 3>(cublasTransposeParams<T2>, const T1 *, T1 *, const T2 *) (786, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.57
    SM Frequency                    Ghz         1.12
    Elapsed Cycles                cycle        5,505
    Memory Throughput                 %        18.91
    DRAM Throughput                   %         0.04
    Duration                         us         4.90
    L1/TEX Cache Throughput           %        23.56
    L2 Cache Throughput               %        23.30
    SM Active Cycles              cycle     2,965.69
    Compute (SM) Throughput           %        32.05
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.9 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    786
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM             108
    Threads                                   thread         201,216
    Uses Green Context                                             0
    Waves Per SM                                                0.91
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            8
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        58.06
    Achieved Active Warps Per SM           warp        37.16
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 41.94%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (58.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle         3.30
    Total DRAM Elapsed Cycles        cycle      308,352
    Average L1 Active Cycles         cycle     2,965.69
    Total L1 Elapsed Cycles          cycle      588,566
    Average L2 Active Cycles         cycle        2,302
    Total L2 Elapsed Cycles          cycle      418,240
    Average SM Active Cycles         cycle     2,965.69
    Total SM Elapsed Cycles          cycle      588,566
    Average SMSP Active Cycles       cycle     2,912.27
    Total SMSP Elapsed Cycles        cycle    2,354,264
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.315%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 11.60% above the average, while the minimum instance value is 9.19% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.492%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 12.15% above the average, while the minimum instance value is 11.79% below  
          the average.                                                                                                  
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.315%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 11.60% above the average, while the minimum instance value is 9.19% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.56%                                                                                           
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 14.90% above the average, while the minimum instance value is 11.29% below the      
          average.                                                                                                      

  void sgemm_largek_lds64<1, 0, 6, 3, 4, 5, 2, 66>(float *, const float *, const float *, int, int, int, int, int, int, const float *, const float *, float, float, int, int, int *, int *) (786, 1, 2)x(32, 4, 1), Context 1, Stream 7, Device 0, CC 8.0

    NVTX Push/Pop Stack for Thread 19059:
     <default domain>
        <0,OPT_Triton_QKV_FC1>
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.15
    Elapsed Cycles                cycle      220,821
    Memory Throughput                 %        53.56
    DRAM Throughput                   %        40.80
    Duration                         us       191.52
    L1/TEX Cache Throughput           %        60.33
    L2 Cache Throughput               %        49.24
    SM Active Cycles              cycle   180,597.94
    Compute (SM) Throughput           %        34.84
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  1,572
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            4.87
    # SMs                                         SM             108
    Threads                                   thread         201,216
    Uses Green Context                                             0
    Waves Per SM                                                1.21
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           12
    Block Limit Shared Mem                block           17
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %           75
    Achieved Occupancy                        %        61.24
    Achieved Active Warps Per SM           warp        39.19
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 18.35%                                                                                    
          The difference between calculated theoretical (75.0%) and measured achieved occupancy (61.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Local Speedup: 25%                                                                                       
          The 12.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the      
          hardware maximum of 16. This kernel's theoretical occupancy (75.0%) is limited by the number of required      
          registers.                                                                                                    

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   124,453.90
    Total DRAM Elapsed Cycles        cycle   12,201,472
    Average L1 Active Cycles         cycle   180,597.94
    Total L1 Elapsed Cycles          cycle   21,967,796
    Average L2 Active Cycles         cycle   187,641.59
    Total L2 Elapsed Cycles          cycle   16,758,080
    Average SM Active Cycles         cycle   180,597.94
    Total SM Elapsed Cycles          cycle   21,967,796
    Average SMSP Active Cycles       cycle   174,345.41
    Total SMSP Elapsed Cycles        cycle   87,871,184
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 15.4%                                                                                           
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 17.35% above the average, while the minimum instance value is 14.77% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 15.77%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 18.40% above the average, while the minimum instance value is 16.19% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 15.4%                                                                                           
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 17.35% above the average, while the minimum instance value is 14.77% below the      
          average.                                                                                                      

  void at::reduce_kernel<512, 1, at::ReduceOp<float, at::MeanOps<float, float, float, float>, unsigned int, float, 4, 4>>(T3) (1, 50, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 8.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         1.59
    SM Frequency                    Ghz         1.14
    Elapsed Cycles                cycle       13,209
    Memory Throughput                 %         7.09
    DRAM Throughput                   %         7.09
    Duration                         us        11.52
    L1/TEX Cache Throughput           %         8.89
    L2 Cache Throughput               %         7.25
    SM Active Cycles              cycle     3,841.50
    Compute (SM) Throughput           %         3.95
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     50
    Registers Per Thread             register/thread              32
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            2.05
    Static Shared Memory Per Block        byte/block              16
    # SMs                                         SM             108
    Threads                                   thread          25,600
    Uses Green Context                                             0
    Waves Per SM                                                0.12
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 53.7%                                                                                           
          The grid for this launch is configured to execute only 50 blocks, which is less than the GPU's 108            
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block           10
    Block Limit Warps                     block            4
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        24.70
    Achieved Active Warps Per SM           warp        15.81
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 75.3%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (24.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     1,296.90
    Total DRAM Elapsed Cycles        cycle      732,160
    Average L1 Active Cycles         cycle     3,841.50
    Total L1 Elapsed Cycles          cycle    1,403,596
    Average L2 Active Cycles         cycle     4,318.35
    Total L2 Elapsed Cycles          cycle    1,001,520
    Average SM Active Cycles         cycle     3,841.50
    Total SM Elapsed Cycles          cycle    1,403,596
    Average SMSP Active Cycles       cycle     3,800.31
    Total SMSP Elapsed Cycles        cycle    5,614,384
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 19.43%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 65.73% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 19.18%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 65.58% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 19.43%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 65.73% above the average, while the minimum instance value is 100.00% below the     
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.127%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 17.76% above the average, while the minimum instance value is 14.46% below the      
          average.                                                                                                      

